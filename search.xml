<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[爬虫-精品聚合网下载图片]]></title>
    <url>%2FSonicHuang%2Ff5189117%2F</url>
    <content type="text"><![CDATA[之前写了两篇下载煎蛋网图片的文章，这篇是精品聚合网妹子图片下载，与从煎蛋网下载极为类似，就不再细说，下载完成后有也有一万多张照片。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import requestsfrom bs4 import BeautifulSoupimport chardetimport os.pathimport osimport timeimport pickle#定义函数返回地址二进制数据内容def bhtml(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers) bhtml = response.content return bhtml #每个地址的soup数据 def data(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers) global code code = chardet.detect(response.content)['encoding'] response.encoding = code html = response.text #print(html) soup = BeautifulSoup(html, 'html.parser') return soup #所有分页的地址(生成器)def getpage(): page = 12 for n in range(page, 0, -1):#页面倒数到1 pageurl = ''.join(['http://www.jingpinjuhe.com/?cat=26&amp;paged=', str(n)]) yield pageurl#所有分页的地址(生成器)def getitempage(soup): list1 = soup.find_all('a', attrs=&#123;'target':'_blank', 'title':True&#125;) # &lt;a target="_blank" href="url address" title="title message"&gt;some message&lt;/a&gt; for eachitem in list1: itemurl = eachitem['href'] itemtitle = eachitem['title'] print(itemtitle, itemurl, '下载中....') yield itemurl#每个页面的图片地址，下载保存到本地, def downpic(soup): passurllist = [] list2 = soup.find_all('img', attrs=&#123;'src':True&#125;) for pictag in list2: eachpicurl = pictag['src'] if '&lt;img src=' in eachpicurl: continue if ' /&gt;' in eachpicurl: eachpicurl = eachpicurl.replace(' /&gt;', '') if 'sinaimg' in eachpicurl: if 'bmiddle' in eachpicurl: eachpicurl = eachpicurl.replace('bmiddle', 'large') if 'mw690' in eachpicurl: eachpicurl = eachpicurl.replace('mw690', 'large') if 'mw600' in eachpicurl: eachpicurl = eachpicurl.replace('mw600', 'large') if 'thumbnail' in eachpicurl: eachpicurl = eachpicurl.replace('thumbnail', 'large') if 'small' in eachpicurl: eachpicurl = eachpicurl.replace('small', 'large') if 'thumb150' in eachpicurl: eachpicurl = eachpicurl.replace('thumb150', 'large') if eachpicurl in ['http://www.jingpinjuhe.com/wp-content/themes/xiu/images/logo.png']:#如果是网站图标则忽略 continue picname = os.path.split(eachpicurl)[1] try: #导出图片的二进制数据 picdata = bhtml(eachpicurl) except: print(eachpicurl, '下载有问题') passurllist.append(eachpicurl) with open('passurl.txt', 'wb') as f1: pickle.dump(passurllist, f1) continue else: if 'gif' not in picname:#不下载gif图片 with open(picname, 'wb') as f: f.write(picdata) time.sleep(0.5)def main(): try:#新建一个文件夹，如果已经存在就pass os.mkdir(r'D:\download\pic\new') except: pass os.chdir(r'D:\download\pic\new') #改变工作目录 url = r'http://www.jingpinjuhe.com/?cat=26&amp;paged=12' #最后一页 pageurl = getpage()#所有页码的地址 for x in pageurl: print(x) pagesoup = data(x)#每个页面的soup itemaddress = getitempage(pagesoup) for itemurl in itemaddress: if itemurl in [r'http://www.jingpinjuhe.com/?p=2965']: continue itemsoup = data(itemurl) downpic(itemsoup) print('下载完毕.') if __name__ == '__main__': main() 只是下载完后发现有一部分地址名后面没有图片的后缀，这个问题也很简单，把所有没有后缀的图片文件放在一个文件夹中，使用os.walk(path)导出所有文件名，然后用os.rename(oldname, oldname+’jpg’)就解决问题了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>chardet</tag>
        <tag>requests</tag>
        <tag>BeautifulSoup</tag>
        <tag>os</tag>
        <tag>time</tag>
        <tag>pickle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-下载上万张煎蛋网旧图片]]></title>
    <url>%2FSonicHuang%2Fdf12fa59%2F</url>
    <content type="text"><![CDATA[继上一篇从煎蛋网下载妹子图片后，在网上发现另外两个途径可以下载到煎蛋网往年的旧图片。第一个是个网页，作者把所有图片地址写入一个网页中，没有一个文字只有图片有五千多张。第二个是一个本地html文件，也是一样用浏览器打开后全是图片，有六千多张。据我后面看着两个地方的图片还没有发现重复的。全部下载完，除去一些失效的图片链接，有上万张。 总的来说，写这个爬虫比较简单，没有复杂的标签，没有分页码，简单粗暴方法就可以应付了。 思路是：1. 用requests得到数据后，分析出所有图片的地址，写入一个allurl.txt文件中。2.从allurl.txt文件中读取图片地址，并下载，下载不成功的地址写入passurl.txt, 成功的地址写入okurl.txt中。3. 把第2步中passurl.txt该名为allurl.txt， 删除okurl.txt和旧的allurl.txt并打开代理或者其他重复第二步，直到没有更多的地址可以下载。 一. 从网页上获取数据写入allurl.txt1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import requestsimport chardetfrom bs4 import BeautifulSoupimport osimport pickle#页面的soup数据def data(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers, timeout = 5) global code code = chardet.detect(response.content)['encoding'] response.encoding = code html = response.text #print(html) soup = BeautifulSoup(html, 'html.parser') return soup#页面所有图片的地址列表def getpicurl(soup): list1 = soup.find_all('img') for eachurl in list1: picurl = eachurl['src'] yield picurl#主程序会生成一个包含有所有图片下载地址的allurl.txt的二进制文件def main(): allurllist = [] try:#新建一个文件夹，如果已经存在就pass os.mkdir('D:\download\pic') except: pass os.chdir(r'D:\download\pic') #改变工作目录 url = r'http://js.funet8.com/html/jiandan-meizhi.html' #网站地址 soupdata = data(url) #网站的soup数据 pictureurl = getpicurl(soupdata) #生成器产生的每张图片的地址 for eachpicurl in pictureurl: #对地址进行删选。由于网页代码的一些问题，导致地址出现一些错误 if '&lt;img src=' in eachpicurl: continue if ' /&gt;' in eachpicurl: eachpicurl = eachpicurl.replace(' /&gt;', '') #修改一下可以得到大图的地址 if 'sinaimg' in eachpicurl: if 'bmiddle' in eachpicurl: eachpicurl = eachpicurl.replace('bmiddle', 'large') if 'mw690' in eachpicurl: eachpicurl = eachpicurl.replace('mw690', 'large') if 'mw600' in eachpicurl: eachpicurl = eachpicurl.replace('mw600', 'large') if 'thumbnail' in eachpicurl: eachpicurl = eachpicurl.replace('thumbnail', 'large') if 'small' in eachpicurl: eachpicurl = eachpicurl.replace('small', 'large') if 'thumb150' in eachpicurl: eachpicurl = eachpicurl.replace('thumb150', 'large') #把所有地址组成的列表写入文件 allurllist.append(eachpicurl) with open('allurl.txt', 'wb') as f2: pickle.dump(allurllist, f2)if __name__ == '__main__': main() ​ 二.从本地html文件上获取数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import requestsimport chardetfrom bs4 import BeautifulSoupimport osimport pickle#页面的soup数据def data(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers, timeout = 5) global code code = chardet.detect(response.content)['encoding'] response.encoding = code html = response.text #print(html) soup = BeautifulSoup(html, 'html.parser') return soup#页面所有图片的地址列表def getpicurl(soup): list1 = soup.find_all('img') for eachurl in list1: picurl = eachurl['src'] yield picurl def main(): allurllist = [] try:#新建一个文件夹，如果已经存在就pass os.mkdir('D:\download\pic') except: pass os.chdir(r'D:\download\pic') #改变工作目录 url = r'E:\python files\IDEL FILES\My py file\爬虫煎蛋网旧图片\jiandan.html' #文件保存地址，需要根据实际情况变化 #读取本地html文件，注意open的参数需要加上encoding的参数，否则会出现编码错误，至于编码方法可以用chardet测试 #得到的f是一个迭代器，迭代出的是包含每个图片地址标签的字符串，如果我们直接对这个标签字符串进行操作会很麻烦，所以用BeautifulSoup进行解析 with open(url, mode='r', encoding='utf-8') as f: soupdata = BeautifulSoup(f, 'html.parser') pictureurl = getpicurl(soupdata) for eachpicurl in pictureurl: if '&lt;img src=' in eachpicurl: continue if ' /&gt;' in eachpicurl: eachpicurl = eachpicurl.replace(' /&gt;', '') if 'sinaimg' in eachpicurl: if 'bmiddle' in eachpicurl: eachpicurl = eachpicurl.replace('bmiddle', 'large') if 'mw690' in eachpicurl: eachpicurl = eachpicurl.replace('mw690', 'large') if 'mw600' in eachpicurl: eachpicurl = eachpicurl.replace('mw600', 'large') if 'thumbnail' in eachpicurl: eachpicurl = eachpicurl.replace('thumbnail', 'large') if 'small' in eachpicurl: eachpicurl = eachpicurl.replace('small', 'large') if 'thumb150' in eachpicurl: eachpicurl = eachpicurl.replace('thumb150', 'large') allurllist.append(eachpicurl) with open('allurl.txt', 'wb') as f2: pickle.dump(allurllist, f2)if __name__ == '__main__': main() 三. 从allurl.txt读取数据下载图片1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import requestsimport chardetfrom bs4 import BeautifulSoupimport os.pathimport osimport pickle#定义函数返回地址二进制数据内容def bhtml(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers, timeout = 5) bhtml = response.content return bhtml#页面所有图片的地址列表def getpicurl(soup): list1 = soup.find_all('img') for eachurl in list1: picurl = eachurl['src'] yield picurl#从allurl.txt读取数据并下载图片 def main(): try:#新建一个文件夹，如果已经存在就pass os.mkdir('D:\download\pic') except: pass os.chdir(r'D:\download\pic') #改变工作目录 #如果第一次运行，就直接从allurl.txt读取数据, passurl和okurl都为空。 #如果中途因为各种原因停止了程序，那么从allurl中删除已经下载的和没有下载成功的地址，这样可以避免重启时重复下载图片。 try: with open(r'D:\download\pic\passurl.txt', 'rb') as psurl: passurllist = pickle.load(psurl) with open(r'D:\download\pic\okurl.txt', 'rb') as goodurl: okurllist = pickle.load(goodurl) with open(r'D:\download\pic\allurl.txt', 'rb') as totalurl: allurllist = pickle.load(totalurl) except: passurllist = [] okurllist = [] with open(r'D:\download\pic\allurl.txt', 'rb') as totalurl: allurllist = pickle.load(totalurl) else: newalllist = [x for x in allurllist if x not in passurllist and x not in okurllist] with open(r'D:\download\pic\allurl.txt', 'wb') as total: pickle.dump(newalllist, total) #遇到读取数据时出现问题就把地址保存在passurl里面，下载完成的保存在okurl里面。 for eachpicurl in allurllist: picname = os.path.split(eachpicurl)[1] try: picdata = bhtml(eachpicurl) except: print(eachpicurl, '下载有问题') passurllist.append(eachpicurl) with open('passurl.txt', 'wb') as f1: pickle.dump(passurllist, f1) continue else: try: with open(picname, 'wb') as f: f.write(picdata) except: print(eachpicurl, '下载有问题') passurllist.append(eachpicurl) with open('passurl.txt', 'wb') as f4: pickle.dump(passurllist, f4) continue else: print(eachpicurl, '下载ok') okurllist.append(eachpicurl) with open('okurl.txt', 'wb') as f3: pickle.dump(okurllist, f3)if __name__ == '__main__': main() 总结： 从本地html文件读取数据的方法 对异常的处理]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>chardet</tag>
        <tag>requests</tag>
        <tag>BeautifulSoup</tag>
        <tag>os</tag>
        <tag>pickle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-煎蛋网图片下载和分类]]></title>
    <url>%2FSonicHuang%2Fd6a406bd%2F</url>
    <content type="text"><![CDATA[一. 通过爬取煎蛋网随手拍，下载网页上的图片。 1.煎蛋网不知道是什么原因把以前妹子图的地址改成随手拍，而且他下面的页码在超过一定页数（目测35页左右），就会删除掉前面几页的内容。他页面地址http://jandan.net/ooxx/page-32#comments， 32就是他的页码数。我们首先需要知道他首页的页码，然后后面-=1，直到1。Chrome浏览器在随手拍页面首页审查元素，看到current-comment-page就是首页第32页的代码位置，所以知道我们找到这个标签就可以确定首页的页码。我使用了BeautifulSoup，得到每个页面的文本soup数据函数data(url)就可以查找到该标签。函数getpage(soup)代码第40行soup.find_all(&#39;span&#39;, attrs={&#39;class&#39;:&#39;current-comment-page&#39;}), 随后再通过函数getpage(soup)其他页面的地址就很容易了。 2.得到页面地址后，我们就需要知道每个页面上所有图片的地址。任意一张页面上的图片鼠标右键审查元素可以看到标签的组成，可以看到原图地址就是把mw600改为large。所以我们只要找到这个标签就可以了。函数downpic(soup)第57行soup.find_all(&#39;a&#39;, attrs={&#39;class&#39;:&#39;view_img_link&#39;}) 其实在我刚写这段代码时，煎蛋网的还采用了反爬机制，对地址进行了加密，所以我代码中还保留的那部分内容作为备注。参考前辈的例子Python爬虫爬取煎蛋网无聊图，煎蛋网也是用心良苦啊，栏目名字都改来来去的。 3.然后就可以读取图片地址的数据，下载图片。下载图片实际上就是把图片的数据写入一个新的图片文件里。函数downpic(soup)第78，79行。4.完善代码。以及其中遇到的问题。 检测网页的编码。自然用到chardet.但是有时会出现不名的原因导致检测的编码类型有误，导致页面有乱码，查找不到我们想要的标签。本来是utf-8, 得到的结果是windows-1254,这是什么鬼，我也不知道，查了很久终于在一篇文章中找到了解决方法，见下面应用内容。参考再也不用担心网页编码的坑了， 把response.encoding = None， 运行一下再改过来 = code.问题就解决了。这里还要注意，chardet.detect的参数是二进制数据，response.text是文本数据，.content才是二进制数据。 123code = chardet.detect(response.content)['encoding']response.encoding = codehtml = response.text 那么当你发现response.text返回乱码的时候，怎么办呢。。。 只要先设置编码为None… 再打印.text就可以了.. 123&gt; response.encoding = None&gt; response.text&gt; 为了不重复下载以前下载过的图片（毕竟每天图片都在更新）， 所以我使用pickle把所有的图片名保存在了文件里，再下载时，先读取保存的文件（一个列表以二进制的数据保存），列表中没有的图片名才下载。由于网站保存的图片名都是固定的，所以当遇到旧的图片名时就可以停止下载。如果把每次出现的新图片都储存在文件里，势必让文件越来越大，为了解决这个问题，我把新图片名写入一个临时文件，再写入一个旧的图片名，最后删除原来的文件，把临时文件改名为正式文件。这样每次下载完后只保存了新的图片名和一个旧图片名作为下载的终点。 同时为了让函数downpic(soup)读取到旧图片名时让函数和主程序main()都立刻停止循环执行下面的脚本不要再重复读取页面数据，就需要从函数中发出一个信号，让程序终止。我使用了raise语句来引发一个异常，我自定义了一个Stopdownload()的异常， 函数和主程序都可以通过接受到异常来终止程序。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import requestsimport refrom bs4 import BeautifulSoupimport chardetimport urllib.parse as upimport base64import os.pathimport osimport timeimport pickle#自定义一个异常类class Stopdownload(Exception): def __init__(self, err='遇到旧图片,停止下载了。'): Exception.__init__(self, err)#定义函数返回地址二进制数据内容def bhtml(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers) bhtml = response.content return bhtml #每个地址的soup数据 def data(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers) global code code = chardet.detect(response.content)['encoding'] response.encoding = code html = response.text soup = BeautifulSoup(html, 'html.parser') return soup #所有分页的地址(生成器)def getpage(soup): list1 = soup.find_all('span', attrs=&#123;'class':'current-comment-page'&#125;) # &lt;span class="current-comment-page"&gt;[56]&lt;/span&gt; a = list1[0].text #a='[56]'他是一个字符串 page = int(a[1:3]) #切片 for n in range(page-1, 0, -1):#页面倒数到1 pageurl = ''.join(['http://jandan.net/ooxx/page-', str(n), '#comments']) yield pageurl#每个页面的图片地址，下载保存到本地, def downpic(soup): try: #打开保存的已下载的图片名数据 with open('D:\download\piclist.txt', 'rb') as f2: list1 = pickle.load(f2)#把数据导入列表1 except: #如果第一次下载，列表1为空列表 list1 = [] #找到包含有页面图片下载地址的标签列表2 #list2 = soup.find_all('span', attrs=&#123;'class':'img-hash'&#125;) #网站使用了反爬机制，参考https://www.jianshu.com/p/5351baf254ef list2 = soup.find_all('a', attrs=&#123;'class':'view_img_link'&#125;) #定义一个空的列表3，用于写入所有新下载的图片。 list3 = [] #找到所有图片的下载地址和图片名称 for eachcode in list2: #piccode = str(base64.b64decode(eachcode.string.encode(code)))[2:].replace('mw600', 'large').replace('\'', '')#使用反爬机制时，需要对数据解密。参考https://www.jianshu.com/p/5351baf254ef piccode = eachcode['href'] picurl = ''.join(['http:', piccode]) if picurl in ['http://ww3.sinaimg.cn/large/006XNEY7gy1g1xabxlnq4j30fo0jwwhv.jpg']:#如果网页的图片出现的加载错误，导致不能读取picdata。就需要跳过这个图片。其实也可以使用异常处理的方法 continue picname = os.path.split(picurl)[1] #导出图片的二进制数据 picdata = bhtml(picurl) #如果图片没有下载过，则下载图片, 并把图片名导入列表3 if picname not in list1: print(picname) list3.append(picname) with open(r'D:\download\temp.txt', 'ab') as f3:#把列表3的数据保存在临时文件(包括新下载的图片名) pickle.dump(list3, f3) if 'gif' not in picname:#不下载gif图片 with open(picname, 'wb') as f: f.write(picdata) time.sleep(0.5) else:#如果有下载过 print(picname, '是旧图片') list3.append(picname) with open(r'D:\download\temp.txt', 'ab') as f3: pickle.dump(list3, f3) raise Stopdownload() #抛出异常def main(): try:#新建一个文件夹，如果已经存在就pass os.mkdir('D:\download\pic') except: pass os.chdir(r'D:\download\pic') #改变工作目录 mainurl = 'http://jandan.net/ooxx' #主页 print(mainurl) mainsoup = data(mainurl)#导出主页的soup try: downpic(mainsoup)#下载主页的图片，如果全部是新图片不会触发异常 except Stopdownload as e:#如果遇到旧图片则停止下载 print(e) pass else:#如果没有遇到旧图片 pageurl = getpage(mainsoup)#下面页码的地址 for x in pageurl: print(x) pagesoup = data(x)#每个页面的soup try: downpic(pagesoup) #下载每个页面的图片 except Stopdownload as e:#如果遇到旧图片立刻停止循环 print(e) break #删除旧的文件数据，把临时文件改为正式文件，储存了最新下载的图片名数据 os.remove(r'D:\download\piclist.txt') os.rename(r'D:\download\temp.txt', r'D:\download\piclist.txt')#为了是在下载图片时遇到旧图片时停止下载，而不用继续循环读取旧页面的数据，#我在下载图片的函数中使用raise语句，当出现旧图片时立刻引发异常。#在主函数中就可以捕获这个异常而中断循环。if __name__ == '__main__': main() 二.对已下载图片进行分类虽然自动下载了图片，但是是否是自己喜欢的，还要自己去判断（没有非人工智能高大上），这段代码就是实现了自动显示图片，自己再选择是否喜欢，并把图片保存在不同的文件夹里。使用了matplotlib.pyplot, PIL/Image, shutil, easygui这四个库。 在写这段代码时遇到os.chdir(&#39;C:\Users\vulcanten\Desktop\test&#39;)出现异常，原因是\在字符串中是当作转义字符来使用, 所以出现了报错，我们只要在使用路径名时在前面加上r, 就可以解决了 os.chdir(r&#39;C:\Users\vulcanten\Desktop\test&#39;)， 还可以使用\\, 或者/来代替。 12345678910111213141516171819202122232425262728293031#对下载的图片进行分类import matplotlib.pyplot as pltfrom PIL import Imageimport osimport timeimport shutilimport easygui as egimport pickleos.chdir(r'D:\download\pic')ways = os.walk(r'D:\download\pic')for each in ways: list1 = each[2] for picname in list1: img = Image.open(picname) #打开图片数据 print(picname) plt.ion()#自动滚动显示图片并执行相关脚本, 需要与pause()一起使用 plt.imshow(img)#显示图片 plt.show() plt.pause(1)#暂停等候 #使用easygui来对图片进行判断，并移动图片到对应文件夹 like = eg.buttonbox('喜欢的点OK', '', ('OK', '不喜欢')) if like == 'OK': shutil.move(picname, ''.join(['D:/download/like/', picname])) elif like == '不喜欢': shutil.move(picname, ''.join(['D:/download/unlike/', picname])) else: breakplt.close() 上图看一下效果 运行后发现这个代码运行速度比较慢，还没有使用图片查看器再结合键盘的删除键来的快。虽然如此，但代码还是提供了一些有用的思路，以备今后使用。 本文离不开以下文章的贡献： python 自定义异常和异常捕捉 创建自定义异常 python之文件操作-复制、剪切、删除等 python 图像的显示关闭以及保存 python中使用PIL和matplotlib.pyplot打开显示关闭暂停和保存图片 总结： 在函数中使用raise来引起异常，实现终止程序的功能。 自定义异常的方法 用shutil在移动或者复制文件 实现滚动显示图片的功能 写爬虫最难的是遇到反爬机制，但是自己又看不懂Jason文件。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
        <tag>requests</tag>
        <tag>BeautifulSoup</tag>
        <tag>pickle</tag>
        <tag>urllib.parse</tag>
        <tag>re</tag>
        <tag>matplotlib.pyplot</tag>
        <tag>PIL/Image</tag>
        <tag>shutil</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-百度词条副标题以及生成器的用法]]></title>
    <url>%2FSonicHuang%2F27e9abdf%2F</url>
    <content type="text"><![CDATA[我们通过输入关键字，爬取百度百科该关键字网页上的解释和其他词条的链接以及对应的副标题，最后控制显示链接的数量。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152'''本代码通过输入关键字，爬取百度百科该关键字网页上的解释和其他词条以及对应的副标题'''import requestsfrom bs4 import BeautifulSoupimport chardetimport reimport urllib.parse as up#定义一个函数返回目标网页的文本数据def ht(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; proxy = &#123;'http':'120.78.174.170:8080'&#125; response = requests.get(url = url, headers = headers, proxies = proxy) code = chardet.detect(response.content)['encoding']#response.content是返回响应的二进制数据 response.encoding = code #response.encoding可以查询编码方式，也可以通过赋值对响应进行编码 html = response.text #response.text返回响应的文本数据 return html#输入关键字查询keyword = input('输入关键字查询：')url = ''.join(['http://baike.baidu.com/search/word?word=', up.quote(keyword)])#对关键字进行编码html = ht(url)soup = BeautifulSoup(html, 'html.parser')list1 = soup.find_all('p') #在关键字没有被收录的页面，搜索结果出现在第一个p便签里面if '百度百科尚未收录词条' in list1[0].text: print(list1[0].text)else: list0 = soup.find_all('meta', attrs=&#123;'name':'description'&#125;) #找到对关键字的解释内容,meta标签，含有name='description'的属性和属性值 description = list0[0]['content'] print(description) print('下面是相关链接：') time = 0 #每次显示十行，再询问是否要继续 for tag in soup.find_all(href = re.compile('item')):#找到所有包含item属性的标签，所有含有href属性，属性值含有item字符串的标签 url = ''.join(['http://baike.baidu.com', up.unquote(tag['href'])]) subhtml = ht(url) subsoup = BeautifulSoup(subhtml, 'html.parser') subtitle = subsoup.title.string.replace('_百度百科', '')#打开词条链接并找到词条的title文本，title标签 if '（' in subtitle: #如果词条含有副标题 print(subtitle, '--&gt;', ''.join(['http://baike.baidu.com', up.unquote(tag['href'])])) else: print(tag.text, '--&gt;', ''.join(['http://baike.baidu.com', up.unquote(tag['href'])])) time += 1 if time == 10: answer = input('退出请回复: n， 其他任意键继续。请输入：') if answer == 'n': break else: time = 0 continue 我的代码直接利用了time叠加的方式对链接数量进行控制。 翻看小甲鱼老师的代码，他使用了定义多个函数的方式和生成器, 于是我把原来的代码进行了改编，并进行比较。先看代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879'''本代码通过输入关键字，爬取百度百科该关键字网页上的解释和其他词条以及对应的副标题'''import requestsfrom bs4 import BeautifulSoupimport chardetimport reimport urllib.parse as up#定义函数返回目标网页的文本数据def ht(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; proxy = &#123;'http':'120.78.174.170:8080'&#125; response = requests.get(url = url, headers = headers, proxies = proxy) code = chardet.detect(response.content)['encoding']#response.content是返回响应的二进制数据 response.encoding = code #response.encoding可以查询编码方式，也可以通过赋值对响应进行编码 html = response.text #response.text返回响应的文本数据 return html#判断是否有收录关键字def havecontent(soup): list1 = soup.find_all('p') #在关键字没有被收录的页面，搜索结果出现在第一个p便签里面 if '百度百科尚未收录词条' in list1[0].text: print(list1[0].text) return True else: return False#提取对关键字的解释def description(soup): list0 = soup.find_all('meta', attrs=&#123;'name':'description'&#125;) #找到对关键字的解释内容,meta标签，含有name='description'的属性和属性值 description = list0[0]['content'] print(description)#提取关键字网页的所有词条副标题和链接def sublinks(soup): for tag in soup.find_all(href = re.compile('item')):#找到所有包含item属性的标签，所有含有href属性，属性值含有item字符串的标签 url = ''.join(['http://baike.baidu.com', up.unquote(tag['href'])]) subhtml = ht(url) subsoup = BeautifulSoup(subhtml, 'html.parser') subtitle = subsoup.title.string.replace('_百度百科', '')#打开词条链接并找到词条的title文本，title标签 if '（' in subtitle: #如果词条含有副标题 sublinks = ''.join([subtitle, '--&gt;', 'http://baike.baidu.com', up.unquote(tag['href'])]) else: sublinks = ''.join([tag.text, '--&gt;', 'http://baike.baidu.com', up.unquote(tag['href'])]) yield sublinks #主程序def main(): #输入关键字查询 keyword = input('输入关键字查询：') url = ''.join(['http://baike.baidu.com/search/word?word=', up.quote(keyword)])#对关键字进行编码 html = ht(url) soup = BeautifulSoup(html, 'html.parser') if havecontent(soup): pass else: description(soup) print('下面是相关链接：') links = sublinks(soup)#注意这个地方需要把函数赋值给变量，才能产生一个生成器，否者只能返回函数的第一个值 time = 0 #每次显示十行，再询问是否要继续 while True: try: print(next(links))#生成器每次迭代出一个值，直到结束 time += 1 if time == 10: answer = input('退出请回复: n， 其他任意键继续。请输入：') if answer == 'n': break else: time = 0 continue except StopIteration: breakif __name__ == '__main__': main() 请注意下面两行 12links = sublinks(soup)#注意这个地方需要把函数赋值给变量，才能产生一个生成器，否者只能返回函数的第一个值print(next(links))#生成器每次迭代出一个值，直到结束 再看下面的例一 123456789101112131415161718192021222324252627282930313233343536#例一&gt;&gt;&gt; a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]&gt;&gt;&gt; def fn(): for x in a: yield x &gt;&gt;&gt; next(fn())1&gt;&gt;&gt; next(fn())1&gt;&gt;&gt; next(fn())1&gt;&gt;&gt; k = fn()&gt;&gt;&gt; print(next(k))1&gt;&gt;&gt; print(next(k))2&gt;&gt;&gt; print(next(k))3#例二&gt;&gt;&gt; def fn(): for x in a: return x&gt;&gt;&gt; print(fn())1&gt;&gt;&gt; print(fn())1&gt;&gt;&gt; print(fn())1&gt;&gt;&gt; k = fn()&gt;&gt;&gt; print(k)1&gt;&gt;&gt; print(k)1&gt;&gt;&gt; print(k)1&gt;&gt;&gt; 上面例二也说明了如果定义了一个循环不是一个生成器，那么他只能return第一个值。而文章前段代码没有用函数来定义for tag in soup.find_all(href = re.compile(&#39;item&#39;))产生的值，而在迭代循环里直接打印出了所有链接。所以这里就显示出了生成器的作用的用法。 总结： 应用BeautifulSoup通过寻找属性和属性值定位标签 正则表达式 生成器的作用的用法]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>BeautifulSoup</tag>
        <tag>request</tag>
        <tag>urllib.parse</tag>
        <tag>re</tag>
        <tag>yield</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度百科“网络爬虫”的词条]]></title>
    <url>%2FSonicHuang%2F73ee126f%2F</url>
    <content type="text"><![CDATA[第一次接触正则表达式，和BeautifulSoup，第一印象就是复杂，必须要专门抽时间深入学习才行。 今天这个脚本初步应用他们的基本功能，爬取百度百科一个词条里面的其他词条链接。脚本没有采用之前的urllib.request而是直接用了request库来读取网页数据，据说这个更强大。 12345678910111213141516171819import requestsfrom bs4 import BeautifulSoupimport chardetimport reimport urllib.parse as upurl = 'http://baike.baidu.com/view/284853.htm'agent = 'User-Agent'agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36'headers = &#123;agent:agentvalue&#125;proxy = &#123;'http':'120.78.174.170:8080'&#125;response = requests.get(url = url, headers = headers, proxies = proxy)code = chardet.detect(response.content)['encoding']#response.content是返回响应的二进制数据response.encoding = code #response.encoding可以查询编码方式，也可以通过赋值对响应进行编码html = response.text #response.text返回响应的文本数据soup = BeautifulSoup(html, 'html.parser')for tag in soup.find_all(href = re.compile('item')): print(tag.text, '--&gt;', ''.join(['http://baike.baidu.com', up.unquote(tag['href'])])) 上面第19行使用url解码把臃肿的连接地址解码成简短的包含有中文的地址，只要浏览器可以识别就没有关系。 把 http://baike.baidu.com/item//item/%E5%BC%80%E6%94%BE%E6%BA%90%E4%BB%A3%E7%A0%81 解码成：http://baike.baidu.com/item/开放源代码 参考： Requests快速上手 python之urlencode()，quote()及unquote()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>BeautifulSoup</tag>
        <tag>request</tag>
        <tag>urllib.parse</tag>
        <tag>re</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-proxy]]></title>
    <url>%2FSonicHuang%2F9df1cdd0%2F</url>
    <content type="text"><![CDATA[昨天看了小甲鱼的爬虫这章用代理和添加user agent来爬取网页的视频，对其中的内容不是十分了解，所以专门写了这篇文章来理解其中的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243'''本代码使用代理ip登录查询ip的网站来验证代理运行是否成功, 以及opener, header的使用'''import urllib.request as urimport chardet#这个网站可以显示当前的ip地址url = 'http://myip.kkcha.com/'agent = 'User-Agent'agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36'proxy = &#123;'http':'120.78.174.170:8080'&#125;#前三行定义一个特殊的opener,来使用代理或者其他来方式打开url. 如果没有定义,urlopen就是用默认的方式去打开url.proxy_support = ur.ProxyHandler(proxy)#handler我的理解就是用什么方法来handleopener = ur.build_opener(proxy_support)#opener就是生成一个工具#0 是否安装opener#ur.install_opener(opener) #安装openerresponse = opener.open(url)#如果前一行不安装opener,就用opener来打开url#response = ur.urlopen(url) #如果已经安装opener, 调用urlopen(url)就会自动用安装的opener的方式来处理#1 request参数添加header'''headers = &#123;agent:agentvalue&#125;request = ur.Request(url = url, headers = headers)response = opener.open(request)'''#2 opener添加header'''opener.addheaders = [(agent, agentvalue)]response = opener.open(url)'''#3 request添加header'''request = ur.Request(url)request.add_header(agent, agentvalue)response = opener.open(request)'''content = response.read()#response.read()运行一次后read的指针就在被读取内容的最后，如果再次调用将返回为空.所以这里把他的值赋值给一个变量, 后面直接调用变量code = chardet.detect(content)['encoding']html = content.decode(code)print(html) 测试了一下，因为代理不稳定或者其他原因，有时会返回错误信息，多试一下就会返回包含如下信息的网页信息，可以看到他的ip地址正是我们代理的地址。 总结： 需要理解handler和opener的作用，以及是否安装opener后的处理方法 可以在哪些地方添加header，以及不同的添加方法 调用read方法后的指针问题，避免重复读取后返回空值。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
        <tag>urllib.request</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为手机通讯录]]></title>
    <url>%2FSonicHuang%2F49cc418f%2F</url>
    <content type="text"><![CDATA[在恢复出厂设置时，把老爸的华为麦芒通讯录搞丢了，但老人家保留了一个比较早的一个通讯录电子表格，自己录入的格式没有固定。我就想有没有比较方便的方法，用代码写一个通讯录，再导入到手机里面。 1.华为麦芒vcard格式 如上图vcard代码格式，一个联系人有一个片段, 中文人名是通过Quoted-printable方式编码 Quoted-printable或QP encoding，没有规范的中文译名，可译为可打印字符引用编码或使用可打印字符的编码。Quoted-printable是使用可打印的ASCII字符（如字母、数字与“=”）表示各种编码格式下的字符，以便能在7-bit数据通路上传输8-bit数据, 或者更一般地说在非8-bit clean媒体上正确处理数据[注 1]。这被定义为MIME content transfer encoding，用于e-mail。 看不懂！其实更简单的是这样： URL编码后把%换成=, 或者用utf-8编码把\x换成=如下： 12345&gt;&gt;&gt; import urllib.parse as up&gt;&gt;&gt; up.quote('黄学')'%E9%BB%84%E5%AD%A6'&gt;&gt;&gt; '黄学'.encode('utf-8')b'\xe9\xbb\x84\xe5\xad\xa6' 看起来用url编码更好一点。如果几个号码，就会用多行TEL;CELL:连接，电话号码的格式为：XXX XXXX XXXX 2.Python 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105'''本程序是通过输入联系人和电话号码然后转为为华为手机vcard的vcf文件，vcf文件可以直接上传到华为手机通讯录。'''import urllib.parse as upimport pickleimport os'''定义函数返回一个联系人的字典'''def contacts(): print('|--- 欢迎进入通讯录 ---|') print('|--- 1：查找联系人 ---|') print('|--- 2：增加或修改联系人 ---|') print('|--- 3：删除联系人 ---|') print('|--- 4：显示所有通讯录 ---|') print('|--- 5：保存并退出通讯录 ---|') #从内存中的二进制文件中读取联系人信息 try: with open('contact.txt', 'rb') as f2: contact = pickle.load(f2) except:#如果内存中没有文件者则赋值为一个空的字典 contact = &#123;&#125; while 1: no = input('请选择:')#input输入的都是字符串 #查找联系人 if no == '1': name = input('输入联系人姓名:') if name in contact:#联系人存在则打印他的号码 NO = contact[name] print('号码是: %s' % NO) else: print('联系人不存在!') #添加联系人 if no == '2': name = input('输入联系人姓名:') if name in contact:#如果联系人已经存在，打印联系电话，并询问是否要修改 print(' 联系人%s 的电话是: %s ' % (name, contact[name])) change = input('修改原号码回复y, 增加号码回复z, 什么都不做回复n:') if change == 'y': NO = input('输入新号码\(多个号码请用+连接\):') contact[name] = NO elif change == 'z': NO = input('输入号码\(多个号码请用+连接\):') contact[name] += NO else: NO = contact[name] print('号码是: %s' % NO) else: NO = input('输入联系人号码:') contact[name] = NO #删除联系人 if no == '3': name = input('输入联系人姓名:') if name in contact: del(contact[name]) else: print('联系人不存在.') #显示所有通讯录 if no == '4': if contact == &#123;&#125;: print('通讯录是空的.') else: for i in contact: print(i, ':', contact[i]) #保存并退出 if no == '5': print('保存并退出.') break #写入一个二进制文件保存数据 with open('contact.txt', 'wb') as f1: pickle.dump(contact, f1) return contact #返回字典数据'''清除之前的通讯录'''try: os.remove('newdata.vcf')except: pass#华为通讯录vcard代码字符串info1 = 'BEGIN:VCARD\nVERSION:2.1\nN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;'info2 = ';;;\nFN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:'info3 = '\nTEL;CELL:'info4 = '\nEND:VCARD\n'#通过输入通讯录后导出联系人的字典数据allno = contacts()#把字典数据转化为vcard代码并写入newdata.vcf文件for i in allno: name = i #通过列表解析式删除数据里面的空格 nostr = ''.join([x for x in allno[i] if x != ' ']) nolist = nostr.split('+') #如果有多个电话号码需要每个电话分离出来组成一个列表 list2 = [] for no in nolist: #注意vcard电话号码的格式数字之间有空格 no = no[:3] + ' ' + no[3:7] + ' ' + no[7:] list2 += [info3, no] #每个电话号码会形成单独一行 name = up.quote(name).replace('%', '=') #vcard联系人的名字是经过url编码后把%换成= with open('newdata.vcf', 'a') as file: #把通讯录代码写入vcard list1 = [info1, name, info2, name] list3 = [info4] data = ''.join(list1 + list2 + list3) file.write(data) 3.测试代码1234567891011121314151617181920212223242526&gt;&gt;&gt; ===================== RESTART: E:\python files\IDEL FILES\My py file\通讯录.py =====================|--- 欢迎进入通讯录 ---||--- 1：查找联系人 ---||--- 2：增加或修改联系人 ---||--- 3：删除联系人 ---||--- 4：显示所有通讯录 ---||--- 5：保存并退出通讯录 ---|请选择:4通讯录是空的.请选择:2输入联系人姓名:sonic输入联系人号码: 123 4567 8901请选择:2输入联系人姓名:joan输入联系人号码: 23456789012 +34567890123 + 98765 432312请选择:2输入联系人姓名:黄晓明输入联系人号码:34567890133请选择:4sonic : 123 4567 8901joan : 23456789012 +34567890123 + 98765 432312黄晓明 : 34567890133请选择:5保存并退出.&gt;&gt;&gt; 生成了两个文件，一个是保存了数据，一个是需要vcard文件可以直接上传到手机通讯录 生成的vcard文件内容： 1234567891011121314151617181920BEGIN:VCARDVERSION:2.1N;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;sonic;;;FN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:sonicTEL;CELL:123 4567 8901END:VCARDBEGIN:VCARDVERSION:2.1N;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;joan;;;FN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:joanTEL;CELL:234 5678 9012TEL;CELL:345 6789 0123TEL;CELL:987 6543 2312END:VCARDBEGIN:VCARDVERSION:2.1N;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;=E9=BB=84=E6=99=93=E6=98=8E;;;FN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:=E9=BB=84=E6=99=93=E6=98=8ETEL;CELL:345 6789 0133END:VCARD 可以看到我在输入电话号码时的空格自动删除了，中文名被编码显示，英文字母没有变。代码写好了，以后再遇到相同的情况就不需要一个名字一个数字的敲了。如果有提供有格式的联系人名单，还可以直接读取名单信息自动生成vcard文件。同时，不同的手机生成的vcard文件格式略有差别，特别是在中文人名的处理方式上不同，不过都可以通过字符串的一些操作来实现。 4.通讯录vcard逆向操作如果我们得到一个vcard文件，需要马上知道里面是哪些人的电话号码就需要进行逆向操作了，这段代码不是我写的，来至记录一些最近用过的编码转换， 使用了正则表达式，还没有学到这个地方，先放在这里，后面再来研究。 123456789101112import reimport urllib.requestwith open('newdata.vcf') as file: vcf = file.read()vcf = vcf.replace('=\n=','=')names = re.findall('(=[\w=]+)',vcf)for name in names: if name[3:4]=='=': name_new = name.replace('=','%') name_new = urllib.request.unquote(name_new) vcf = vcf.replace(name,name_new,1)print(vcf) 运行后就会把上一个代码保存的文件转译过来，英文字符还是不变，只看中文名部分。 123456BEGIN:VCARDVERSION:2.1N;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;黄晓明;;;FN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:黄晓明TEL;CELL:345 6789 0133END:VCARD 参考： 廖雪峰-字符串和编码 python之urlencode()，quote()及unquote() 记录一些最近用过的编码转换 总结： 本文用了urllib.parse, quote, pickle的用法 用了try..except..语句，join, split, 切片等对字符串进行操作]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>pickle</tag>
        <tag>urllib.parse</tag>
        <tag>quote</tag>
        <tag>正则表达式</tag>
        <tag>解析式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-Markdown-Typora操作问题解决方案记录]]></title>
    <url>%2FSonicHuang%2F4e074924%2F</url>
    <content type="text"><![CDATA[学习Markdown, Git, Typora第一天开始就出现很多操作上的问题，当时解决了后来又忘记。我想有必要把每次出现的问题记录一下。这篇文章将会持续更新。 从学习Markdown, Git, Typora第一天开始就出现很多操作上的问题，当时解决了后来又忘记。我想有必要把每次出现的问题记录一下。持续更新中。。。。 一. GitGit图解： Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 1. diverged123# Your branch and &apos;origin/master&apos; have diverged,# and have 3 and 8 different commits each, respectively.# (use &quot;git pull&quot; to merge the remote branch into yours) 方法是： 12git fetch origingit reset --hard origin/master 还没有搞懂什么意思，结果是删除了工作区新的文件，恢复到到了之前的状态，状态正常。 2.Changes to be committed123456789$ git statusOn branch masterYour branch is up to date with &apos;origin/master&apos;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) deleted: EXE文件/EvenRuiDict/EvenRuiDictionaryv1.0DeveloperSonicHuang.exe deleted: EXE文件/downloadcatpicture/CatPicDownloadv1.0DeveloperSonicHuang.exe 情况：这两个文件我改了名字重新提交了，但是提示缓存区有被删除的文件 解决方法：后来明白，本地进行的删除操作也要经过add, commit, push这个程序才能把所有记录和远程上的文件删除 123$ git add EXE文件/EvenRuiDict/EvenRuiDictionaryv1.0DeveloperSonicHuang.exe$ git commit -m &apos;删除EvenRuiDictionaryv1.0DeveloperSonicHuang.exe&apos;$ git push 1234567891011$ git statusOn branch masterYour branch is up to date with &apos;origin/master&apos;.Changes not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) deleted: EXE文件/downloadcatpicture/CatPicDownloadv1.0DeveloperSonicHuang.exeno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 可以看到第一个文件已经被删除了。第二个文件通过同样的方法删除掉。 参考Git删除文件 总结：凡是在本地删除文件都要提交删除操作，才能使所有本地和远程保持一致 3. LF will be replaced by CRLF12warning: LF will be replaced by CRLF in 通讯录.py.The file will have its original line endings in your working directory. 出现上述问题是在通讯录.py中使用了回车换行CRLF，而git是LF。表示在远端仓库保持了CRLF， 但是我的py文件都是回车换行，为什么只有这个文件有提示呢？搞不懂 参考： CRLF和LF在跨平台工作时候带来的烦恼以及解决方法 Git中的“LF will be replaced by CRLF”警告详解 4. 在命令行出现&lt;符号12345678vulcanten@vulcanten-pc MINGW32 /e/python files/IDEL FILES/My py file (master)$ git commit -m &apos;add 爬虫-词条副标题.py&gt; ^Cvulcanten@vulcanten-pc MINGW32 /e/python files/IDEL FILES/My py file (master)$ git commit -m &apos;add 爬虫-词条副标题.py&gt; bash: unexpected EOF while looking for matching `&apos;&apos;bash: syntax error: unexpected end of file 如上当输入不完整，双引号，单引号没有闭合就会出现另起一行并出现&lt;符号，这是可以用Ctr+c, 或者Ctr+d退出输入 二. Markdown(记录Typora不能表达的情况)1. 在段落中插入一小段代码，使用英文输入法反引号(~键)引用例如：Python的print() 2.三. Typora1.查看markdown源代码在typora编辑器左下角可以查看]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>typora</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫 喵星人图片下载工具]]></title>
    <url>%2FSonicHuang%2Fb0f08802%2F</url>
    <content type="text"><![CDATA[爬取喵星人图片下载网站用easygui提供交互界面输入图片尺寸并下载图片到本地。难度不大，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465'''本脚本是利用easygui提供的交互程序在placekitten.com下载喵星人图片'''import urllib.request as urimport easygui as egimport os#基础信息，默认尺寸msg = '输入图片尺寸'title = '喵星人图片下载 v1.0 Developer:SonicHuang'length = '400'width = '600'while True: size = eg.multenterbox(msg, title, ['长', '宽'], [length, width]) if size != None and size[0] != '' and size[1] != '': length = size[0] width = size[1] url = ''.join(['http://placekitten.com/g/', length, '/', width]) #处理网络异常和输入错误 try: #int()的参数必须为整数 int(length) int(width) response = ur.urlopen(url) except: #提示异常信息并返回主循环重新输入 eg.msgbox( '出错了！请检查输入的数据(必须为整数)和检查网络是否连接.', title, 'OK') continue else: #直接读取图片，并保存为一个临时图片 catimage = response.read() with open('temp.jpg', 'wb') as f1: f1.write(catimage) #预览临时图片 asksave = eg.buttonbox('预览\n需要保存请点击图片', title, ('保存图片', '重新输入尺寸', '退出'), 'temp.jpg') #删除临时文件再保存图片，如果点击了图片会返回图片名 if asksave == '保存图片' or asksave == 'temp.jpg': os.remove('temp.jpg') savepath = eg.filesavebox('选择需要保存的位置', title = title, default = 'newimage.jpg', filetypes = ['*.jpg']) if savepath == None:#点击取消或者X返回None ask = eg.buttonbox('要继续下载吗?', title, ('继续', '退出')) if ask == '继续': continue else: break else: with open(savepath, 'wb') as f: f.write(catimage) ask = eg.buttonbox('要继续下载吗?', title, ('继续', '退出')) if ask == '继续': continue else: break #如果不喜欢这个图片或者尺寸，删除之前的临时图片，再重新输入尺寸 elif asksave == '重新输入尺寸': os.remove('temp.jpg') continue #退出时也要删除临时文件 else: os.remove('temp.jpg') break elif size == None:#退出 break else:#输入错误时 feedback = eg.buttonbox('输入错误，请重新输入?', title, ('重新输入', '退出')) if feedback == '重新输入': continue else: break pyinstaller打包后：参考用PyInstaller-3.4打包python程序为exe程序 下载一个封面图片 预览图片 总结： 脚本写好了初次运行没有报错只是完成了一半，剩下一半或者更多的是调试，修改。 要坚持写笔记，对于一个普通人来讲很有用，如果你是天才请忽略。 今天又写了一篇文章记录使用Git, Markdown, Typora使用问题和解决方法。真的感觉Git很复杂，要慢慢理解，遇到问题再解决问题，在实践中学习。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>easygui</tag>
        <tag>pyinstaller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫 利用Easygui和Pyinstaller写一个简单的字典]]></title>
    <url>%2FSonicHuang%2F8f8af010%2F</url>
    <content type="text"><![CDATA[基于上一篇文章《爬虫-有道词典》，我们用Easygui做一个简单的交互界面，用Pyinstaller打包成EXE文件，就可以方便传输和使用了。 1.先单独写一个easgui的脚本然后把字典脚本改成一个模块的形式导入就可以了。修改成模块的 youdao.py文件代码如下： 1234567891011121314151617181920212223242526272829'''本脚本是利用easygui的交互界面和youdao.py模块实现简单的词典功能。基于有道在线词典'''import easygui as egimport youdao as yd#定义相关参数，初始的输入框值为空白msg = '请输入你想查询的单词或句子\n支持英语，中文等'title = 'EvenRui小词典 v1.0 开发者:SonicHuang'words = ''meaning = ''while True: m = eg.multenterbox(msg, title,['请输入：', '意思是：'],[words, meaning]) if m != None and m[0] != '' and m[1] == '': #输入正确的状态是首先m的值不为None,在输入框有值，在'意思是'这一栏是空白. words = m[0]#再赋值给multenterbox的两个参数，使其同时显示单词和他的翻译结果。 meaning = yd.translate(words) m = eg.multenterbox(msg, title,['请输入：', '意思是：'],[words, meaning]) if m == None: #再点击cancel或者X者退出程序 break else: #点击ok 继续查询，参数为空白回到原始状态 words = '' meaning = '' continue else: #输入错误时，错误提示 ask1 = eg.buttonbox('输入错误，请在正确的位置重新输入。', title, ('重新查词', '退出词典')) if ask1 == '重新查词': words = '' meaning = '' continue else: break 2.把两个py文件放在同一个文件夹里，主程序为EvenRui.py, youdao.py为需要导入的模块, 然后CMD进入这个文件夹后，命令行输入pyinstaller -F -w EvenRui.py，具体使用方法参考我之前的文章 用PyInstaller-3.4打包python程序为exe程序。生成EXE文件后可以删除其他多余的文件。 3.双击EXE文件运行一切正常。4.总结： 运用导入模块的方法 运用pyinstaller,easygui]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>easygui</tag>
        <tag>pyinstaller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-有道词典]]></title>
    <url>%2FSonicHuang%2F118256ff%2F</url>
    <content type="text"><![CDATA[本文通过爬取有道词典，用python代码实现简单的翻译任务。 小甲鱼在教材中也讲了有道词典的爬取方法，但是时隔几年后的今天，有道工程师已经部署了反爬机制，对请求的表单数据进行了加密。如果不知道他们的加密方法，我们就会爬取失败，得到的数据是{‘errorCode’: 50} ， 或者请求非法等错误信息。一开始我也遭遇了同样的问题，通过反复的查资料不断的调试代码，虽然我不懂json, 也最终成功达到了我想要的目的。后面我们来看一下是怎么实现的。 我的配置： Python 3.7.0 window 7 简化版 ChromeVersion 69.0.3497.81 (Official Build) (32-bit) 1. Chrome打开有道在线翻译, 鼠标右键审查元素–Network–输入hello回车后就可以看到POST请求数据–点开左边的链接 2.点开链接后在Response我们可以看到有反馈的结果’你好’，返回到Headers就可以看到我们请求的地址Request URL 3.Headers下面图中Request Headers的Cookie, Referer, User-Agent后面的数据我们需要用到。 4.Headers最后面Form Data就是我们向服务器请求的数据，后面我们需要用python脚本把这些数据发到有道服务器。 请注意标注的四个参数，如果我们另外输入一个需要翻译的词语，会发现其中i, salt, sign, ts的数据是变化的。第二图与第一图比较就可以发现。我们需要知道变化的三个参数的规律，才能返回我们想要的结果。 5.找到定义这些参数的json文件，在Elements下面如图可以看到一个fanyi.min.js的json文件，复制他的地址在浏览器中打开，会发现很乱的文本。再复制所有文本到Json在线格式化工具,转化一下就比较清晰了。可以把转化后的文本在记事本中打开，查找salt的文本，就会发现相关的定义。 6.如下图，我们就可以找到ts, salt, sign的值。 我没有学过json以上的方法参考python3爬虫之有道翻译(上)，如有错误请指正。注意定义salt中最后一个字符串“p09@Bn{h02_BIEe]$P^nG”有可能会变化，根据实际情况而定。 7.知道表单参数的值后，我们根据小甲鱼的教材就可以编写Python脚本了。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import urllib.request as urimport urllib.parse as upimport timeimport randomimport hashlibimport json#请求的地址：在第2步中的Request URLurl = 'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule'#需要翻译的内容：content = input('Input what you want to translate:')#表单数据关键数据ts, salt, sign的计算方法client = 'fanyideskweb'ts = int(time.time() * 1000) num = random.randint(1, 10)#从1到10的随机数salt = str(ts) + str(num)flowerStr = 'p09@Bn&#123;h02_BIEe]$P^nG'sign = hashlib.md5((client + content + salt + flowerStr).encode('utf-8')).hexdigest()data = &#123;&#125;data['i'] = contentdata['from'] = 'AUTO'data['to'] = 'AUTO'data['smartresult'] = 'dict'data['client'] = clientdata['salt'] = salt #salt的值可以是时间戳和随机数数值相加的结果，也可以是对应字符串相加的结果。可以是int,也可以是strdata['sign'] = sign#sign的值是client(固定值),content(变化值),salt(变化值),固定字符串拼接后的md5值data['ts'] = ts #ts的值是一个时间戳，可以是int也可以是strdata['bv'] = '3e6546407cd5c219c8baa670735759b6' #bv是浏览器和版本信息的md5值data['doctype'] = 'json'data['version'] = '2.1'data['keyfrom'] = 'fanyi.web'data['action'] = 'FY_BY_CLICKBUTTION'data['typoResult'] = 'false'#解析表单数据并编码data = up.urlencode(data).encode('utf-8')#下面的header数据需要填入,并实例化req = ur.Request(url = url, data = data, method = 'POST')req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36')req.add_header('Referer', 'http://fanyi.youdao.com/')req.add_header('Cookie', 'OUTFOX_SEARCH_USER_ID=602357771@10.169.0.84')#请求并接受反馈信息, timeout是以秒单位确定连接的超时时间response = ur.urlopen(req, timeout = 5)#读取反馈信息并解码html = response.read().decode('utf-8')#利用json库把json数据转化为python的列表或者字典result = json.loads(html)['translateResult'][0][0]['tgt']print('The result is ==&gt;&gt; %s.' % result) Json和hashlib的用法参考：Python中的json库的简单使用 hashlib简介 总结： 没有学过json要做爬虫很被动，学爬虫本身就是被动的，昨天读了一片文章《爬虫VS反爬虫》，他们之前的内耗摧残着做这些工作的程序猿们，和消耗着网络上绝大多数的流量，浪费！所以我不打算把爬虫作为我今后主攻的方向。 一个人外部最大的敌人不是困难而是诱惑。面对诱惑，仍然坚守正道，才是高人。 知识都是相通的。要学好python，得了解很多其他方面的知识。 通过今天这段代码，了解了json, urllib.parse包，hashlib库]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>json</tag>
        <tag>urllib</tag>
        <tag>hashlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-访问网站数据并保存在对应文件]]></title>
    <url>%2FSonicHuang%2F183747d7%2F</url>
    <content type="text"><![CDATA[题目：访问网址读取数据并保存在对应的文本文件里 我的代码：12345678910111213141516171819'''此脚本通过读取urls.txt上的网站的信息，并保存在对应的文本文件里。'''import urllib.request as urimport chardet as chdef main(): with open('urls.txt') as file: urls = file.readlines()#分行读取 n = 0 for url in urls: sources = ur.urlopen(url).read() code = ch.detect(sources)['encoding']#检测网站的编码方式 html = sources.decode(code)#对网站信息进行相应的解码,decode默认的是encoding = 'utf-8'的编码 n += 1 filename = ''.join(['file_', str(n), '.txt'])#生成对应的文件 with open(filename, 'w', encoding = code) as nfile:#再用相应的编码写入对应的文件，encoding默认的编码是utf-8 nfile.write(html)if __name__ == '__main__': main() 老师的代码：123456789101112131415161718192021222324252627282930'''读取网站信息并保存在对应的文件里。老师的方法与我的相似，不同之处，读取网站信息时使用了分割换行符的方法'''import urllib.requestimport chardetdef main(): i = 0 with open("urls.txt", "r") as f: # 读取待访问的网址 # 由于urls.txt每一行一个URL # 所以按换行符'\n'分割 urls = f.read().splitlines() for each_url in urls: response = urllib.request.urlopen(each_url) html = response.read() # 识别网页编码 encode = chardet.detect(html)['encoding'] if encode == 'GB2312': encode = 'GBK' i += 1 filename = "url_%d.txt" % i #老师用了格式化，我用了join的方法 with open(filename, "w", encoding=encode) as each_file: each_file.write(html.decode(encode, "ignore")) #decode的第二个参数。对于有些字符的特殊编码方式，我们可以通过这个方式进行忽略,详细请参考后面链接。if __name__ == "__main__": main() 以下内容参考：Python中解码decode()与编码encode()与错误处理UnicodeDecodeError: ‘gbk’ codec can’t decode byte 0xab 使用python的时候经常会遇到文本的编码与解码问题，其中很常见的一种解码错误如题目所示，下面介绍该错误的解决方法，将‘gbk’换成‘utf-8’也适用。（1）、首先在打开文本的时候，设置其编码格式，如：open(‘1.txt’,encoding=’gbk’)；（2）、若（1）不能解决，可能是文本中出现的一些特殊符号超出了gbk的编码范围，可以选择编码范围更广的‘gb18030’，如：open(‘1.txt’,encoding=’gb18030’)；（3）、若（2）仍不能解决，说明文中出现了连‘gb18030’也无法编码的字符，可以使用‘ignore’属性进行忽略，如：open(‘1.txt’,encoding=’gb18030’，errors=‘ignore’)； （4）、还有一种常见解决方法为open(‘1.txt’).read().decode(‘gb18030’,’ignore’ 总结： 复习了文件的读取与写入 decode 与 encode 的应用 urllib 与 chardet的应用]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
        <tag>urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-编码检测应用]]></title>
    <url>%2FSonicHuang%2Fbd5cd896%2F</url>
    <content type="text"><![CDATA[上一篇我们安装编码检测工具chardet, 小甲鱼老师有道题要求用户输入任意网址，我们通过脚本判断出该网站使用的编码方式。 题目演示： 下面是我的代码：123456789101112131415161718192021222324252627282930'''本脚本是利用文本编码检测工具chardet检测用户输入的网站所使用的编码。'''import urllib.request as urimport chardet as ch#定义一个函数用于接收网站数据def source(): url = input('Please input the URL you want to detect: ') try:#检测用户输入的网址是否正确 content = ur.urlopen(url).read() except: print('The URL is wrong or it\'s not available.') else: return content#检测网站的编码def detect(): try:#如果用户输入的网址有问题这里就会抛出异常，为了使脚本运行正常，这里进行了处理，并提示用户。 sources = source() result = ch.detect(sources) except: print('You should restart the application again.') else: result = result['encoding'] if result == 'GB2312': result = 'GBK' print('The encoding way is %s.' % result)#如果作为单独脚本运行时if __name__ == '__main__': detect() 参考关于GB2312 和 GBK 总结： 要注意 1__name__ == '__main__': 的使用]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
        <tag>urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中安装编码检测工具chardet]]></title>
    <url>%2FSonicHuang%2Fb9b1e485%2F</url>
    <content type="text"><![CDATA[在Github建好博客后开始学习新的一章，爬虫，首先需要安装一个检测网页代码编码的工具chardet. 参考chardet官网 1. pip 安装在cmd输入命令 1pip install chardet 2. 使用detect()模块检测在IDLE 12345&gt;&gt;&gt; import urllib.request as ur&gt;&gt;&gt; respond = ur.urlopen('https://sonichuang.github.io')&gt;&gt;&gt; import chardet&gt;&gt;&gt; chardet.detect(respond.read())&#123;'encoding': 'utf-8', 'confidence': 0.99, 'language': ''&#125; 意思百分之九十九是utf-8, 还可以检测字符串的语言，如：中文，英文等]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Github博客加入博客背景 评论功能以及修改侧栏]]></title>
    <url>%2FSonicHuang%2F2be1fd41%2F</url>
    <content type="text"><![CDATA[一.加入博客背景1. 找到主博客\themes\next\source\css_custom下custom.styl文件，原始文件里面是空白的，用记事本打开然后添加如下文本。 123456789101112131415// Custom styles.@media screen and (min-width:1200px) &#123; body &#123; background-image:url(/images/backgroundpicturename.jpg); background-repeat: no-repeat; background-attachment:fixed; background-position:50% 50%; background-size: cover &#125; #footer a &#123; color:#eee; &#125;&#125; 注意上面url后面括号里面内容。 2.把需要的背景图片放在\themes\next\source\images文件夹里然后把图片名称填入步骤1，url后面backgroundpicturename,注意扩展名例子是jpg, 要根据实际情况修改。3. 部署后就可以看到我们的背景了。我把图片进行了修改，在图片编辑器里面修改他的像素为电脑屏幕的实际像素值。 4. 如果感觉背景图片会挡住博客上的字可以调节，图片的透明度。在步骤1的文件中下面加入如下代码，12345//background color and opacity.main-inner &#123; background: #fff; opacity: 0.9;&#125; background后面的值是颜色代码，这里是白色。opacity是指透明度。这里可以根据需要进行调整。 二. 加入评论功能1.网上推荐使用LiveRe, 在其主页申请一个免费帐号，再Install(安装)，city(一般网站)就可以得到一串代码注意data-uid后面引号里面的字符串。 2.再打开主目录下\themes\next的配置文件_config.yml文件找到如下内容，然后在your uid后面填上步骤1的字符串，注意不要引号，并去掉livere uid前面的#。部署好就可以了。 三. 修改侧栏目录的显示在打开某一个文章的时候，侧栏老是在显示（下图一）如果想把隐藏掉。可以这样修改，在主目录下主题的配置文件\themes\next， _config.yml中sidebar display中在post值那里最前面加上#, 再去掉hide前的#, 就可以了（如下图二） 总结：修改博客背景，添加评论功能，修改侧栏目录的显示方式]]></content>
      <categories>
        <category>github pages</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
        <tag>github pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在Typora中插入本地图片和加入Readmore按钮]]></title>
    <url>%2FSonicHuang%2F58b77a70%2F</url>
    <content type="text"><![CDATA[第一部分：插入本地图片我现在遇到的问题是：在用Typora编辑MD时，插入本地图片后，不能在网页上显示, 只能看到空白的方框。 我的博客是用Hexo搭建，使用next主题，用的windows系统，苹果电脑有个专门的软件叫ipic听说很好用，但是windows却没有。图片不能显示的原因是：本地插入的图片在Typora本地是图片的绝对地址，部署到网站后，还是显示的绝对路径，很明显，系统找不到这个地址，也就不能显示图片了。解决的方法在网上也是很多，比如安装插件，修改配置文件等等，我觉得太复杂，其实最简单的办法只要简单的几步: 1. 在博客根目录下&gt;source下新建一个assets文件夹。 2.在Typora&lt;File&lt;Preference选择如下： a. 第一个拷贝图片到指定文件夹b. 第二个选择步骤1的assets文件夹，意思是拖放到Typora中的图片会自动复制到这个assets文件夹。c. 第三，四指适用于拖入的本地图片和网络图片都会复制到assets文件夹d. 第五，使用相对路径3. 在Typora&lt;Edit&lt;Image Tools&lt;Use Image Root Path在这里选择储存图片的根目录，定位到步骤1的source文件夹，不能定位到放图片的assets文件夹，我之前就把这步弄错了，导致网站上不能显示图片。定位好后会在Typora的MD文件头里面出现如图的语句： 4.设置好了后就就可以插入图片了，显示的是图片的相对路径。 5. 部署后就能在网站正确显示了。第二部分：在博客首页加入Readmore按钮有两个办法： 1. 手动添加（推荐），在MD文中的适当位置重启一行纯文本写入如下图这一行，系统就会自动在首页显示之前的内容 2. 自动添加。在主题文件夹的配置文件\themes\next下面_config.yml，中找到如下代码： 实际上官方推荐使用手动加入Readmore按钮。因为自动添加会让文章看起来不人性化, 会出现突然在某个地方咔嚓一下断掉的感觉。另外如果同时设置的手动和自动，会优先在手动的地方显示。 总结：1. 网络上的方法不一定适合自己，所以需要不断试错。2. 不要轻易放弃。3. 本文章解决了Typora编写MD后部署到Github后图片显示不正常的问题以及加入Readmore按钮的方法。本文参考：关于Typora和hexo图片路径的问题]]></content>
      <categories>
        <category>github pages</category>
      </categories>
      <tags>
        <tag>typora</tag>
        <tag>hexo</tag>
        <tag>next</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[更新安卓平板固件]]></title>
    <url>%2FSonicHuang%2F379f2134%2F</url>
    <content type="text"><![CDATA[本篇文章单纯为了记录安卓平板固件的更新过程。 之前为了让小孩子的英语作业能够一天多做一点，而寻求通过一些方法修改安卓app的对于时间上的限制，想了很多办法想让平板ROOT后，获取更多的权限。但是最终下载下来的固件，通过Odin重装后机子都无法正常启动。最后看到网上有人讲SM T700的平板升级到这个安卓版本6.0.1后几乎没有办法ROOT，而且后来得知App是部署在服务器上的，没有办法通过修改App的程序改变限制。 但是现在机子几乎变砖，怎么也要把它救过来。 1. 下载平板对应版本的固件。固件下载地址 2. 下载Odin官方下载地址 3. 在AP处添加步骤1的文件，其他参数不用修改。4. 按住平板电源键，音量下键，和Home键几秒中就可以看到下载模式，再按音量上键就开始启动下载。5. 用数据线连接平板和电脑，然后点击Odin3， Start.6. 直到平板安装完成重启，Odin3显示Pass，结束。 结束语：1.第一次使用Typora编辑MD文档，感觉很方便，比想象的更方便。但是现在还有一个问题没有解决，就是在Typora中插入本地的图片然后部署后能够正确的显示。2. 三星平板真难用，按照上面的步骤更新后发现还是启动不了，后来再启动Recovery模式，按住电源键，音量上键和Home。恢复到出厂设置才解决问题。3. 遇到困难的时候，在将要放弃的时候再坚持一下，问题就解决了。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>android</tag>
        <tag>ROM</tag>
        <tag>Record</tag>
      </tags>
  </entry>
</search>
