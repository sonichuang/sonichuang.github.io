<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[识别《冰与火》第一部的英语卡片]]></title>
    <url>%2FSonicHuang%2Fa3cf63b1%2F</url>
    <content type="text"><![CDATA[看完权力游戏后我买了一套原著，用了差不多6个月读完第一本。第二本还在继续拜读中。。 从79页开始遇到不认识的单词都有用有道词典查询并保存了单词卡片如图，这是我保存的第一张英语卡片. 我就想以后可以把这些单词卡可以整理出来复习一下这些单词。 第一个问题：我想知道在这本书中我查询的单词次数最多的是哪些，这些单词就是我最需要记住的了。 要解决这个问题，我想需要分以下几步： 用python识别出单个单词卡的单词 利用遍历，循环识别出所有单词，并记录重复单词的次数 列出top榜单 第1步：用python识别出单个单词卡的单词 查了一些资料最终使用百度人工智能（免费）中的文字识别现有的API接口 1.注册百度云进入文字识别，有百度账号的直接登录：https://cloud.baidu.com/product/ocr， 然后点创建应用后就可以得到如图的三个参数 2.然后查看技术文档的python部分 里面的内容浅显易懂，现在用的的接口是通用文字识别，实测高精度版在识别这些单词上没有通用文字识别准确，下面举例说明。 然后用pip安装baidu api ,代码：pip install baidu-aip 如果遇到出错可以多试几次就可以了。 通用文字识别的代码： 12345678910111213141516171819from aip import AipOcr""" 你的 APPID AK SK """APP_ID = '你的 App ID'API_KEY = '你的 Api Key'SECRET_KEY = '你的 Secret Key'client = AipOcr(APP_ID, API_KEY, SECRET_KEY)""" 读取图片 """def get_file_content(filePath): with open(filePath, 'rb') as fp: return fp.read()image = get_file_content('example.jpg')""" 调用通用文字识别, 图片参数为本地图片 """client.basicGeneral(image) #如果是高精度版代码为client.basicAccurate(image) 这样得到的是一个包含图片信息的字典，以第一张图片为例： 123image = get_file_content('2019-05-08 234422.png')wordsinfo = client.basicGeneral(image)print(wordsinfo) 结果为： 12&#123;'log_id': 2967069863786893502, 'words_result_num': 4, 'words_result': [&#123;'words': 'labyrinth'&#125;, &#123;'words': "/'laeberIne/"&#125;, &#123;'words': 'n.迷宫;[解剖]迷路;难解的事物'&#125;, &#123;'words': '道网易有道词典'&#125;]&#125;&gt;&gt;&gt; 如果用高精版： 123image = get_file_content('2019-05-08 234422.png')wordsinfo = client.basicAccurate(image)print(wordsinfo) 结果为： 12&#123;'log_id': 7443256712844240862, 'words_result_num': 4, 'words_result': [&#123;'words': ' abyrinth'&#125;, &#123;'words': " /'laeberin0/"&#125;, &#123;'words': 'n.迷宫;[解剖]迷路;难解的事物'&#125;, &#123;'words': '有道网易有道词典'&#125;]&#125;&gt;&gt;&gt; 可以看出单词labyrinth通用基础版辨别的单词为：{&#39;words&#39;: &#39;labyrinth&#39;}， 而高精度版为：{&#39;words&#39;: &#39; abyrinth&#39;}，很明显后者少了一个‘l’, 但是音标都不怎么准确，这个问题后面再查资料看能不能解决。就目前我们需要的信息看，暂时用通用版比较准确。 到这里第1步就很简单解决了，要得到最终的单词只要读取这个字典就可以了 123wordsineed = wordsinfo['words_result']englishwords = wordsineed[0]['words']print(englishwords) 结果： 12labyrinth&gt;&gt;&gt; 未完待续。。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>百度云</tag>
        <tag>英语卡片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeplearning 笔记：梯度法]]></title>
    <url>%2FSonicHuang%2F1195dc17%2F</url>
    <content type="text"><![CDATA[4. Deep Learning From Scratch4.4 梯度法 通过梯度法求函数最小值时，参数的值 12345678910111213141516171819202122232425262728293031323334353637# coding: utf-8import numpy as npimport matplotlib.pylab as pltfrom gradient_2d import numerical_gradient#计算函数f值最小时，x的值def gradient_descent(f, init_x, lr=0.01, step_num=100): x = init_x #初始值 x_history = [] #定义一个空的列表 for i in range(step_num): x_history.append( x.copy() ) #拷贝x的值后，就不会随着原x的变化而变化 grad = numerical_gradient(f, x) #求梯度值(斜率), 得到一个二维数组 x -= lr * grad #沿梯度变化的方向减少，函数值相应的减少 return x, np.array(x_history) #返回最后x的值以及他的变化轨迹def function_2(x): return x[0]**2 + x[1]**2init_x = np.array([-3.0, 4.0]) lr = 0.1step_num = 20x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)plt.plot( [-5, 5], [0,0], '--b')plt.plot( [0,0], [-5, 5], '--b')plt.plot(x_history[:,0], x_history[:,1], 'o')plt.xlim(-3.5, 3.5)plt.ylim(-4.5, 4.5)plt.xlabel("X0")plt.ylabel("X1")plt.show() 后记： 1.在书中求函数的极小值，但是我觉得这样是不对的，因为gradient_descent返回的值是x和它的历史记录, 而不是f(x)的值。所以应该是f(x)取最小时，x的值。 2.另外在运行这个代码的时候遇到一个问题，matplotlib的某一个模块找不到了。之前也出现过这样类似的情况，是因为杀毒软件把一些模块给屏蔽掉了，到软件中找的这些模块，把模块返回原来的位置，再添加信任就可以了。但是这次没有在杀毒软件中找到，所以就用pip卸载了matplotlib然后在重装就可以了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>python</tag>
        <tag>梯度</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeplearning 笔记：meshgrid函数 梯度]]></title>
    <url>%2FSonicHuang%2F8126b86f%2F</url>
    <content type="text"><![CDATA[2.Numpy2.18 meshgrid函数 参考文章Numpy中Meshgrid函数介绍及2种应用场景 [xv,yv] = meshgrid(x,y) 将向量x和y定义的区域转换成矩阵xv和yv,其中矩阵xv的行向量是向量x的简单复制，而矩阵yv的列向量是向量y的简单复制(注：下面代码中xv和yv均是数组，在文中统一称为矩阵了)。假设x是长度为m的向量，y是长度为n的向量，则最终生成的矩阵xv和yv的维度都是 nm （注意不是mn）。 参考上图再看下面的代码，x, y为一维数组 12345678910111213141516171819202122232425&gt;&gt;&gt; x = np.arange(3)&gt;&gt;&gt; xarray([0, 1, 2])&gt;&gt;&gt; y = np.arange(2)&gt;&gt;&gt; yarray([0, 1])&gt;&gt;&gt; xv, yv = np.meshgrid(x, y)&gt;&gt;&gt; xv #x保持横轴数据不变，在纵轴上复制的次数为y的数据个数，本例中为复制2次array([[0, 1, 2], #最终得到的数组shape都为(2, 3) [0, 1, 2]])&gt;&gt;&gt; yv #如下保持数据在纵轴上的不变，复制的次数为x的数据个数，本例为复制3次array([[0, 0, 0], [1, 1, 1]])&gt;&gt;&gt; z = [i for i in zip(xv.flat,yv.flat)]&gt;&gt;&gt; z[(0, 0), (1, 0), (2, 0), (0, 1), (1, 1), (2, 1)]&gt;&gt;&gt; x.shape(3,)&gt;&gt;&gt; y.shape(2,)&gt;&gt;&gt; xv.shape(2, 3)&gt;&gt;&gt; yv.shape(2, 3)&gt;&gt;&gt; 4. Deep Learning From Scratch4.2 函数求导教材中求f(x)=0.01x**2 + 0.1x，当x=5时的导数，以及它的切线函数 12345678910111213141516171819202122232425262728293031# coding: utf-8import numpy as npimport matplotlib.pylab as plt#求f(x)的导数，f为函数def numerical_diff(f, x): h = 1e-4 # 0.0001 return (f(x+h) - f(x-h)) / (2*h)#要求导数的函数f(x)def function_1(x): return 0.01*x**2 + 0.1*x #求x在f(x)上的导数，以及对应的切线函数.下面的推导过程参考下面的图片def tangent_line(f, x): d = numerical_diff(f, x) print(d) y = f(x) - d*x #y为切线函数与纵轴的交点。我不是非常理解为什么是f(x)-d*x而不是后者减去前者，这个正负问题。 return lambda t: d*t + y x = np.arange(0.0, 20.0, 0.1)y = function_1(x)plt.xlabel("x")plt.ylabel("f(x)")tf = tangent_line(function_1, 5)y2 = tf(x)plt.plot(x, y)plt.plot(x, y2)plt.show() 推导过程： 4.3 梯度这段代码，我主要分析了梯度的计算过程，而没有分析它绘图的过程 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970# coding: utf-8# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3import numpy as npimport matplotlib.pylab as pltfrom mpl_toolkits.mplot3d import Axes3D#求一维数组在函数中的导数def _numerical_gradient_no_batch(f, x): h = 1e-4 # 0.0001 grad = np.zeros_like(x) #生成一个同x形状，元素全为0的数组，grad同x在这里是一维数组 for idx in range(x.size): tmp_val = x[idx] #导出一维数组中每个元素并赋值给tmp_val x[idx] = float(tmp_val) + h #然后再在这个元素上加上一个微小数，并修改x数组的对应元素 fxh1 = f(x) # f(x+h) #把x数组代入f(x)函数中得到一个值 x[idx] = tmp_val - h #同理求出同一元素减去微小数后数组的代入函数后得到的值 fxh2 = f(x) # f(x-h) grad[idx] = (fxh1 - fxh2) / (2*h) #得到一个导数，再导入grad同一位置。 x[idx] = tmp_val # 还原值 #将x还原 return grad #这样得到每个元素对应的导数组成的一维数组#2维数组在函数f的求导def numerical_gradient(f, X): if X.ndim == 1: #如果参数X是一维数组 return _numerical_gradient_no_batch(f, X) #求一维数组的导数 else: grad = np.zeros_like(X) #如果是2维数组，则grad生成对应的元素为0的2维数组 for idx, x in enumerate(X): #idx为列的下标，x为idx列下标下每一行的数组为一维数组 grad[idx] = _numerical_gradient_no_batch(f, x) #每一行的数组生成对应的导数组成的一维数组，再导入grad对应下标列的每行 return grad #最终得到一个完整的2维数组#函数def function_2(x): if x.ndim == 1: return np.sum(x**2) else: #2维数组中求横轴方向的数组平方和 return np.sum(x**2, axis=1)#求x在函数中的导数和对应的切线函数，在这段代码中似乎没有用上def tangent_line(f, x): d = numerical_gradient(f, x) print(d) y = f(x) - d*x return lambda t: d*t + y if __name__ == '__main__': x0 = np.arange(-2, 2.5, 0.25) x1 = np.arange(-2, 2.5, 0.25) X, Y = np.meshgrid(x0, x1) #参考文章前面meshgrid函数，x0, y0均为一维数组，shape均为(18, ),所以X, Yshape均为(18, 18)二维数组 X = X.flatten() #把X, Y分别改写为一维数组 Y = Y.flatten() grad = numerical_gradient(function_2, np.array([X, Y]) ) #这里np.array([X, Y])是二维数组 plt.figure() plt.quiver(X, Y, -grad[0], -grad[1], angles="xy",color="#666666")#,headwidth=10,scale=40,color="#444444") plt.xlim([-2, 2]) plt.ylim([-2, 2]) plt.xlabel('x0') plt.ylabel('x1') plt.grid() plt.legend() plt.draw() plt.show() 后记： 1.最近业余时间带小孩，电脑又罢工了一段时间，学习进度变得很慢。 2.感觉数学知识越来越多了，越来越复杂了。 3.还得一步一个脚印向前走。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>python</tag>
        <tag>梯度</tag>
        <tag>numpy</tag>
        <tag>meshgrid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeplearning 笔记：MNIST数据集初体验]]></title>
    <url>%2FSonicHuang%2F3224e5c7%2F</url>
    <content type="text"><![CDATA[2.Numpy2.17 dtype, astype 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&gt;&gt;&gt; a = np.array([-1, 1]) &gt;&gt;&gt; b = np.array([-2, 2]) &gt;&gt;&gt; a.dtype #dtype查看数据类型dtype('int32')&gt;&gt;&gt; b.dtypedtype('int32')&gt;&gt;&gt; a.astype(np.int16) #astype转换数据类型 参数np.int16与直接'int16'一样的结果 array([-1, 1], dtype=int16)&gt;&gt;&gt; a.astype('int16') #astype当参数为signed符号整数时,转换结果没有发生变化array([-1, 1], dtype=int16) &gt;&gt;&gt; b.astype('int16') array([-2, 2], dtype=int16) &gt;&gt;&gt; a.astype('int8') array([-1, 1], dtype=int8)&gt;&gt;&gt; b.astype('int8') array([-2, 2], dtype=int8)&gt;&gt;&gt; a.astype('uint8') #astype当参数为unsigned无符号整数时，-1则为对应数据类型uint8所表示的最大值255，-2则对应的减去1 array([255, 1], dtype=uint8)&gt;&gt;&gt; b.astype('uint8') array([254, 2], dtype=uint8)&gt;&gt;&gt; a #astype数据类型转换并不会改变原始数据的值，除非再次赋值给同一个变量如：a = a.astype('int8')array([-1, 1])&gt;&gt;&gt; b array([-2, 2])&gt;&gt;&gt; a.dtype dtype('int32')&gt;&gt;&gt; b.dtype dtype('int32')&gt;&gt;&gt;&gt;&gt;&gt; a = np.array([-1, 1, 2, 3]) &gt;&gt;&gt; b = np.array([-2, 4, 5, 6]) &gt;&gt;&gt; a.shape (4,)&gt;&gt;&gt; b.shape (4,)&gt;&gt;&gt; a.dtype dtype('int32')&gt;&gt;&gt; b.dtype dtype('int32')&gt;&gt;&gt; a.dtype = 'int16' # 对dtype进行定义则改变原来的数据类型，用新的数据类型来表示（其改变原理还没有搞清楚） &gt;&gt;&gt; a.shape #int32是4个字节的宽度，int16是2个字节的宽度，所以从原来的4个元素变成8个元素 (8,)&gt;&gt;&gt; a array([-1, -1, 1, 0, 2, 0, 3, 0], dtype=int16) #为什么会变成这些元素目前还没有搞清楚&gt;&gt;&gt; a = np.array([-1, 1, 2, 3]) &gt;&gt;&gt; a.dtype = 'int64' #64位是8个字节，相应的元素从32位的4个元素变成2个元素 &gt;&gt;&gt; a.shape (2,)&gt;&gt;&gt; a array([ 8589934591, 12884901890], dtype=int64) #为什么会变成这些元素目前还没有搞清楚&gt;&gt;&gt; b = np.array([-2, 4, 5, 6]) #dtype下面为什么用不同的数据类型符号整数和无符号整数结果是一样？目前还不清楚，估计跟数字大小有关系&gt;&gt;&gt; b.dtype = 'int64' &gt;&gt;&gt; b array([21474836478, 25769803781], dtype=int64)&gt;&gt;&gt; b = np.array([-2, 4, 5, 6]) &gt;&gt;&gt; b.dtype = 'uint64' #跟int64是相同的数组&gt;&gt;&gt; b array([21474836478, 25769803781], dtype=uint64)&gt;&gt;&gt; a = np.array([-1, 1]) &gt;&gt;&gt; a.dtype = 'int16' &gt;&gt;&gt; a array([-1, -1, 1, 0], dtype=int16) #与uint16是不同的数组，实际上等于转换成uint16的值&gt;&gt;&gt; a = np.array([-1, 1]) &gt;&gt;&gt; a.dtype = 'uint16' &gt;&gt;&gt; a array([65535, 65535, 1, 0], dtype=uint16) 另外如果从浮点数用astype转换成整数或者无符号整数如下： 12345678910111213&gt;&gt;&gt; a = np.array([-1.23])&gt;&gt;&gt; a.dtypedtype('float64')&gt;&gt;&gt; a.astype('int64') #去掉了小数部分array([-1], dtype=int64)&gt;&gt;&gt; a.astype('uint64') #同上面一样-1转换为对应uint64表示的最大值array([18446744073709551615], dtype=uint64)&gt;&gt;&gt; a.astype('int32') #符号整数可以为负，所以没有变化array([-1])&gt;&gt;&gt; a.astype('uint32') #同理2的（32-1）次方array([4294967295], dtype=uint32)&gt;&gt;&gt; a.astype(np.uint32) array([4294967295], dtype=uint32) 3. enumerate(), Python的内置函数用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。enumerate(sequence, [start=0]) 1234567891011121314151617181920212223&gt;&gt;&gt; list1 = ["这", "是", "一个", "测试"]&gt;&gt;&gt; for index, item in enumerate(list1, 1): print(index, item) 1 这2 是3 一个4 测试&gt;&gt;&gt; for index, item in enumerate(list1): print(index, item) 0 这1 是2 一个3 测试&gt;&gt;&gt; for index, item in enumerate(list1, -1): print(index, item) -1 这0 是1 一个2 测试&gt;&gt;&gt; enumerate用于统计文件行数 123456# method 1count = len(open(filepath, 'r').readlines())# method 2count = -1for index, line in enumerate(open(filepath, 'r')): count += 1 4. Deep Learning From Scratch4.1 Mnist数据集初体验经过一段时间的Numpy的学习，上一周又回到书本上，把之前学过的数据集的内容又看了一遍。第一遍的时候，不知所云，主要是因为Numpy知识的缺乏，第二遍后，终于理解了书中代码的内容。下面是书中源码的备注和自己的理解。 第一个源码：mnist.py 目的是下载训练和测试数据，并转化为数组，然后保存在本地pickle文件里面 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136# coding: utf-8try: import urllib.requestexcept ImportError: raise ImportError('You should use Python 3.x')import os.pathimport gzipimport pickleimport osimport numpy as np#数据下载地址url_base = 'http://yann.lecun.com/exdb/mnist/'#对应的文件类型和其被压缩后的文件名组成的字典，包含训练图像，训练标签，测试图像，测试标签key_file = &#123; 'train_img':'train-images-idx3-ubyte.gz', 'train_label':'train-labels-idx1-ubyte.gz', 'test_img':'t10k-images-idx3-ubyte.gz', 'test_label':'t10k-labels-idx1-ubyte.gz'&#125;dataset_dir = os.path.dirname(os.path.abspath(__file__))#os.path.abspath(__file__)表示当前py文件的绝对路径，os.path.dirname表示其所在的文件夹save_file = dataset_dir + "/mnist.pkl"train_num = 60000test_num = 10000img_dim = (1, 28, 28)img_size = 784#下载压缩文件def _download(file_name): #参数是压缩文件名 file_path = dataset_dir + "/" + file_name #文件的绝对路径 if os.path.exists(file_path): #如果该压缩文件已经存在则退出 return print("Downloading " + file_name + " ... ") urllib.request.urlretrieve(url_base + file_name, file_path) #下载远程文件到本地指定地址和文件名 print("Done")#下载数据 def download_mnist(): for v in key_file.values(): #迭代出压缩文件名 _download(v) #下载压缩文件#将标签压缩文件转化为数组（一维，uint8）def _load_label(file_name): file_path = dataset_dir + "/" + file_name print("Converting " + file_name + " to NumPy Array ...") with gzip.open(file_path, 'rb') as f: labels = np.frombuffer(f.read(), np.uint8, offset=8) #注意这里offset=8，生成一维数组(label的个数, ) print("Done") return labels#将图片压缩文件转化为数组（二维，uint8）def _load_img(file_name): file_path = dataset_dir + "/" + file_name #压缩文件的绝对路径 print("Converting " + file_name + " to NumPy Array ...") with gzip.open(file_path, 'rb') as f: #gzip模块用于压缩和解压缩文件 data = np.frombuffer(f.read(), np.uint8, offset=16) #frombuffer把提供的数据流转化为对应dtype类型，从offset位置开始读取的一维数组。offset为什么要设置为16呢？ data = data.reshape(-1, img_size) #-1表示根据系统设定，只要第一维是img_size。转化后0维为图片个数，1维为图片的像素值. (-1, 784) print("Done") return data#转化成数组，文件类型和ta对应的数组组成的字典def _convert_numpy(): dataset = &#123;&#125; dataset['train_img'] = _load_img(key_file['train_img']) dataset['train_label'] = _load_label(key_file['train_label']) dataset['test_img'] = _load_img(key_file['test_img']) dataset['test_label'] = _load_label(key_file['test_label']) return dataset#初始化数据,把文件下载到本地并读出里面的数据信息并保存在文件里def init_mnist(): download_mnist() #下载数据，从网上地址直接下载压缩文件到本地 dataset = _convert_numpy() #文件类型和ta对应的数组组成的字典 print("Creating pickle file ...") #把字典数据保存在文件mnist.pkl里面 with open(save_file, 'wb') as f: pickle.dump(dataset, f, -1) print("Done!")#把标签保存为仅正确解标签为1， 其余为0的数组。参数X为训练或者测试标签的一维数组def _change_one_hot_label(X): T = np.zeros((X.size, 10)) #生成一个二维数组，X的每个元素为一行10个0的数组,及X的第0个元素对应T的第0行数组（10个0组成），以此类推 for idx, row in enumerate(T):#遍历数组， row[X[idx]] = 1 #X标签（一维数组）的第0个元素对应的值设为n，T第0行数组的第n个元素设置为1，以此类推。也就是说X标签是什么值，ta对应的那行数组（T里面的对应行）的标签（序列号）就为1 return T def load_mnist(normalize=True, flatten=True, one_hot_label=False): """读入MNIST数据集 Parameters ---------- normalize : 将图像的像素值正规化为0.0~1.0 one_hot_label : one_hot_label为True的情况下，标签作为one-hot数组返回 one-hot数组是指[0,0,1,0,0,0,0,0,0,0]这样的数组 flatten : 是否将图像展开为一维数组 Returns ------- (训练图像, 训练标签), (测试图像, 测试标签) """ if not os.path.exists(save_file): #如果不存在save_file文件夹 init_mnist() #则初始化数据 with open(save_file, 'rb') as f:#导出pickle文件的数据，数据是文件类型和其对应的数组组成的字典 dataset = pickle.load(f)#使图片数据正规化 if normalize: #如果这个参数为True, 那么把图片（二维，uint8）数据转化为float32, 并除以255,使其图片数据正规化为0.0-1.0的值 for key in ('train_img', 'test_img'): dataset[key] = dataset[key].astype(np.float32) dataset[key] /= 255.0 if one_hot_label:#把标签转化为one-hot数组 dataset['train_label'] = _change_one_hot_label(dataset['train_label']) dataset['test_label'] = _change_one_hot_label(dataset['test_label']) if not flatten: #如果flatten为False，图片数据则不转为(-1, 1, 28, 28)，为True则数据结构不变为(-1, 784) for key in ('train_img', 'test_img'): dataset[key] = dataset[key].reshape(-1, 1, 28, 28) return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) if __name__ == '__main__': init_mnist() 第二个源码：neuralnet_mnist.py 目的是利用训练好的数据，测试判断手写字的准确率 123456789101112131415161718192021222324252627282930313233343536373839404142434445# coding: utf-8import sys, ossys.path.append(os.pardir) # 为了导入父目录的文件而进行的设定import numpy as npimport picklefrom dataset.mnist import load_mnistfrom common.functions import sigmoid, softmax#从load_minist中得到所有训练和测试数据，正则化。这里只需要测试数据def get_data(): (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False) return x_test, t_test#从保存的训练数据中，导出每层的权重的偏置数据。一个字典变量def init_network(): with open("sample_weight.pkl", 'rb') as f: network = pickle.load(f) return network#从字典变量中导出对应键的值，并进行数据计算，最后得到x的概率值y。network是权重偏置数据，x是输入的图片数据 def predict(network, x): W1, W2, W3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3'] a1 = np.dot(x, W1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, W2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, W3) + b3 y = softmax(a3) return yx, t = get_data() #正则化的测试数据。标签是一维数据(标签个数10000, )，图片是二维数据(图片个数10000, 像素值784)network = init_network() #从训练数据中得到权重偏置数据的字典变量accuracy_cnt = 0for i in range(len(x)): #len(x)=10000 y = predict(network, x[i]) #x[i]就是导出每张图片的像素数据，最终得到每个图片的概率值y p= np.argmax(y) # 从10个神经元中获取概率最高的元素的索引，就是该测试结果的值 if p == t[i]: #t[i]是这个测试图片正确值，如果测试结果是正确的，那么就算测试正确一次 accuracy_cnt += 1print("Accuracy:" + str(float(accuracy_cnt) / len(x))) #计算正确率 第三个源码：neuralnet_mnist_batch.py 目的是对数据进行批量处理 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# coding: utf-8import sys, ossys.path.append(os.pardir) # 为了导入父目录的文件而进行的设定import numpy as npimport picklefrom dataset.mnist import load_mnistfrom common.functions import sigmoid, softmaxdef get_data(): (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False) return x_test, t_testdef init_network(): with open("sample_weight.pkl", 'rb') as f: network = pickle.load(f) return networkdef predict(network, x): w1, w2, w3 = network['W1'], network['W2'], network['W3'] b1, b2, b3 = network['b1'], network['b2'], network['b3'] a1 = np.dot(x, w1) + b1 z1 = sigmoid(a1) a2 = np.dot(z1, w2) + b2 z2 = sigmoid(a2) a3 = np.dot(z2, w3) + b3 y = softmax(a3) return yx, t = get_data()network = init_network()batch_size = 100 # 批数量accuracy_cnt = 0for i in range(0, len(x), batch_size): x_batch = x[i:i+batch_size] #100个图片为一批，进行批处理shape是(100, 784) y_batch = predict(network, x_batch) #每次生成一个二维数组shape是(100, 10) p = np.argmax(y_batch, axis=1) #在轴上第一维上最大概率的索引组成一维数组，shape是(100,) accuracy_cnt += np.sum(p == t[i:i+batch_size]) #np.sum(bool), 统计True的个数，也就是正确的个数print("Accuracy:" + str(float(accuracy_cnt) / len(x))) 初体验以及遗留的问题： 如果没有Numpy基础理解这些源代码会比较难。 识别手写字是个很神奇的事情，也可以通过数组之间的运算来实现。 问题1：怎么设置frombuffer函数的offset参数，在源码中要设置为16或者8呢？ 问题2：为什么MNIST的神经网络中隐藏层的神经元个数要设置成50和100，而不设置成其他？ 问题3：怎么通过第一层每个神经元的像素值输入就可以判断数字是什么，这是什么原理？]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>python</tag>
        <tag>numpy</tag>
        <tag>MNIST</tag>
        <tag>deep learning from scratch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeplearning 笔记：数组转置 empty_like tile 广播等]]></title>
    <url>%2FSonicHuang%2F9a3bb40b%2F</url>
    <content type="text"><![CDATA[2. Numpy2.13 数组转置数组转置就是对数组的维度相互调整。可以用到三个方法：transpose函数，T属性，和swapaxes函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145&gt;&gt;&gt; a = np.arange(24)&gt;&gt;&gt; aarray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])&gt;&gt;&gt; np.transpose(a) #对于一位数组，转置之后不变array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])&gt;&gt;&gt; b = a.reshape(3, 8)&gt;&gt;&gt; barray([[ 0, 1, 2, 3, 4, 5, 6, 7], [ 8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 21, 22, 23]])&gt;&gt;&gt; c = np.transpose(b) #二维数组，数组形状倒置，从(3, 8)到(8, 3)&gt;&gt;&gt; carray([[ 0, 8, 16], [ 1, 9, 17], [ 2, 10, 18], [ 3, 11, 19], [ 4, 12, 20], [ 5, 13, 21], [ 6, 14, 22], [ 7, 15, 23]])&gt;&gt;&gt; c.shape(8, 3)&gt;&gt;&gt; b.T #T属性就是让数组形状倒置 b.T与np.transpose(b)是相等的array([[ 0, 8, 16], [ 1, 9, 17], [ 2, 10, 18], [ 3, 11, 19], [ 4, 12, 20], [ 5, 13, 21], [ 6, 14, 22], [ 7, 15, 23]])&gt;&gt;&gt; d = a.reshape(2, 3, 4)&gt;&gt;&gt; darray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]])&gt;&gt;&gt; e = np.transpose(d) #transpose倒置数组的形状从(2, 3, 4)到(4, 3, 2)&gt;&gt;&gt; earray([[[ 0, 12], [ 4, 16], [ 8, 20]], [[ 1, 13], [ 5, 17], [ 9, 21]], [[ 2, 14], [ 6, 18], [10, 22]], [[ 3, 15], [ 7, 19], [11, 23]]])&gt;&gt;&gt; e.shape(4, 3, 2)&gt;&gt;&gt; d.T #同理d.T与np.transpose(d)是相等的array([[[ 0, 12], [ 4, 16], [ 8, 20]], [[ 1, 13], [ 5, 17], [ 9, 21]], [[ 2, 14], [ 6, 18], [10, 22]], [[ 3, 15], [ 7, 19], [11, 23]]])&gt;&gt;&gt; f = np.transpose(d, (1, 0, 2)) #同时，transpose还可以指定倒置的方法，(1, 0, 2)是指把d的形状(2, 3, 4)按照索引元组(1, 0, 2)倒置及倒置后为形状为(3, 2, 4)&gt;&gt;&gt; farray([[[ 0, 1, 2, 3], [12, 13, 14, 15]], [[ 4, 5, 6, 7], [16, 17, 18, 19]], [[ 8, 9, 10, 11], [20, 21, 22, 23]]])&gt;&gt;&gt; f.shape(3, 2, 4)&gt;&gt;&gt; d.reshape(3, 2, 4) #但是倒置的形状并不是源数组的reshape, d.reshape(3,2,4) != np.transpose(d, (1, 0, 2))array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7]], [[ 8, 9, 10, 11], [12, 13, 14, 15]], [[16, 17, 18, 19], [20, 21, 22, 23]]])&gt;&gt;&gt; g = d.swapaxes(0, 1) #函数swapaxes指定两个维度的倒置，从d的(2, 3, 4)倒置(0, 1), 倒置后为(3, 2, 4)&gt;&gt;&gt; garray([[[ 0, 1, 2, 3], [12, 13, 14, 15]], [[ 4, 5, 6, 7], [16, 17, 18, 19]], [[ 8, 9, 10, 11], [20, 21, 22, 23]]])&gt;&gt;&gt; h = np.swapaxes(d, 0, 1) #或者这样的写法也可以&gt;&gt;&gt; harray([[[ 0, 1, 2, 3], [12, 13, 14, 15]], [[ 4, 5, 6, 7], [16, 17, 18, 19]], [[ 8, 9, 10, 11], [20, 21, 22, 23]]])&gt;&gt;&gt; i = d.swapaxes(1, 2) #119，120行的结果也是一样的&gt;&gt;&gt; j = d.transpose(0, 2, 1)&gt;&gt;&gt; iarray([[[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]], [[12, 16, 20], [13, 17, 21], [14, 18, 22], [15, 19, 23]]])&gt;&gt;&gt; jarray([[[ 0, 4, 8], [ 1, 5, 9], [ 2, 6, 10], [ 3, 7, 11]], [[12, 16, 20], [13, 17, 21], [14, 18, 22], [15, 19, 23]]])&gt;&gt;&gt; i.shape(2, 4, 3)&gt;&gt;&gt; j.shape(2, 4, 3)&gt;&gt;&gt; 2.14 empty_like, ones_like, zeros_like, full_like, empty123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475&gt;&gt;&gt; aarray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])&gt;&gt;&gt; a = a.reshape((2, 3, 4))&gt;&gt;&gt; aarray([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]])&gt;&gt;&gt; b = np.empty_like(a) #生成一个与a同一形状的随机数组&gt;&gt;&gt; barray([[[ 87213328, 4304144, 1667845468, 1869836146], [1130132582, 1953528178, 1634887535, 1551460464], [1634100548, 1937009781, 1869762652, 1701079414]], [[1766677618, 1936683619, 544499311, 1869771859], [1126197102, 1953528178, 1634887535, 1667852400], [1869762592, 1701079414, 738197618, 1141389320]]])&gt;&gt;&gt; c = np.ones_like(a) #生成一个同a形状的由1组成的数组&gt;&gt;&gt; carray([[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]])&gt;&gt;&gt; d = np.zeros_like(a) #生成一个同a形状的由0组成的数组&gt;&gt;&gt; darray([[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]])&gt;&gt;&gt; e = np.full_like(a, 8) #生成一个同a形状，指定参数组成的数组&gt;&gt;&gt; earray([[[8, 8, 8, 8], [8, 8, 8, 8], [8, 8, 8, 8]], [[8, 8, 8, 8], [8, 8, 8, 8], [8, 8, 8, 8]]])&gt;&gt;&gt; e = np.full_like(a, True)&gt;&gt;&gt; earray([[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]], [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]])&gt;&gt;&gt; e = np.full_like(a, False)&gt;&gt;&gt; earray([[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]])&gt;&gt;&gt; np.empty([2, 3]) #生成一个指定形状，指定数据类型的的随机，随机数组array([[2.67276450e+185, 1.69506143e+190, 1.75184137e+190], [9.48819320e+077, 1.63730399e-306, 0.00000000e+000]])&gt;&gt;&gt; np.empty([2, 2], dtype = int)array([[1734934692, 1912645992], [ 50, 977492218]])&gt;&gt;&gt; np.empty([2, 2], dtype = int)array([[167, 1], [ 0, 0]]) 2.15 numpy.tile(A, reps)把数组A沿着指定的维度方向复制，reps是一个元组或者整数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&gt;&gt;&gt; aarray([[0, 1, 2], [3, 4, 5]])&gt;&gt;&gt; b = np.tile(a, 2) #一维(x轴)的方向的两倍, 从(2, 3)变为(2, 6)&gt;&gt;&gt; barray([[0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5]])&gt;&gt;&gt; c = np.tile(a, (2, 1)) #先x轴一倍(及不变)，再y轴两倍,从(2, 3)变为(4, 3)&gt;&gt;&gt; carray([[0, 1, 2], [3, 4, 5], [0, 1, 2], [3, 4, 5]])&gt;&gt;&gt; d = np.tile(a, (1, 2, 3)) #x轴3倍， y轴2倍， z轴1倍&gt;&gt;&gt; darray([[[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5], [0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]]])&gt;&gt;&gt; e = np.tile(a, 3) # x轴3倍，from a (2, 3) to e (2, 9)&gt;&gt;&gt; earray([[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]])&gt;&gt;&gt; f = np.tile(e, (2, 1)) #y轴2倍, from e (2, 9) to f (4, 9)&gt;&gt;&gt; farray([[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5], [0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]])&gt;&gt;&gt; f.shape(4, 9)&gt;&gt;&gt; g = np.tile(f, (1, 1, 1)) # z轴1倍，from f (4, 9) to g (1, 4, 9)&gt;&gt;&gt; garray([[[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5], [0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]]])&gt;&gt;&gt; g.shape(1, 4, 9)&gt;&gt;&gt; h = np.tile(a, (2, 2, 3)) # from a (2, 3) to h (2, 4, 9)&gt;&gt;&gt; harray([[[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5], [0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]], [[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5], [0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]]])&gt;&gt;&gt; i = np.tile(f, (2, 1, 1)) # from f (4, 9) to i (2, 4, 9) 等于h = np.tile(a, (2, 2, 3))&gt;&gt;&gt; iarray([[[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5], [0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]], [[0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5], [0, 1, 2, 0, 1, 2, 0, 1, 2], [3, 4, 5, 3, 4, 5, 3, 4, 5]]])&gt;&gt;&gt; h.shape(2, 4, 9)&gt;&gt;&gt; i.shape(2, 4, 9)&gt;&gt;&gt; 2.16 广播 broadcasting理解了2.15， tile函数后，理解广播的规则就简单了 两个数组广播的规则是： 如果数组不具有相同的rank，则将较低等级数组的形状添加1，直到两个形状具有相同的长度。 如果两个数组在维度上具有相同的大小，或者如果其中一个数组在该维度中的大小为1，则称这两个数组在维度上是兼容的。 如果数组在所有维度上兼容，则可以一起广播。 广播之后，每个阵列的行为就好像它的形状等于两个输入数组的形状的元素最大值。 在一个数组的大小为1且另一个数组的大小大于1的任何维度中，第一个数组的行为就像沿着该维度复制一样 看例子： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.arange(9)&gt;&gt;&gt; a = a.reshape(3, 3)&gt;&gt;&gt; b = np.arange(6)&gt;&gt;&gt; b = b.reshape(2, 3, 1)&gt;&gt;&gt; aarray([[0, 1, 2], [3, 4, 5], [6, 7, 8]])&gt;&gt;&gt; a.shape #我们可以理解为3x3,(3, 3)&gt;&gt;&gt; barray([[[0], [1], [2]], [[3], [4], [5]]])&gt;&gt;&gt; b.shape #我们可以理解为2x3x1(2, 3, 1)#当a（2维）,b（3维）规则1，两个数组进行广播时，不具有相同的维度，较低等级数组a的形状添加1，及理解为a:1x3x3#规则2，a:1x3x3, b:2x3x1,a,b在y轴上都为3，这个维度上具有相同的大小，或者a,b在x轴上分别为3和1，在z轴上分别为1和2，其中都有一个数组维度的大小为1，这样就成为两个数组在维度上是兼容的。#规则3 a,b符合广播的条件#规则4 a:1x3x3, b:2x3x1 在广播时，每个轴上取最大值，x轴取3，y轴取3，z轴取2，则a,b广播后的形状是2x3x3,(2, 3, 3)&gt;&gt;&gt; c = a + b &gt;&gt;&gt; c # c就是广播后的结果，不光是加，减法，乘除等都符合这一规则array([[[ 0, 1, 2], [ 4, 5, 6], [ 8, 9, 10]], [[ 3, 4, 5], [ 7, 8, 9], [11, 12, 13]]])&gt;&gt;&gt; c.shape(2, 3, 3)#下面是运算过程#规则5 a:1x3x3, b:2x3x1，c:2x3x3,实际上我们看到每个轴上取最大值，那么对应较小（为1）的维度就要复制为最大值的大小。看下面详细解释&gt;&gt;&gt; d = np.tile(a, (2, 1, 1)) #取最大值后（广播后）为2x3x3,那么a:1x3x3就要在z轴上复制为它的2倍，我们可以用tile函数进行复制&gt;&gt;&gt; darray([[[0, 1, 2], [3, 4, 5], [6, 7, 8]], [[0, 1, 2], [3, 4, 5], [6, 7, 8]]])&gt;&gt;&gt; e = np.tile(b, 3) #同理 b:2x3x1,广播后为2x3x3,及b在x轴上复制为3倍&gt;&gt;&gt; earray([[[0, 0, 0], [1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4], [5, 5, 5]]])&gt;&gt;&gt; f = d + e #实际上a,b的运算就是d与e的运算。注意这里不是dot()的运算方法&gt;&gt;&gt; farray([[[ 0, 1, 2], [ 4, 5, 6], [ 8, 9, 10]], [[ 3, 4, 5], [ 7, 8, 9], [11, 12, 13]]])&gt;&gt;&gt; f.shape(2, 3, 3)&gt;&gt;&gt;]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>python</tag>
        <tag>numpy</tag>
        <tag>数组装置</tag>
        <tag>tile</tag>
        <tag>broadcasting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeplearning 笔记：索引 切片 where,eye,reshape等函数,轴，秩的含义，整数数组索引]]></title>
    <url>%2FSonicHuang%2F8ea5a68%2F</url>
    <content type="text"><![CDATA[2. Numpy2.6 索引, 切片 12345678910&gt;&gt;&gt; aarray([[ 0, 10, 20, 30, 40], [50, 60, 70, 80, 90]])&gt;&gt;&gt; b = a[0, 3] #第0行，第3列&gt;&gt;&gt; b30&gt;&gt;&gt; c = a[1, 2] #第1行，第2列&gt;&gt;&gt; c70&gt;&gt;&gt; 12345&gt;&gt;&gt; a = np.array([[11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25], [26, 27, 28 ,29, 30], [31, 32, 33, 34, 35]]) 2.7 where()函数，按照维度先后，列出满足条件的索引值12345678910111213141516171819202122&gt;&gt;&gt; a = np.array([[11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25], [26, 27, 28 ,29, 30], [31, 32, 33, 34, 35]])&gt;&gt;&gt; b = np.where(a&lt;=20)&gt;&gt;&gt; b #b的shape是(5, 5), 先按照第一个维度5行，第一行的索引都为0，从11-15都为0， 从16-20都为1(array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1], dtype=int32), array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4], dtype=int32)) #再按照第二个维度5列，从11-15为0-4，从16-20为0-4组成一个2维数组&gt;&gt;&gt; c = np.where(a&lt;=30)&gt;&gt;&gt; c(array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3], dtype=int32), array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4], dtype=int32))&gt;&gt;&gt; d = np.where(a&lt;=29)&gt;&gt;&gt; d(array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3], dtype=int32), array([0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3], dtype=int32))&gt;&gt;&gt; e = np.where(a&lt;=29)[0]&gt;&gt;&gt; earray([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3], dtype=int32) 再看一个例子 12345678910111213141516171819&gt;&gt;&gt; a = np.arange(20)&gt;&gt;&gt; a = np.reshape(a, (2, 5, 2))&gt;&gt;&gt; aarray([[[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7], [ 8, 9]], [[10, 11], [12, 13], [14, 15], [16, 17], [18, 19]]])&gt;&gt;&gt; b = np.where(a&lt;=13)&gt;&gt;&gt; b #第一维2，从0-9都为0， 10-19都为1。第二维5，0-1为0，2-3为1，4-5为2以此类推，10-11为0，12-13为1，第三维2，其中0,2,4,5,6,8,10,12为0，1,3,5,7,9,11,13为1，这样按照先后顺序组成一个3为数组。(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1], dtype=int32), array([0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 0, 0, 1, 1], dtype=int32), array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], dtype=int32))&gt;&gt;&gt; np.shape(b)(3, 14) 2.8 eye()函数numpy.eye(N,M=None,k=0,dtype=&lt;class &#39;float&#39;&gt;,order=&#39;C)返回的是一个二维的数组(N,M)，对角线的地方为1，其余的地方为0. 当M等于N时及相当于：identity(n,dtype=None) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; a = np.eye(3) #M默认等于N,此例返回一个(3,3)的二维数组.k默认等于0表示主对角线，及第一个1的下标为0，往右移动为正，往左移动为负&gt;&gt;&gt; aarray([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])&gt;&gt;&gt; b = np.eye(3, M=3, k=0) #等于a&gt;&gt;&gt; barray([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])&gt;&gt;&gt; c = np.eye(4, M=5, k=0) #主对角线&gt;&gt;&gt; carray([[1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.]])&gt;&gt;&gt; c = np.eye(4, M=5, k=1) #k=1,向右移动一个位置&gt;&gt;&gt; carray([[0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.]])&gt;&gt;&gt; d = np.eye(4, M=5, k=2) #k=2, 主对角线向右移动两个位置&gt;&gt;&gt; darray([[0., 0., 1., 0., 0.], [0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 0., 0.]])&gt;&gt;&gt; e = np.eye(4, M=5, k=3)&gt;&gt;&gt; earray([[0., 0., 0., 1., 0.], [0., 0., 0., 0., 1.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]])&gt;&gt;&gt; f = np.eye(4, M=5, k=4)&gt;&gt;&gt; farray([[0., 0., 0., 0., 1.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]])&gt;&gt;&gt; g = np.eye(4, M=5, k=-1) #k=-1,主对角线想左移动一个位置&gt;&gt;&gt; garray([[0., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.], [0., 0., 1., 0., 0.]])&gt;&gt;&gt; h = np.eye(4, M=5, k=-2)&gt;&gt;&gt; harray([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [1., 0., 0., 0., 0.], [0., 1., 0., 0., 0.]])&gt;&gt;&gt; i = np.eye(4, M=5, k=-3)&gt;&gt;&gt; iarray([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [1., 0., 0., 0., 0.]])&gt;&gt;&gt; j = np.eye(4, M=5, k=-4) #k=-4,向左移出了范围，所以都为0&gt;&gt;&gt; jarray([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]])&gt;&gt;&gt; 2.9 reshape()参数中的-1来源：https://www.zhihu.com/question/52684594/answer/297441394 举个简单的例子，要记住，python默认是按行取元素 1c = np.array([[1,2,3],[4,5,6]]) 输出： [[1 2 3][4 5 6]] 我们看看不同的reshape 12345678910print &apos;改成2行3列:&apos;print c.reshape(2,3)print &apos;改成3行2列:&apos;print c.reshape(3,2)print &apos;我也不知道几行，反正是1列:&apos;print c.reshape(-1,1)print &apos;我也不知道几列，反正是1行：&apos;print c.reshape(1,-1)print &apos;不分行列，改成1串&apos;print c.reshape(-1) 输出为： 改成2行3列:[[1 2 3][4 5 6]]改成3行2列:[[1 2][3 4][5 6]]我也不知道几行，反正是1列:[[1][2][3][4][5][6]]我也不知道几列，反正是1行：[[1 2 3 4 5 6]]不分行列，改成1串[1 2 3 4 5 6] 一串是啥意思？一串就是秩rank()为0的矩阵～ (参看后面部分的3.0的解释) 2.10 NumPy中的维度(dimension)、轴(axis)、秩(rank)的含义来源：https://zhuanlan.zhihu.com/p/51200424 在学习NumPy的时候，其中最重要的就是学习它的 ndarray 对象，它是多维度的同数据类型的数组。这个和Python自带的列表有较大的区别，列表中的元素类型是可以不相同的，如一个列表中，它可以包含数字、字符、字符串等，而在数组中，它的数据类型是相同的，如都是整型或者浮点型。 为什么Python中已经有了列表之后，在NumPy中还要引进一个数组对象呢？有以下三点可以作为参考，但在本文中不做具体描述： 数组对象可以去掉元素间运算所需的循环，使一维向量更像单个数据 设置专门的数组对象，经过优化，可以提升这类应用的运算速度 数组对象采用相同的数据类型，有助于节省运算和存储空间 NumPy中有几个概念比较绕，对于我来说比较难理解，因此以此文作为记录。它们分别是：维度、轴、秩。 对于维度的介绍，官网是这么写的“ In NumPy dimensions are called axes”，即维度称为轴。为了更直观的理解，可以将其与现实世界联系起来，比如在平面中即二维的世界中，我们描述一个点的时候，通常使用 x 轴、y 轴，这样就能确定一个点的具体位置了。因此，这里的两个维度，也就跟两个轴对应了起来。如果是立体的三维世界中，我们就会多出一个z轴，以此更加准确的来反映点的位置。所以，我么可以把以上的维度和轴进行等价。 什么是秩(rank)？它是指轴的数量，或者维度的数量，是一个标量 在下面的例子中，有一个数组 [1,2,1]， 它的维度是1，也就是有一个轴，这个轴的长度是3，而它的秩也为1。这些信息，都可以通过NumPy提供的数组属性来获得。 ndarray.ndimthe number of axes (dimensions) of the array秩，数组轴的数量，或者维度的数量 ndarray.shapethe dimensions of the array. This is a tuple of integers indicating the size of the array in each dimension. For a matrix with n rows and m columns, shape will be (n,m). The length of the shape tuple is therefore the number of axes, ndim.数组的维度。它的返回值是一个元组，这个元组描述了每个维度中数组的大小。相对于一个矩阵来说，shape表示的就是n行m列。这个元组的长度，等价于轴/维度的个数，即秩的值 依旧以下图为例，我们已经知道数组a的维度数是1。查看它的shape，返回值为元组(3,) ，从这里也可以反映出这个数组的基本形状，即它是1维的，在这1维的空间中，拥有3个数据。 现在我们升级为一个2维的数组。如下图中的数组b，从属性 ndim 中可以知道，它的秩为2，即轴的个数是2，或者维度的数量是2（行和列两个维度）。shape中反映出来的是，它是2行3列的一个矩阵。 当前轴的数量已经上升为2，那么NumPy中是怎么定义这个轴的方向的呢？在二维中，轴0表示了数组的行，轴1表示了数组的列，见下方的示意图。因此，在该列中轴0的长度是2，轴1的长度是3。 如果我们把轴0上的数进行相加，可以得到一个一维数组，值为[5,7,9]，数组的元素个数为3。 如果我们把轴1上的数进行相加，也可以得到一个一维数组，但值为[6, 15]，数组的元素个数为2。 所以在不同轴上进行数据操作会得到不同的值，数组中的值是由沿轴方向上的数据相加所得。 现在，我们再升级一个维度，使其成为一个3维数组，如下所示。秩的值已经变为3，它的shape反映出它的轴0长度为2，轴1长度为2，轴2的长度为3。 轴0-轴2，他们具体是怎么表现的呢？参见下面的图示，从图中可以看到，NumPy对于轴的编号由外向内，从行到列。 我们通过求和运算来验证上面轴方向的猜测是否正确。 先计算轴0上的数值，从示意图中看更像是，从表面到内部的一次“叠加”操作。这样计算后的一个结果，应该是一个2行3列的一个数组，形状如下： [12, 14, 16] [18, 20, 22] 再计算轴1上的数值，从示意图中可以看出，这像是从上到下的一次“叠加”操作，计算后的值也是一个2行3列的新数组，形状如下： [5, 7, 9] [25, 27, 29] 继续计算轴2上的数值，这次像是一次“右移叠加”的操作，形成的结果是一个2行2列的新数组，形状如下： [6, 15] [36, 45] 以下是运行结果，和我们的计算结果一致 对于3维及更高维度的数组，理解及计算起来比较复杂。因此，我们也可以采用降维的方法来进行计算，如果维度降到二维，那么就非常便于我们的理解，简单的说就是“替换、降维打击”。 依旧以上面的三维数组c为例，我们只关心最外层的两个维度，将最内层维度的数据看成一个整体。 转化后的形状就如下图所示，这时我们计算轴0就等于计算 A+C，B+D；计算轴1就等于计算 A + B，C + D 运行结果如下所示 轴2要怎么计算呢？由于我们上面已经将最内层的数组看成了一个整体，如图所示。所以，我们计算轴2，就等同于对数组 A、B、C、D进行分别求和 2.11 整数索引与切片索引混合使用1234567891011121314151617&gt;&gt;&gt; a #a是(4, 5)两个维度array([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])&gt;&gt;&gt; b = a[1:3, 3] #整数索引与切边索引混用会产生一个低维度的数组&gt;&gt;&gt; barray([ 8, 13])&gt;&gt;&gt; b.shape #b是一个维度(2,)&gt;&gt;&gt; c = a[1:3, 3:4] #而只是用切片索引，会产生同原始数组一样维度的数组&gt;&gt;&gt; carray([[ 8], [13]])&gt;&gt;&gt; c.shape(2, 1)&gt;&gt;&gt; 继续看一个例子： 12345678910111213141516171819&gt;&gt;&gt; barray([ 8, 13])&gt;&gt;&gt; carray([[ 8], [13]])&gt;&gt;&gt; a[1, 3]8&gt;&gt;&gt; a[1, 3] = 88 #修改a数组的一个元素，b,c对应的元素也会一起改变。据说是共用统一内存的原因&gt;&gt;&gt; aarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 88, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])&gt;&gt;&gt; barray([88, 13])&gt;&gt;&gt; carray([[88], [13]])&gt;&gt;&gt; 2.12 整数数组索引12345678910111213141516171819&gt;&gt;&gt; aarray([[ 0, 1, 2, 3, 4], [ 5, 6, 7, 88, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]])&gt;&gt;&gt; a[[0, 1, 2], [0, 1, 2]] #整数数组通过每个维度分别定位，第一个维度[0, 1, 2]0行，1行，2行，然后分别对应相应的列0行的0列，1行的1列，2行的2列array([ 0, 6, 12])&gt;&gt;&gt; np.array([a[0, 0], a[1, 1], a[2, 2]])array([ 0, 6, 12])&gt;&gt;&gt; a[[3, 3], [4, 4]] #同理对应3行的4列 和 3行的4列array([19, 19])&gt;&gt;&gt; np.array([a[3, 4], a[3, 4]])array([19, 19])&gt;&gt;&gt; a[[0, 1, 2], [0, 1, 2]] += 10 #可以快速的计算&gt;&gt;&gt; aarray([[10, 1, 2, 3, 4], [ 5, 16, 7, 88, 9], [10, 11, 22, 13, 14], [15, 16, 17, 18, 19]])]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>python</tag>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的网络电台]]></title>
    <url>%2FSonicHuang%2Fe9456ec%2F</url>
    <content type="text"><![CDATA[一直以来喜欢听网络电台，占用内存少，而且内容丰富，不断更新。 最好用的收听软件是VLC, 手机上使用简直是神器，没有广告，支持的格式很多，简单实用。我也用过sPlayer, 但由于不支持mms，https也没有反应，所以弃用。 打开VLC后直接在网络串流输入地址，然后打开就可以收听了。下面的栏目长按可以重命名，或者拷贝地址。 由于串流不是VLC的主要功能，所以它的缺点是当你删除一个栏目时，会出现后面的栏目名称出现混乱，所以最好自己备份一个我这样的地址列表。 下面的地址是我个人喜欢的电台，通过很长的时间收集而来，都可以在一般的网络环境下流畅收听. 收集方法是搜索网站上的网络电台，通过Chrome的审查元素查到的实际播放地址。 清晨音乐台: mms://radio.qingc.net/live 青苹果音乐台: mms://mms.qpgfm.com/ting 音乐之声MusicRadio: http://rtmpcnr003.cnr.cn/live/yyzs/playlist.m3u8 RADIO IQ: https://wvtf.streamguys1.com/wvtf-edge/radioiq-mp3-64/icecast.audio Relax FM http://ic4.101.ru:8000/a200?userid=0&amp;setst=3kfb3gtnch9hom0niji6jfnf4j Greatest Hit Radio http://107.181.227.250:8256/;stream.nsv Radio Monte Carlo http://node-07.zeno.fm/4fx3k355puquv Smooth Jazz : http://144.217.158.59:5176/;stream.nsv Smack Urban : http://shannon1.serverroom.net:9602/; Z108 Dance/Pop: http://node-01.zeno.fm/w69adnmq0y5tv Big B Radio Cpop: http://64.71.79.181:6065/stream ABC News Live: https://abclive2-lh.akamaihd.net/i/abc_live11@423404/master.m3u8 z108 http://edge1-b.exa.live365.net/a77298 KISS UK https://stream-kiss.planetradio.co.uk/kissnational.aac?direct=true&amp;listenerid=d6bad1510412759ed6b76350f8bb65a9&amp;amsparams=playerid%3ABMUK_html5%3Bskey%3A1561730582%3B&amp;awparams=loggedin%3Afalse%3B&amp;aw_0_req.gdpr=true Backup WVTF Classic https://wvtf.streamguys1.com/wvtf-edge/wvtfmusic-mp3-128/icecast.audio CBSN https://www.cbsnews.com/common/video/dai_prod.m3u8 BYUTV https://byubhls-i.akamaihd.net/hls/live/267187/byutvhls/master.m3u8 北京流行音乐台 http://123.56.16.201:1935/live/fm974/96K/tzwj_video.m3u8 岷江音乐 http://satellitepull.cnr.cn/live/wxscmjyyt/playlist.m3u8 Big B Radio Asian pop http://217.116.9.142:9149/stream Instrumental Hits Radio http://149.56.195.94:8453/stream Russia Relax music https://listen1.myradio24.com/6262 CAPITAL FM http://media-sov.musicradio.com/CapitalSouthCoast?amsparams=playerid:UKRP;skey:1561731372&amp;aw_0_req.gdpr=true&amp;aw_0_1st.playerid=UKRP&amp;aw_0_1st.ts=1561731272&amp;nmcuid=e1622235f00c3db144de07c59e35339c&amp;aw_0_1st.nmc=%5B%22198432%22%2C%22199681%22%2C%22199689%22%2C%22317581%22%2C%22332533%22%2C%22333664%22%2C%22335062%22%2C%22346919%22%2C%22379732%22%5D&amp;amsparams=playerid:UKRP;skey:1561731372&amp;aw_0_req.gdpr=true&amp;aw_0_1st.playerid=UKRP&amp;aw_0_1st.ts=1561731272&amp;nmcuid=e1622235f00c3db144de07c59e35339c&amp;aw_0_1st.nmc=%5B%22198432%22%2C%22199681%22%2C%22199689%22%2C%22317581%22%2C%22332533%22%2C%22333664%22%2C%22335062%22%2C%22346919%22%2C%22379732%22%5D&amp;amsparams=playerid:UKRP;skey:1561731375&amp;aw_0_req.gdpr=true&amp;aw_0_1st.playerid=UKRP&amp;aw_0_1st.ts=1561731272&amp;nmcuid=e1622235f00c3db144de07c59e35339c&amp;aw_0_1st.nmc=%5B%22198432%22%2C%22199681%22%2C%22199689%22%2C%22317581%22%2C%22332533%22%2C%22333664%22%2C%22335062%22%2C%22346919%22%2C%22379732%22%5D&amp;amsparams=playerid:UKRP;skey:1561731394&amp;listenerid=d6bad1510412759ed6b76350f8bb65a9&amp;awparams=companionAds%3Atrue SMOOTH MUSIC UK http://media-the.musicradio.com/SmoothLondon?amsparams=playerid:UKRP;skey:1561731788&amp;aw_0_req.gdpr=true&amp;aw_0_1st.playerid=UKRP&amp;aw_0_1st.ts=1561731760&amp;nmcuid=e1622235f00c3db144de07c59e35339c&amp;aw_0_1st.nmc=%5B%22193254%22%2C%22198432%22%2C%22199681%22%2C%22199689%22%2C%22317581%22%2C%22332533%22%2C%22332606%22%2C%22333664%22%2C%22335062%22%2C%22346919%22%2C%22379732%22%2C%22379747%22%5D&amp;amsparams=playerid:UKRP;skey:1561731788&amp;aw_0_req.gdpr=true&amp;aw_0_1st.playerid=UKRP&amp;aw_0_1st.ts=1561731760&amp;nmcuid=e1622235f00c3db144de07c59e35339c&amp;aw_0_1st.nmc=%5B%22193254%22%2C%22198432%22%2C%22199681%22%2C%22199689%22%2C%22317581%22%2C%22332533%22%2C%22332606%22%2C%22333664%22%2C%22335062%22%2C%22346919%22%2C%22379732%22%2C%22379747%22%5D&amp;amsparams=playerid:UKRP;skey:1561731800&amp;listenerid=d6bad1510412759ed6b76350f8bb65a9&amp;awparams=companionAds%3Atrue UK POSTCODE: SO16 3UB 下面的地址是我老爸手机上的收听地址： 相声广播 http://rtmpcnr023.cnr.cn/live28/xsxp/playlist.m3u8 中国之声 http://rtmpcnr001.cnr.cn/live/zgzs/playlist.m3u8 老年之声 http://rtmpcnr011.cnr.cn/live28/lnzs/playlist.m3u8 经济之声 http://rtmpcnr002.cnr.cn/live28/jjzs/playlist.m3u8 相声小品 http://rtmpcnr023.cnr.cn/live28/xsxp/playlist.m3u8 四川新闻广播 http://satellitepull.cnr.cn/live/wxscjjgb/playlist.m3u8 北京怀旧金曲广播 http://cnlive.cnr.cn/hls/bjhjjqgb.m3u8]]></content>
      <tags>
        <tag>internet Radio</tag>
        <tag>vLC</tag>
        <tag>music</tag>
        <tag>news</tag>
        <tag>talkshow</tag>
        <tag>english</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deeplearning 笔记：Numpy dot,cumsum,花式索引,布尔屏蔽等]]></title>
    <url>%2FSonicHuang%2F0%2F</url>
    <content type="text"><![CDATA[1. os.abspath(__file__)表示当前所在py文件的绝对路径，注意这个函数不能单独在命令行运行必须要在py文件里面。os.path.dirname()表示其所在的文件夹 2. Numpy2.1 numpy.ndarray()标题中的函数就是numpy的构造函数，我们可以使用这个函数创建一个ndarray对象。构造函数有如下几个可选参数： 参数 类型 作用 shape int型tuple 多维数组的形状 dtype data-type 数组中元素的类型 buffer 用于初始化数组的buffer offset int buffer中用于初始化数组的首个数据的偏移 strides int型tuple 每个轴的下标增加1时，数据指针在内存中增加的字节数 order ‘C’ 或者 ‘F’ ‘C’:行优先；’F’:列优先 参考numpy中 C order与F order的区别是什么？ 1. order参数的C和F是numpy中数组元素存储区域的两种排列格式，即C语言格式和Fortran语言格式。 创建一个3×3的2维数组 123&gt; import numpy as np&gt; a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)&gt; 数组a在内存中的数据存储区域中存储方式(默认order=”C”，其中一个格子是4bytes)： 12&gt; |1|2|3|4|5|6|7|8|9|&gt; 在C语言中当第一维数组也就是第0轴的下标增加1时，元素在内存中的地址增加3个元素的字节数，在此例中也就是12bytes，从1的地址增加12bytes到4的地址。此时 12&gt; a.strides = (12, 4)&gt; 若以F order创建数组： 12&gt; b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32, order="F")&gt; 数组b在内存中的数据存储区域中的存储方式： 12&gt; |1|4|7|2|5|8|3|6|9|&gt; 在Fortran语言中，第一维也就是第0轴下标增加1时，元素的地址增加一个元素字节数，在此例子中也就是4 bytes，从1的地址增加4bytes到4的地址。 12&gt; b.strides = (4, 12)&gt; 2.2 多维数组在内存中的存储顺序问题。 以一个二维数组a[2][2]为例，在C语言中，其在内存中存储为 12&gt; a[0][0] a[0][1] a[1][0] a[1][1]&gt; 而在Fortran语言中，其顺序为 12&gt; a[0][0] a[1][0] a[0][1] a[1][1]&gt; 实例： 123456np.ndarray(shape=(2,3), dtype=int, buffer=np.array([1,2,3,4,5,6,7]), offset=0, order="C") array([[1, 2, 3], [4, 5, 6]])np.ndarray(shape=(2,3), dtype=int, buffer=np.array([1,2,3,4,5,6,7]), offset=0, order="F")array([[1, 3, 5], [2, 4, 6]]) 参考数据格式汇总及type, astype, dtype区别2.3 dot()函数123456789101112131415161718192021&gt;&gt;&gt; aarray([4, 6, 2])&gt;&gt;&gt; barray([-2, 5, 10])&gt;&gt;&gt; c = a.dot(b)&gt;&gt;&gt; c #4*(-2)+6*5+2*10=42 对于一维数组得到的是数组的内积（一一对应相乘）42&gt;&gt;&gt; type(c)&lt;class 'numpy.int32'&gt;&gt;&gt;&gt; darray([[1, 2], [3, 4]])&gt;&gt;&gt; earray([[5, 6], [7, 8]])&gt;&gt;&gt; f = d.dot(e)&gt;&gt;&gt; f #二维数组得到的是矩阵积 1*5+2*7=19, 3*5+4*7=43array([[19, 22], [43, 50]])&gt;&gt;&gt; type(f)&lt;class 'numpy.ndarray'&gt; 2.4 numpy.cumsum(a, axis=None)函数12345678910111213import numpy as npa = np.range(4)a = a.reshape((2,2))array([[0, 1], [2, 3]])&gt;&gt;&gt; np.cumsum(a) #没有axis参数，就把数组a当作一维数组得到[0, 0+1, 0+1+2, 0+1+2+3]array([0, 1, 3, 6], dtype=int32)&gt;&gt;&gt; np.cumsum(a, 0) #axis=0, 就以行为单位，对应列的元素累加得到[[0, 1], [0+2, 1+3]]array([[0, 1], [2, 4]], dtype=int32)&gt;&gt;&gt; np.cumsum(a, 1) #axis=1, 就以列为单位，对应元素行的元素累加得到[[0, 0+1], [2, 2+3]]array([[0, 1], [2, 5]], dtype=int32) 2.5 花式索引1234567&gt;&gt;&gt; a = np.arange(0, 100, 10)&gt;&gt;&gt; aarray([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])&gt;&gt;&gt; indices = [1, 5, -1]&gt;&gt;&gt; b = a[indices] #用列表作为参数，进行高效索引&gt;&gt;&gt; barray([10, 50, 90]) 2.6 布尔屏蔽123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import matplotlib.pyplot as plta = np.linspace(0, 2 * np.pi, 50) a array([0. , 0.12822827, 0.25645654, 0.38468481, 0.51291309, 0.64114136, 0.76936963, 0.8975979 , 1.02582617, 1.15405444, 1.28228272, 1.41051099, 1.53873926, 1.66696753, 1.7951958 , 1.92342407, 2.05165235, 2.17988062, 2.30810889, 2.43633716, 2.56456543, 2.6927937 , 2.82102197, 2.94925025, 3.07747852, 3.20570679, 3.33393506, 3.46216333, 3.5903916 , 3.71861988, 3.84684815, 3.97507642, 4.10330469, 4.23153296, 4.35976123, 4.48798951, 4.61621778, 4.74444605, 4.87267432, 5.00090259, 5.12913086, 5.25735913, 5.38558741, 5.51381568, 5.64204395, 5.77027222, 5.89850049, 6.02672876, 6.15495704, 6.28318531])b = np.sin(a)barray([ 0.00000000e+00, 1.27877162e-01, 2.53654584e-01, 3.75267005e-01, 4.90717552e-01, 5.98110530e-01, 6.95682551e-01, 7.81831482e-01, 8.55142763e-01, 9.14412623e-01, 9.58667853e-01, 9.87181783e-01, 9.99486216e-01, 9.95379113e-01, 9.74927912e-01, 9.38468422e-01, 8.86599306e-01, 8.20172255e-01, 7.40277997e-01, 6.48228395e-01, 5.45534901e-01, 4.33883739e-01, 3.15108218e-01, 1.91158629e-01, 6.40702200e-02, -6.40702200e-02, -1.91158629e-01, -3.15108218e-01, -4.33883739e-01, -5.45534901e-01, -6.48228395e-01, -7.40277997e-01, -8.20172255e-01, -8.86599306e-01, -9.38468422e-01, -9.74927912e-01, -9.95379113e-01, -9.99486216e-01, -9.87181783e-01, -9.58667853e-01, -9.14412623e-01, -8.55142763e-01, -7.81831482e-01, -6.95682551e-01, -5.98110530e-01, -4.90717552e-01, -3.75267005e-01, -2.53654584e-01, -1.27877162e-01, -2.44929360e-16])plt.plot(a,b) #画出(a, b) 的曲线 图一b &gt;= 0 #数组元素中从0到24索引值的元素都为Truearray([ True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False])mask = (b &gt;= 0) #b的数组元素大于等于零就是True,否则是False，然后把组成新的数组，并赋值给maskplt.plot(a[mask], b[mask], 'bo') #画出a, b索引值为True的点，及0-24索引值 图二a &lt;= np.pi / 2array([ True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False])mask = [(b &gt;= 0) &amp; (a &lt;= np.pi / 2)] #(b &gt;= 0) &amp; (a &lt;= np.pi / 2)两则的交集为0-12索引值为Trueplt.plot(a[mask], b[mask], 'go') #画出(a,b)的点 图三plt.show() #显示出三个图像，由于之前两个没有显示，这里调用show()会全部显示出来。图四 图一 图二 图三 图四 注意下面的代码，系统出现了警告提示，意思就是说不能用非元组序列来作为数组的索引，将在后面的版本中视为错误。 12345plt.plot(a[mask], b[mask], 'go')Warning (from warnings module): File "__main__", line 1FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.[&lt;matplotlib.lines.Line2D object at 0x0C78ADF0&gt;] 根据提示可以改为： 12plt.plot(a[tuple(mask)], b[tuple(mask)], 'go')[&lt;matplotlib.lines.Line2D object at 0x0C6391B0&gt;] 最后补充一点： 123456789101112131415161718&gt;&gt;&gt; aarray([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])&gt;&gt;&gt; b = (a&gt;3)&gt;&gt;&gt; barray([[False, False], [False, False], [ True, True], [ True, True], [ True, True]])&gt;&gt;&gt; a[b]array([4, 5, 6, 7, 8, 9])&gt;&gt;&gt; a[a&gt;3] #这样也等同于用一个布尔数组做参数的方法array([4, 5, 6, 7, 8, 9])&gt;&gt;&gt;]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>deeplearning</tag>
        <tag>python</tag>
        <tag>numpy</tag>
        <tag>os.path</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[近期关于学习Python的思考]]></title>
    <url>%2FSonicHuang%2F50144ef0%2F</url>
    <content type="text"><![CDATA[从写完几篇爬虫后至今有二十几天没有更新文章了。 这期间我做了三件事： 第一件事 从头追了一部剧’Game of Thrones’,买了一套原版原著’Ice and Fire‘，目前看到第一卷160页，感觉比剧更精彩。目前剧更新到第八季第五集，每周一更新，书继续看。 第二件事 学习Python一年有余，小甲鱼的零基础学习Python的教材基础知识部分已经学完，现在’是时候呼叫超级飞侠了‘，咳咳，跑题了，是时候思考一下今后的学习方向了。首先我想到的是用python去做一个什么项目，通过项目来学习，上网查资料，什么项目适合我呢？什么爬虫，数据分析，我都不感兴趣，感觉做爬虫和数据分析都太被动（可能是自己知识局限，未能深刻理解，请见谅。），需要找一个适合自己长期专研的项目，于是看到网站开发Django, 刚好前一段时间，朋友问我能不能做网站，当时我确实没有学过。所以可以通过学习Django帮助别人开发喜欢的网站，于是我在B站找到了一个Django2.0的视频教程, 如果你感兴趣的话可以去学习。另外分享一个下载B站视频的方法，在视频地址bilibili前加一个字母i就可以跳转到下载网站，然后右键另存就可以了。学习了七八课后，突然感觉网站开发不是我心里最想去干的事情。看过篇知乎的一个问答，关于web开发的前途问题，其中一个叫暗灭的回复的内容我比较感兴趣，他文章并不是讲有没有前途，讲的web开发工作内容以及前端后端的分工。回到前面，我并不是觉得web开发没有前途，而是认为ta不适合我，ta更适合励志做程序员的人。 然后我做了第三个事情，我问了自己一个问题，我为什么要学习python? 回答是：为了主动与未来的世界保持沟通。说人话就是：几十年后，还有我说话的份，与年轻一代减少代沟。我指的不是那些‘不听老人言，吃亏在眼前’的‘人生哲理’，而是能够理解，使用和传授新知识，新技术。即使做不到授业解惑，也要能够自信的说未来不光是属于你们，还属于我。那么未来是什么样的呢？我需要学习什么呢？Python能够完成未来的什么任务呢？想必大家都能够猜到了，深度学习以及人工智能AI. WAF? 现在人人都在说AI，不说AI就不是社会人是吧？你也要来凑这个热闹吗？就像Jaime最后跟Cersei说的‘Nothing else matters, Only us’，其他什么都不重要了，只有我们俩。我们俩同生死。对于我来说，不管外面怎么吵怎么闹，我选好我的方向，只要与ta同在就行了。我能够像想对于一个数学和逻辑思维都不太好的奔四的人来说（好像这些都是对我极大不利的条件）有多困难了，或许是吧？但或许不是，我不求依靠这些知识来养活我自己，只求能够武装我自己。引用Tyrion跟Jon说的一句话’Never forget what you are, for surely the world will not. Make it your strength. Then it can never be your weakness. Armor yourself in it, and it will never be used to hurt you’. 别忘记自己谁，用自己的弱点武装自己，就没有人能够打败你。我觉得自己是一个富有正能量的人。。。我曾经一个美国的网友，她本人就是做Deeplearning的，我抱怨我的数学不好，她说不是我数学不好，而是not be well taught, 我没有遇到一个会教我数学的老师。这一点也是十分重要的，要不是初中二年级时英语老师对我的影响，我不会选择英语作为母语外的第二语言。（他的诙谐幽默让我喜欢上了英语这门课。）现在要找一个合适的老师或者一本书，真的不容易啊，刚好在网店上看到一本叫‘深度学习入门 -基于python的理论与实现’的书，是一个日本作者写的，看评论说日本人比较细心，像一位老者慢慢道来的感觉，适合初学者。入手后已经学习了一小部分，的确如评论所说，讲的比较有耐心，该重复说的知识点会重复提到，而且翻译也很到位啊，有作者本人在论述的感觉，知识都讲的浅显易懂。 上面就是我最近所思考的问题，所做的的事情。在此分享。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>Django</tag>
        <tag>Deeplearning</tag>
        <tag>Game of Thrones</tag>
        <tag>Ice and Fire</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-精品聚合网下载图片]]></title>
    <url>%2FSonicHuang%2Ff5189117%2F</url>
    <content type="text"><![CDATA[之前写了两篇下载煎蛋网图片的文章，这篇是精品聚合网妹子图片下载，与从煎蛋网下载极为类似，就不再细说，下载完成后有也有一万多张照片。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117import requestsfrom bs4 import BeautifulSoupimport chardetimport os.pathimport osimport timeimport pickle#定义函数返回地址二进制数据内容def bhtml(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers) bhtml = response.content return bhtml #每个地址的soup数据 def data(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers) global code code = chardet.detect(response.content)['encoding'] response.encoding = code html = response.text #print(html) soup = BeautifulSoup(html, 'html.parser') return soup #所有分页的地址(生成器)def getpage(): page = 12 for n in range(page, 0, -1):#页面倒数到1 pageurl = ''.join(['http://www.jingpinjuhe.com/?cat=26&amp;paged=', str(n)]) yield pageurl#所有分页的地址(生成器)def getitempage(soup): list1 = soup.find_all('a', attrs=&#123;'target':'_blank', 'title':True&#125;) # &lt;a target="_blank" href="url address" title="title message"&gt;some message&lt;/a&gt; for eachitem in list1: itemurl = eachitem['href'] itemtitle = eachitem['title'] print(itemtitle, itemurl, '下载中....') yield itemurl#每个页面的图片地址，下载保存到本地, def downpic(soup): passurllist = [] list2 = soup.find_all('img', attrs=&#123;'src':True&#125;) for pictag in list2: eachpicurl = pictag['src'] if '&lt;img src=' in eachpicurl: continue if ' /&gt;' in eachpicurl: eachpicurl = eachpicurl.replace(' /&gt;', '') if 'sinaimg' in eachpicurl: if 'bmiddle' in eachpicurl: eachpicurl = eachpicurl.replace('bmiddle', 'large') if 'mw690' in eachpicurl: eachpicurl = eachpicurl.replace('mw690', 'large') if 'mw600' in eachpicurl: eachpicurl = eachpicurl.replace('mw600', 'large') if 'thumbnail' in eachpicurl: eachpicurl = eachpicurl.replace('thumbnail', 'large') if 'small' in eachpicurl: eachpicurl = eachpicurl.replace('small', 'large') if 'thumb150' in eachpicurl: eachpicurl = eachpicurl.replace('thumb150', 'large') if eachpicurl in ['http://www.jingpinjuhe.com/wp-content/themes/xiu/images/logo.png']:#如果是网站图标则忽略 continue picname = os.path.split(eachpicurl)[1] try: #导出图片的二进制数据 picdata = bhtml(eachpicurl) except: print(eachpicurl, '下载有问题') passurllist.append(eachpicurl) with open('passurl.txt', 'wb') as f1: pickle.dump(passurllist, f1) continue else: if 'gif' not in picname:#不下载gif图片 with open(picname, 'wb') as f: f.write(picdata) time.sleep(0.5)def main(): try:#新建一个文件夹，如果已经存在就pass os.mkdir(r'D:\download\pic\new') except: pass os.chdir(r'D:\download\pic\new') #改变工作目录 url = r'http://www.jingpinjuhe.com/?cat=26&amp;paged=12' #最后一页 pageurl = getpage()#所有页码的地址 for x in pageurl: print(x) pagesoup = data(x)#每个页面的soup itemaddress = getitempage(pagesoup) for itemurl in itemaddress: if itemurl in [r'http://www.jingpinjuhe.com/?p=2965']: continue itemsoup = data(itemurl) downpic(itemsoup) print('下载完毕.') if __name__ == '__main__': main() 只是下载完后发现有一部分地址名后面没有图片的后缀，这个问题也很简单，把所有没有后缀的图片文件放在一个文件夹中，使用os.walk(path)导出所有文件名，然后用os.rename(oldname, oldname+’jpg’)就解决问题了。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>chardet</tag>
        <tag>BeautifulSoup</tag>
        <tag>requests</tag>
        <tag>os</tag>
        <tag>time</tag>
        <tag>pickle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-下载上万张煎蛋网旧图片]]></title>
    <url>%2FSonicHuang%2Fdf12fa59%2F</url>
    <content type="text"><![CDATA[继上一篇从煎蛋网下载妹子图片后，在网上发现另外两个途径可以下载到煎蛋网往年的旧图片。第一个是个网页，作者把所有图片地址写入一个网页中，没有一个文字只有图片有五千多张。第二个是一个本地html文件，也是一样用浏览器打开后全是图片，有六千多张。据我后面看着两个地方的图片还没有发现重复的。全部下载完，除去一些失效的图片链接，有上万张。 总的来说，写这个爬虫比较简单，没有复杂的标签，没有分页码，简单粗暴方法就可以应付了。 思路是：1. 用requests得到数据后，分析出所有图片的地址，写入一个allurl.txt文件中。2.从allurl.txt文件中读取图片地址，并下载，下载不成功的地址写入passurl.txt, 成功的地址写入okurl.txt中。3. 把第2步中passurl.txt该名为allurl.txt， 删除okurl.txt和旧的allurl.txt并打开代理或者其他重复第二步，直到没有更多的地址可以下载。 一. 从网页上获取数据写入allurl.txt1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import requestsimport chardetfrom bs4 import BeautifulSoupimport osimport pickle#页面的soup数据def data(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers, timeout = 5) global code code = chardet.detect(response.content)['encoding'] response.encoding = code html = response.text #print(html) soup = BeautifulSoup(html, 'html.parser') return soup#页面所有图片的地址列表def getpicurl(soup): list1 = soup.find_all('img') for eachurl in list1: picurl = eachurl['src'] yield picurl#主程序会生成一个包含有所有图片下载地址的allurl.txt的二进制文件def main(): allurllist = [] try:#新建一个文件夹，如果已经存在就pass os.mkdir('D:\download\pic') except: pass os.chdir(r'D:\download\pic') #改变工作目录 url = r'http://js.funet8.com/html/jiandan-meizhi.html' #网站地址 soupdata = data(url) #网站的soup数据 pictureurl = getpicurl(soupdata) #生成器产生的每张图片的地址 for eachpicurl in pictureurl: #对地址进行删选。由于网页代码的一些问题，导致地址出现一些错误 if '&lt;img src=' in eachpicurl: continue if ' /&gt;' in eachpicurl: eachpicurl = eachpicurl.replace(' /&gt;', '') #修改一下可以得到大图的地址 if 'sinaimg' in eachpicurl: if 'bmiddle' in eachpicurl: eachpicurl = eachpicurl.replace('bmiddle', 'large') if 'mw690' in eachpicurl: eachpicurl = eachpicurl.replace('mw690', 'large') if 'mw600' in eachpicurl: eachpicurl = eachpicurl.replace('mw600', 'large') if 'thumbnail' in eachpicurl: eachpicurl = eachpicurl.replace('thumbnail', 'large') if 'small' in eachpicurl: eachpicurl = eachpicurl.replace('small', 'large') if 'thumb150' in eachpicurl: eachpicurl = eachpicurl.replace('thumb150', 'large') #把所有地址组成的列表写入文件 allurllist.append(eachpicurl) with open('allurl.txt', 'wb') as f2: pickle.dump(allurllist, f2)if __name__ == '__main__': main() ​ 二.从本地html文件上获取数据123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import requestsimport chardetfrom bs4 import BeautifulSoupimport osimport pickle#页面的soup数据def data(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers, timeout = 5) global code code = chardet.detect(response.content)['encoding'] response.encoding = code html = response.text #print(html) soup = BeautifulSoup(html, 'html.parser') return soup#页面所有图片的地址列表def getpicurl(soup): list1 = soup.find_all('img') for eachurl in list1: picurl = eachurl['src'] yield picurl def main(): allurllist = [] try:#新建一个文件夹，如果已经存在就pass os.mkdir('D:\download\pic') except: pass os.chdir(r'D:\download\pic') #改变工作目录 url = r'E:\python files\IDEL FILES\My py file\爬虫煎蛋网旧图片\jiandan.html' #文件保存地址，需要根据实际情况变化 #读取本地html文件，注意open的参数需要加上encoding的参数，否则会出现编码错误，至于编码方法可以用chardet测试 #得到的f是一个迭代器，迭代出的是包含每个图片地址标签的字符串，如果我们直接对这个标签字符串进行操作会很麻烦，所以用BeautifulSoup进行解析 with open(url, mode='r', encoding='utf-8') as f: soupdata = BeautifulSoup(f, 'html.parser') pictureurl = getpicurl(soupdata) for eachpicurl in pictureurl: if '&lt;img src=' in eachpicurl: continue if ' /&gt;' in eachpicurl: eachpicurl = eachpicurl.replace(' /&gt;', '') if 'sinaimg' in eachpicurl: if 'bmiddle' in eachpicurl: eachpicurl = eachpicurl.replace('bmiddle', 'large') if 'mw690' in eachpicurl: eachpicurl = eachpicurl.replace('mw690', 'large') if 'mw600' in eachpicurl: eachpicurl = eachpicurl.replace('mw600', 'large') if 'thumbnail' in eachpicurl: eachpicurl = eachpicurl.replace('thumbnail', 'large') if 'small' in eachpicurl: eachpicurl = eachpicurl.replace('small', 'large') if 'thumb150' in eachpicurl: eachpicurl = eachpicurl.replace('thumb150', 'large') allurllist.append(eachpicurl) with open('allurl.txt', 'wb') as f2: pickle.dump(allurllist, f2)if __name__ == '__main__': main() 三. 从allurl.txt读取数据下载图片1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import requestsimport chardetfrom bs4 import BeautifulSoupimport os.pathimport osimport pickle#定义函数返回地址二进制数据内容def bhtml(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers, timeout = 5) bhtml = response.content return bhtml#页面所有图片的地址列表def getpicurl(soup): list1 = soup.find_all('img') for eachurl in list1: picurl = eachurl['src'] yield picurl#从allurl.txt读取数据并下载图片 def main(): try:#新建一个文件夹，如果已经存在就pass os.mkdir('D:\download\pic') except: pass os.chdir(r'D:\download\pic') #改变工作目录 #如果第一次运行，就直接从allurl.txt读取数据, passurl和okurl都为空。 #如果中途因为各种原因停止了程序，那么从allurl中删除已经下载的和没有下载成功的地址，这样可以避免重启时重复下载图片。 try: with open(r'D:\download\pic\passurl.txt', 'rb') as psurl: passurllist = pickle.load(psurl) with open(r'D:\download\pic\okurl.txt', 'rb') as goodurl: okurllist = pickle.load(goodurl) with open(r'D:\download\pic\allurl.txt', 'rb') as totalurl: allurllist = pickle.load(totalurl) except: passurllist = [] okurllist = [] with open(r'D:\download\pic\allurl.txt', 'rb') as totalurl: allurllist = pickle.load(totalurl) else: newalllist = [x for x in allurllist if x not in passurllist and x not in okurllist] with open(r'D:\download\pic\allurl.txt', 'wb') as total: pickle.dump(newalllist, total) #遇到读取数据时出现问题就把地址保存在passurl里面，下载完成的保存在okurl里面。 for eachpicurl in allurllist: picname = os.path.split(eachpicurl)[1] try: picdata = bhtml(eachpicurl) except: print(eachpicurl, '下载有问题') passurllist.append(eachpicurl) with open('passurl.txt', 'wb') as f1: pickle.dump(passurllist, f1) continue else: try: with open(picname, 'wb') as f: f.write(picdata) except: print(eachpicurl, '下载有问题') passurllist.append(eachpicurl) with open('passurl.txt', 'wb') as f4: pickle.dump(passurllist, f4) continue else: print(eachpicurl, '下载ok') okurllist.append(eachpicurl) with open('okurl.txt', 'wb') as f3: pickle.dump(okurllist, f3)if __name__ == '__main__': main() 总结： 从本地html文件读取数据的方法 对异常的处理]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>chardet</tag>
        <tag>BeautifulSoup</tag>
        <tag>requests</tag>
        <tag>os</tag>
        <tag>pickle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-煎蛋网图片下载和分类]]></title>
    <url>%2FSonicHuang%2Fd6a406bd%2F</url>
    <content type="text"><![CDATA[一. 通过爬取煎蛋网随手拍，下载网页上的图片。 1.煎蛋网不知道是什么原因把以前妹子图的地址改成随手拍，而且他下面的页码在超过一定页数（目测35页左右），就会删除掉前面几页的内容。他页面地址http://jandan.net/ooxx/page-32#comments， 32就是他的页码数。我们首先需要知道他首页的页码，然后后面-=1，直到1。Chrome浏览器在随手拍页面首页审查元素，看到current-comment-page就是首页第32页的代码位置，所以知道我们找到这个标签就可以确定首页的页码。我使用了BeautifulSoup，得到每个页面的文本soup数据函数data(url)就可以查找到该标签。函数getpage(soup)代码第40行soup.find_all(&#39;span&#39;, attrs={&#39;class&#39;:&#39;current-comment-page&#39;}), 随后再通过函数getpage(soup)其他页面的地址就很容易了。 2.得到页面地址后，我们就需要知道每个页面上所有图片的地址。任意一张页面上的图片鼠标右键审查元素可以看到标签的组成，可以看到原图地址就是把mw600改为large。所以我们只要找到这个标签就可以了。函数downpic(soup)第57行soup.find_all(&#39;a&#39;, attrs={&#39;class&#39;:&#39;view_img_link&#39;}) 其实在我刚写这段代码时，煎蛋网的还采用了反爬机制，对地址进行了加密，所以我代码中还保留的那部分内容作为备注。参考前辈的例子Python爬虫爬取煎蛋网无聊图，煎蛋网也是用心良苦啊，栏目名字都改来来去的。 3.然后就可以读取图片地址的数据，下载图片。下载图片实际上就是把图片的数据写入一个新的图片文件里。函数downpic(soup)第78，79行。4.完善代码。以及其中遇到的问题。 检测网页的编码。自然用到chardet.但是有时会出现不名的原因导致检测的编码类型有误，导致页面有乱码，查找不到我们想要的标签。本来是utf-8, 得到的结果是windows-1254,这是什么鬼，我也不知道，查了很久终于在一篇文章中找到了解决方法，见下面应用内容。参考再也不用担心网页编码的坑了， 把response.encoding = None， 运行一下再改过来 = code.问题就解决了。这里还要注意，chardet.detect的参数是二进制数据，response.text是文本数据，.content才是二进制数据。 123code = chardet.detect(response.content)['encoding']response.encoding = codehtml = response.text 那么当你发现response.text返回乱码的时候，怎么办呢。。。 只要先设置编码为None… 再打印.text就可以了.. 123&gt; response.encoding = None&gt; response.text&gt; 为了不重复下载以前下载过的图片（毕竟每天图片都在更新）， 所以我使用pickle把所有的图片名保存在了文件里，再下载时，先读取保存的文件（一个列表以二进制的数据保存），列表中没有的图片名才下载。由于网站保存的图片名都是固定的，所以当遇到旧的图片名时就可以停止下载。如果把每次出现的新图片都储存在文件里，势必让文件越来越大，为了解决这个问题，我把新图片名写入一个临时文件，再写入一个旧的图片名，最后删除原来的文件，把临时文件改名为正式文件。这样每次下载完后只保存了新的图片名和一个旧图片名作为下载的终点。 同时为了让函数downpic(soup)读取到旧图片名时让函数和主程序main()都立刻停止循环执行下面的脚本不要再重复读取页面数据，就需要从函数中发出一个信号，让程序终止。我使用了raise语句来引发一个异常，我自定义了一个Stopdownload()的异常， 函数和主程序都可以通过接受到异常来终止程序。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128import requestsimport refrom bs4 import BeautifulSoupimport chardetimport urllib.parse as upimport base64import os.pathimport osimport timeimport pickle#自定义一个异常类class Stopdownload(Exception): def __init__(self, err='遇到旧图片,停止下载了。'): Exception.__init__(self, err)#定义函数返回地址二进制数据内容def bhtml(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers) bhtml = response.content return bhtml #每个地址的soup数据 def data(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; #proxy = &#123;'http':'120.78.174.170:8080'&#125; , proxies=proxy response = requests.get(url=url, headers=headers) global code code = chardet.detect(response.content)['encoding'] response.encoding = code html = response.text soup = BeautifulSoup(html, 'html.parser') return soup #所有分页的地址(生成器)def getpage(soup): list1 = soup.find_all('span', attrs=&#123;'class':'current-comment-page'&#125;) # &lt;span class="current-comment-page"&gt;[56]&lt;/span&gt; a = list1[0].text #a='[56]'他是一个字符串 page = int(a[1:3]) #切片 for n in range(page-1, 0, -1):#页面倒数到1 pageurl = ''.join(['http://jandan.net/ooxx/page-', str(n), '#comments']) yield pageurl#每个页面的图片地址，下载保存到本地, def downpic(soup): try: #打开保存的已下载的图片名数据 with open('D:\download\piclist.txt', 'rb') as f2: list1 = pickle.load(f2)#把数据导入列表1 except: #如果第一次下载，列表1为空列表 list1 = [] #找到包含有页面图片下载地址的标签列表2 #list2 = soup.find_all('span', attrs=&#123;'class':'img-hash'&#125;) #网站使用了反爬机制，参考https://www.jianshu.com/p/5351baf254ef list2 = soup.find_all('a', attrs=&#123;'class':'view_img_link'&#125;) #定义一个空的列表3，用于写入所有新下载的图片。 list3 = [] #找到所有图片的下载地址和图片名称 for eachcode in list2: #piccode = str(base64.b64decode(eachcode.string.encode(code)))[2:].replace('mw600', 'large').replace('\'', '')#使用反爬机制时，需要对数据解密。参考https://www.jianshu.com/p/5351baf254ef piccode = eachcode['href'] picurl = ''.join(['http:', piccode]) if picurl in ['http://ww3.sinaimg.cn/large/006XNEY7gy1g1xabxlnq4j30fo0jwwhv.jpg']:#如果网页的图片出现的加载错误，导致不能读取picdata。就需要跳过这个图片。其实也可以使用异常处理的方法 continue picname = os.path.split(picurl)[1] #导出图片的二进制数据 picdata = bhtml(picurl) #如果图片没有下载过，则下载图片, 并把图片名导入列表3 if picname not in list1: print(picname) list3.append(picname) with open(r'D:\download\temp.txt', 'ab') as f3:#把列表3的数据保存在临时文件(包括新下载的图片名) pickle.dump(list3, f3) if 'gif' not in picname:#不下载gif图片 with open(picname, 'wb') as f: f.write(picdata) time.sleep(0.5) else:#如果有下载过 print(picname, '是旧图片') list3.append(picname) with open(r'D:\download\temp.txt', 'ab') as f3: pickle.dump(list3, f3) raise Stopdownload() #抛出异常def main(): try:#新建一个文件夹，如果已经存在就pass os.mkdir('D:\download\pic') except: pass os.chdir(r'D:\download\pic') #改变工作目录 mainurl = 'http://jandan.net/ooxx' #主页 print(mainurl) mainsoup = data(mainurl)#导出主页的soup try: downpic(mainsoup)#下载主页的图片，如果全部是新图片不会触发异常 except Stopdownload as e:#如果遇到旧图片则停止下载 print(e) pass else:#如果没有遇到旧图片 pageurl = getpage(mainsoup)#下面页码的地址 for x in pageurl: print(x) pagesoup = data(x)#每个页面的soup try: downpic(pagesoup) #下载每个页面的图片 except Stopdownload as e:#如果遇到旧图片立刻停止循环 print(e) break #删除旧的文件数据，把临时文件改为正式文件，储存了最新下载的图片名数据 os.remove(r'D:\download\piclist.txt') os.rename(r'D:\download\temp.txt', r'D:\download\piclist.txt')#为了是在下载图片时遇到旧图片时停止下载，而不用继续循环读取旧页面的数据，#我在下载图片的函数中使用raise语句，当出现旧图片时立刻引发异常。#在主函数中就可以捕获这个异常而中断循环。if __name__ == '__main__': main() 二.对已下载图片进行分类虽然自动下载了图片，但是是否是自己喜欢的，还要自己去判断（没有非人工智能高大上），这段代码就是实现了自动显示图片，自己再选择是否喜欢，并把图片保存在不同的文件夹里。使用了matplotlib.pyplot, PIL/Image, shutil, easygui这四个库。 在写这段代码时遇到os.chdir(&#39;C:\Users\vulcanten\Desktop\test&#39;)出现异常，原因是\在字符串中是当作转义字符来使用, 所以出现了报错，我们只要在使用路径名时在前面加上r, 就可以解决了 os.chdir(r&#39;C:\Users\vulcanten\Desktop\test&#39;)， 还可以使用\\, 或者/来代替。 12345678910111213141516171819202122232425262728293031#对下载的图片进行分类import matplotlib.pyplot as pltfrom PIL import Imageimport osimport timeimport shutilimport easygui as egimport pickleos.chdir(r'D:\download\pic')ways = os.walk(r'D:\download\pic')for each in ways: list1 = each[2] for picname in list1: img = Image.open(picname) #打开图片数据 print(picname) plt.ion()#自动滚动显示图片并执行相关脚本, 需要与pause()一起使用 plt.imshow(img)#显示图片 plt.show() plt.pause(1)#暂停等候 #使用easygui来对图片进行判断，并移动图片到对应文件夹 like = eg.buttonbox('喜欢的点OK', '', ('OK', '不喜欢')) if like == 'OK': shutil.move(picname, ''.join(['D:/download/like/', picname])) elif like == '不喜欢': shutil.move(picname, ''.join(['D:/download/unlike/', picname])) else: breakplt.close() 上图看一下效果 运行后发现这个代码运行速度比较慢，还没有使用图片查看器再结合键盘的删除键来的快。虽然如此，但代码还是提供了一些有用的思路，以备今后使用。 本文离不开以下文章的贡献： python 自定义异常和异常捕捉 创建自定义异常 python之文件操作-复制、剪切、删除等 python 图像的显示关闭以及保存 python中使用PIL和matplotlib.pyplot打开显示关闭暂停和保存图片 总结： 在函数中使用raise来引起异常，实现终止程序的功能。 自定义异常的方法 用shutil在移动或者复制文件 实现滚动显示图片的功能 写爬虫最难的是遇到反爬机制，但是自己又看不懂Jason文件。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
        <tag>urllib.parse</tag>
        <tag>re</tag>
        <tag>BeautifulSoup</tag>
        <tag>requests</tag>
        <tag>pickle</tag>
        <tag>matplotlib.pyplot</tag>
        <tag>PIL/Image</tag>
        <tag>shutil</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-百度词条副标题以及生成器的用法]]></title>
    <url>%2FSonicHuang%2F27e9abdf%2F</url>
    <content type="text"><![CDATA[我们通过输入关键字，爬取百度百科该关键字网页上的解释和其他词条的链接以及对应的副标题，最后控制显示链接的数量。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152'''本代码通过输入关键字，爬取百度百科该关键字网页上的解释和其他词条以及对应的副标题'''import requestsfrom bs4 import BeautifulSoupimport chardetimport reimport urllib.parse as up#定义一个函数返回目标网页的文本数据def ht(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; proxy = &#123;'http':'120.78.174.170:8080'&#125; response = requests.get(url = url, headers = headers, proxies = proxy) code = chardet.detect(response.content)['encoding']#response.content是返回响应的二进制数据 response.encoding = code #response.encoding可以查询编码方式，也可以通过赋值对响应进行编码 html = response.text #response.text返回响应的文本数据 return html#输入关键字查询keyword = input('输入关键字查询：')url = ''.join(['http://baike.baidu.com/search/word?word=', up.quote(keyword)])#对关键字进行编码html = ht(url)soup = BeautifulSoup(html, 'html.parser')list1 = soup.find_all('p') #在关键字没有被收录的页面，搜索结果出现在第一个p便签里面if '百度百科尚未收录词条' in list1[0].text: print(list1[0].text)else: list0 = soup.find_all('meta', attrs=&#123;'name':'description'&#125;) #找到对关键字的解释内容,meta标签，含有name='description'的属性和属性值 description = list0[0]['content'] print(description) print('下面是相关链接：') time = 0 #每次显示十行，再询问是否要继续 for tag in soup.find_all(href = re.compile('item')):#找到所有包含item属性的标签，所有含有href属性，属性值含有item字符串的标签 url = ''.join(['http://baike.baidu.com', up.unquote(tag['href'])]) subhtml = ht(url) subsoup = BeautifulSoup(subhtml, 'html.parser') subtitle = subsoup.title.string.replace('_百度百科', '')#打开词条链接并找到词条的title文本，title标签 if '（' in subtitle: #如果词条含有副标题 print(subtitle, '--&gt;', ''.join(['http://baike.baidu.com', up.unquote(tag['href'])])) else: print(tag.text, '--&gt;', ''.join(['http://baike.baidu.com', up.unquote(tag['href'])])) time += 1 if time == 10: answer = input('退出请回复: n， 其他任意键继续。请输入：') if answer == 'n': break else: time = 0 continue 我的代码直接利用了time叠加的方式对链接数量进行控制。 翻看小甲鱼老师的代码，他使用了定义多个函数的方式和生成器, 于是我把原来的代码进行了改编，并进行比较。先看代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879'''本代码通过输入关键字，爬取百度百科该关键字网页上的解释和其他词条以及对应的副标题'''import requestsfrom bs4 import BeautifulSoupimport chardetimport reimport urllib.parse as up#定义函数返回目标网页的文本数据def ht(url): agent = 'User-Agent' agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36' headers = &#123;agent:agentvalue&#125; proxy = &#123;'http':'120.78.174.170:8080'&#125; response = requests.get(url = url, headers = headers, proxies = proxy) code = chardet.detect(response.content)['encoding']#response.content是返回响应的二进制数据 response.encoding = code #response.encoding可以查询编码方式，也可以通过赋值对响应进行编码 html = response.text #response.text返回响应的文本数据 return html#判断是否有收录关键字def havecontent(soup): list1 = soup.find_all('p') #在关键字没有被收录的页面，搜索结果出现在第一个p便签里面 if '百度百科尚未收录词条' in list1[0].text: print(list1[0].text) return True else: return False#提取对关键字的解释def description(soup): list0 = soup.find_all('meta', attrs=&#123;'name':'description'&#125;) #找到对关键字的解释内容,meta标签，含有name='description'的属性和属性值 description = list0[0]['content'] print(description)#提取关键字网页的所有词条副标题和链接def sublinks(soup): for tag in soup.find_all(href = re.compile('item')):#找到所有包含item属性的标签，所有含有href属性，属性值含有item字符串的标签 url = ''.join(['http://baike.baidu.com', up.unquote(tag['href'])]) subhtml = ht(url) subsoup = BeautifulSoup(subhtml, 'html.parser') subtitle = subsoup.title.string.replace('_百度百科', '')#打开词条链接并找到词条的title文本，title标签 if '（' in subtitle: #如果词条含有副标题 sublinks = ''.join([subtitle, '--&gt;', 'http://baike.baidu.com', up.unquote(tag['href'])]) else: sublinks = ''.join([tag.text, '--&gt;', 'http://baike.baidu.com', up.unquote(tag['href'])]) yield sublinks #主程序def main(): #输入关键字查询 keyword = input('输入关键字查询：') url = ''.join(['http://baike.baidu.com/search/word?word=', up.quote(keyword)])#对关键字进行编码 html = ht(url) soup = BeautifulSoup(html, 'html.parser') if havecontent(soup): pass else: description(soup) print('下面是相关链接：') links = sublinks(soup)#注意这个地方需要把函数赋值给变量，才能产生一个生成器，否者只能返回函数的第一个值 time = 0 #每次显示十行，再询问是否要继续 while True: try: print(next(links))#生成器每次迭代出一个值，直到结束 time += 1 if time == 10: answer = input('退出请回复: n， 其他任意键继续。请输入：') if answer == 'n': break else: time = 0 continue except StopIteration: breakif __name__ == '__main__': main() 请注意下面两行 12links = sublinks(soup)#注意这个地方需要把函数赋值给变量，才能产生一个生成器，否者只能返回函数的第一个值print(next(links))#生成器每次迭代出一个值，直到结束 再看下面的例一 123456789101112131415161718192021222324252627282930313233343536#例一&gt;&gt;&gt; a = [1, 2, 3, 4, 5, 6, 7, 8, 9, 0]&gt;&gt;&gt; def fn(): for x in a: yield x &gt;&gt;&gt; next(fn())1&gt;&gt;&gt; next(fn())1&gt;&gt;&gt; next(fn())1&gt;&gt;&gt; k = fn()&gt;&gt;&gt; print(next(k))1&gt;&gt;&gt; print(next(k))2&gt;&gt;&gt; print(next(k))3#例二&gt;&gt;&gt; def fn(): for x in a: return x&gt;&gt;&gt; print(fn())1&gt;&gt;&gt; print(fn())1&gt;&gt;&gt; print(fn())1&gt;&gt;&gt; k = fn()&gt;&gt;&gt; print(k)1&gt;&gt;&gt; print(k)1&gt;&gt;&gt; print(k)1&gt;&gt;&gt; 上面例二也说明了如果定义了一个循环不是一个生成器，那么他只能return第一个值。而文章前段代码没有用函数来定义for tag in soup.find_all(href = re.compile(&#39;item&#39;))产生的值，而在迭代循环里直接打印出了所有链接。所以这里就显示出了生成器的作用的用法。 总结： 应用BeautifulSoup通过寻找属性和属性值定位标签 正则表达式 生成器的作用的用法]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>request</tag>
        <tag>urllib.parse</tag>
        <tag>re</tag>
        <tag>BeautifulSoup</tag>
        <tag>yield</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[百度百科“网络爬虫”的词条]]></title>
    <url>%2FSonicHuang%2F73ee126f%2F</url>
    <content type="text"><![CDATA[第一次接触正则表达式，和BeautifulSoup，第一印象就是复杂，必须要专门抽时间深入学习才行。 今天这个脚本初步应用他们的基本功能，爬取百度百科一个词条里面的其他词条链接。脚本没有采用之前的urllib.request而是直接用了request库来读取网页数据，据说这个更强大。 12345678910111213141516171819import requestsfrom bs4 import BeautifulSoupimport chardetimport reimport urllib.parse as upurl = 'http://baike.baidu.com/view/284853.htm'agent = 'User-Agent'agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36'headers = &#123;agent:agentvalue&#125;proxy = &#123;'http':'120.78.174.170:8080'&#125;response = requests.get(url = url, headers = headers, proxies = proxy)code = chardet.detect(response.content)['encoding']#response.content是返回响应的二进制数据response.encoding = code #response.encoding可以查询编码方式，也可以通过赋值对响应进行编码html = response.text #response.text返回响应的文本数据soup = BeautifulSoup(html, 'html.parser')for tag in soup.find_all(href = re.compile('item')): print(tag.text, '--&gt;', ''.join(['http://baike.baidu.com', up.unquote(tag['href'])])) 上面第19行使用url解码把臃肿的连接地址解码成简短的包含有中文的地址，只要浏览器可以识别就没有关系。 把 http://baike.baidu.com/item//item/%E5%BC%80%E6%94%BE%E6%BA%90%E4%BB%A3%E7%A0%81 解码成：http://baike.baidu.com/item/开放源代码 参考： Requests快速上手 python之urlencode()，quote()及unquote()]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>request</tag>
        <tag>urllib.parse</tag>
        <tag>re</tag>
        <tag>BeautifulSoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-proxy]]></title>
    <url>%2FSonicHuang%2F9df1cdd0%2F</url>
    <content type="text"><![CDATA[昨天看了小甲鱼的爬虫这章用代理和添加user agent来爬取网页的视频，对其中的内容不是十分了解，所以专门写了这篇文章来理解其中的方法。 12345678910111213141516171819202122232425262728293031323334353637383940414243'''本代码使用代理ip登录查询ip的网站来验证代理运行是否成功, 以及opener, header的使用'''import urllib.request as urimport chardet#这个网站可以显示当前的ip地址url = 'http://myip.kkcha.com/'agent = 'User-Agent'agentvalue = 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36'proxy = &#123;'http':'120.78.174.170:8080'&#125;#前三行定义一个特殊的opener,来使用代理或者其他来方式打开url. 如果没有定义,urlopen就是用默认的方式去打开url.proxy_support = ur.ProxyHandler(proxy)#handler我的理解就是用什么方法来handleopener = ur.build_opener(proxy_support)#opener就是生成一个工具#0 是否安装opener#ur.install_opener(opener) #安装openerresponse = opener.open(url)#如果前一行不安装opener,就用opener来打开url#response = ur.urlopen(url) #如果已经安装opener, 调用urlopen(url)就会自动用安装的opener的方式来处理#1 request参数添加header'''headers = &#123;agent:agentvalue&#125;request = ur.Request(url = url, headers = headers)response = opener.open(request)'''#2 opener添加header'''opener.addheaders = [(agent, agentvalue)]response = opener.open(url)'''#3 request添加header'''request = ur.Request(url)request.add_header(agent, agentvalue)response = opener.open(request)'''content = response.read()#response.read()运行一次后read的指针就在被读取内容的最后，如果再次调用将返回为空.所以这里把他的值赋值给一个变量, 后面直接调用变量code = chardet.detect(content)['encoding']html = content.decode(code)print(html) 测试了一下，因为代理不稳定或者其他原因，有时会返回错误信息，多试一下就会返回包含如下信息的网页信息，可以看到他的ip地址正是我们代理的地址。 总结： 需要理解handler和opener的作用，以及是否安装opener后的处理方法 可以在哪些地方添加header，以及不同的添加方法 调用read方法后的指针问题，避免重复读取后返回空值。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
        <tag>urllib.request</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[华为手机通讯录]]></title>
    <url>%2FSonicHuang%2F49cc418f%2F</url>
    <content type="text"><![CDATA[在恢复出厂设置时，把老爸的华为麦芒通讯录搞丢了，但老人家保留了一个比较早的一个通讯录电子表格，自己录入的格式没有固定。我就想有没有比较方便的方法，用代码写一个通讯录，再导入到手机里面。 1.华为麦芒vcard格式 如上图vcard代码格式，一个联系人有一个片段, 中文人名是通过Quoted-printable方式编码 Quoted-printable或QP encoding，没有规范的中文译名，可译为可打印字符引用编码或使用可打印字符的编码。Quoted-printable是使用可打印的ASCII字符（如字母、数字与“=”）表示各种编码格式下的字符，以便能在7-bit数据通路上传输8-bit数据, 或者更一般地说在非8-bit clean媒体上正确处理数据[注 1]。这被定义为MIME content transfer encoding，用于e-mail。 看不懂！其实更简单的是这样： URL编码后把%换成=, 或者用utf-8编码把\x换成=如下： 12345&gt;&gt;&gt; import urllib.parse as up&gt;&gt;&gt; up.quote('黄学')'%E9%BB%84%E5%AD%A6'&gt;&gt;&gt; '黄学'.encode('utf-8')b'\xe9\xbb\x84\xe5\xad\xa6' 看起来用url编码更好一点。如果几个号码，就会用多行TEL;CELL:连接，电话号码的格式为：XXX XXXX XXXX 2.Python 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105'''本程序是通过输入联系人和电话号码然后转为为华为手机vcard的vcf文件，vcf文件可以直接上传到华为手机通讯录。'''import urllib.parse as upimport pickleimport os'''定义函数返回一个联系人的字典'''def contacts(): print('|--- 欢迎进入通讯录 ---|') print('|--- 1：查找联系人 ---|') print('|--- 2：增加或修改联系人 ---|') print('|--- 3：删除联系人 ---|') print('|--- 4：显示所有通讯录 ---|') print('|--- 5：保存并退出通讯录 ---|') #从内存中的二进制文件中读取联系人信息 try: with open('contact.txt', 'rb') as f2: contact = pickle.load(f2) except:#如果内存中没有文件者则赋值为一个空的字典 contact = &#123;&#125; while 1: no = input('请选择:')#input输入的都是字符串 #查找联系人 if no == '1': name = input('输入联系人姓名:') if name in contact:#联系人存在则打印他的号码 NO = contact[name] print('号码是: %s' % NO) else: print('联系人不存在!') #添加联系人 if no == '2': name = input('输入联系人姓名:') if name in contact:#如果联系人已经存在，打印联系电话，并询问是否要修改 print(' 联系人%s 的电话是: %s ' % (name, contact[name])) change = input('修改原号码回复y, 增加号码回复z, 什么都不做回复n:') if change == 'y': NO = input('输入新号码\(多个号码请用+连接\):') contact[name] = NO elif change == 'z': NO = input('输入号码\(多个号码请用+连接\):') contact[name] += NO else: NO = contact[name] print('号码是: %s' % NO) else: NO = input('输入联系人号码:') contact[name] = NO #删除联系人 if no == '3': name = input('输入联系人姓名:') if name in contact: del(contact[name]) else: print('联系人不存在.') #显示所有通讯录 if no == '4': if contact == &#123;&#125;: print('通讯录是空的.') else: for i in contact: print(i, ':', contact[i]) #保存并退出 if no == '5': print('保存并退出.') break #写入一个二进制文件保存数据 with open('contact.txt', 'wb') as f1: pickle.dump(contact, f1) return contact #返回字典数据'''清除之前的通讯录'''try: os.remove('newdata.vcf')except: pass#华为通讯录vcard代码字符串info1 = 'BEGIN:VCARD\nVERSION:2.1\nN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;'info2 = ';;;\nFN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:'info3 = '\nTEL;CELL:'info4 = '\nEND:VCARD\n'#通过输入通讯录后导出联系人的字典数据allno = contacts()#把字典数据转化为vcard代码并写入newdata.vcf文件for i in allno: name = i #通过列表解析式删除数据里面的空格 nostr = ''.join([x for x in allno[i] if x != ' ']) nolist = nostr.split('+') #如果有多个电话号码需要每个电话分离出来组成一个列表 list2 = [] for no in nolist: #注意vcard电话号码的格式数字之间有空格 no = no[:3] + ' ' + no[3:7] + ' ' + no[7:] list2 += [info3, no] #每个电话号码会形成单独一行 name = up.quote(name).replace('%', '=') #vcard联系人的名字是经过url编码后把%换成= with open('newdata.vcf', 'a') as file: #把通讯录代码写入vcard list1 = [info1, name, info2, name] list3 = [info4] data = ''.join(list1 + list2 + list3) file.write(data) 3.测试代码1234567891011121314151617181920212223242526&gt;&gt;&gt; ===================== RESTART: E:\python files\IDEL FILES\My py file\通讯录.py =====================|--- 欢迎进入通讯录 ---||--- 1：查找联系人 ---||--- 2：增加或修改联系人 ---||--- 3：删除联系人 ---||--- 4：显示所有通讯录 ---||--- 5：保存并退出通讯录 ---|请选择:4通讯录是空的.请选择:2输入联系人姓名:sonic输入联系人号码: 123 4567 8901请选择:2输入联系人姓名:joan输入联系人号码: 23456789012 +34567890123 + 98765 432312请选择:2输入联系人姓名:黄晓明输入联系人号码:34567890133请选择:4sonic : 123 4567 8901joan : 23456789012 +34567890123 + 98765 432312黄晓明 : 34567890133请选择:5保存并退出.&gt;&gt;&gt; 生成了两个文件，一个是保存了数据，一个是需要vcard文件可以直接上传到手机通讯录 生成的vcard文件内容： 1234567891011121314151617181920BEGIN:VCARDVERSION:2.1N;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;sonic;;;FN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:sonicTEL;CELL:123 4567 8901END:VCARDBEGIN:VCARDVERSION:2.1N;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;joan;;;FN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:joanTEL;CELL:234 5678 9012TEL;CELL:345 6789 0123TEL;CELL:987 6543 2312END:VCARDBEGIN:VCARDVERSION:2.1N;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;=E9=BB=84=E6=99=93=E6=98=8E;;;FN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:=E9=BB=84=E6=99=93=E6=98=8ETEL;CELL:345 6789 0133END:VCARD 可以看到我在输入电话号码时的空格自动删除了，中文名被编码显示，英文字母没有变。代码写好了，以后再遇到相同的情况就不需要一个名字一个数字的敲了。如果有提供有格式的联系人名单，还可以直接读取名单信息自动生成vcard文件。同时，不同的手机生成的vcard文件格式略有差别，特别是在中文人名的处理方式上不同，不过都可以通过字符串的一些操作来实现。 4.通讯录vcard逆向操作如果我们得到一个vcard文件，需要马上知道里面是哪些人的电话号码就需要进行逆向操作了，这段代码不是我写的，来至记录一些最近用过的编码转换， 使用了正则表达式，还没有学到这个地方，先放在这里，后面再来研究。 123456789101112import reimport urllib.requestwith open('newdata.vcf') as file: vcf = file.read()vcf = vcf.replace('=\n=','=')names = re.findall('(=[\w=]+)',vcf)for name in names: if name[3:4]=='=': name_new = name.replace('=','%') name_new = urllib.request.unquote(name_new) vcf = vcf.replace(name,name_new,1)print(vcf) 运行后就会把上一个代码保存的文件转译过来，英文字符还是不变，只看中文名部分。 123456BEGIN:VCARDVERSION:2.1N;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:;黄晓明;;;FN;CHARSET=UTF-8;ENCODING=QUOTED-PRINTABLE:黄晓明TEL;CELL:345 6789 0133END:VCARD 参考： 廖雪峰-字符串和编码 python之urlencode()，quote()及unquote() 记录一些最近用过的编码转换 总结： 本文用了urllib.parse, quote, pickle的用法 用了try..except..语句，join, split, 切片等对字符串进行操作]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>urllib.parse</tag>
        <tag>pickle</tag>
        <tag>quote</tag>
        <tag>正则表达式</tag>
        <tag>解析式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git-Markdown-Typora操作问题解决方案记录]]></title>
    <url>%2FSonicHuang%2F4e074924%2F</url>
    <content type="text"><![CDATA[学习Markdown, Git, Typora第一天开始就出现很多操作上的问题，当时解决了后来又忘记。我想有必要把每次出现的问题记录一下。这篇文章将会持续更新。 从学习Markdown, Git, Typora第一天开始就出现很多操作上的问题，当时解决了后来又忘记。我想有必要把每次出现的问题记录一下。持续更新中。。。。 一. GitGit图解： Workspace：工作区 Index / Stage：暂存区 Repository：仓库区（或本地仓库） Remote：远程仓库 1. diverged123# Your branch and &apos;origin/master&apos; have diverged,# and have 3 and 8 different commits each, respectively.# (use &quot;git pull&quot; to merge the remote branch into yours) 方法是： 12git fetch origingit reset --hard origin/master 还没有搞懂什么意思，结果是删除了工作区新的文件，恢复到到了之前的状态，状态正常。 2.Changes to be committed123456789$ git statusOn branch masterYour branch is up to date with &apos;origin/master&apos;.Changes to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) deleted: EXE文件/EvenRuiDict/EvenRuiDictionaryv1.0DeveloperSonicHuang.exe deleted: EXE文件/downloadcatpicture/CatPicDownloadv1.0DeveloperSonicHuang.exe 情况：这两个文件我改了名字重新提交了，但是提示缓存区有被删除的文件 解决方法：后来明白，本地进行的删除操作也要经过add, commit, push这个程序才能把所有记录和远程上的文件删除 123$ git add EXE文件/EvenRuiDict/EvenRuiDictionaryv1.0DeveloperSonicHuang.exe$ git commit -m &apos;删除EvenRuiDictionaryv1.0DeveloperSonicHuang.exe&apos;$ git push 1234567891011$ git statusOn branch masterYour branch is up to date with &apos;origin/master&apos;.Changes not staged for commit: (use &quot;git add/rm &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) deleted: EXE文件/downloadcatpicture/CatPicDownloadv1.0DeveloperSonicHuang.exeno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;) 可以看到第一个文件已经被删除了。第二个文件通过同样的方法删除掉。 参考Git删除文件 总结：凡是在本地删除文件都要提交删除操作，才能使所有本地和远程保持一致 3. LF will be replaced by CRLF12warning: LF will be replaced by CRLF in 通讯录.py.The file will have its original line endings in your working directory. 出现上述问题是在通讯录.py中使用了回车换行CRLF，而git是LF。表示在远端仓库保持了CRLF， 但是我的py文件都是回车换行，为什么只有这个文件有提示呢？搞不懂 参考： CRLF和LF在跨平台工作时候带来的烦恼以及解决方法 Git中的“LF will be replaced by CRLF”警告详解 4. 在命令行出现&lt;符号12345678vulcanten@vulcanten-pc MINGW32 /e/python files/IDEL FILES/My py file (master)$ git commit -m &apos;add 爬虫-词条副标题.py&gt; ^Cvulcanten@vulcanten-pc MINGW32 /e/python files/IDEL FILES/My py file (master)$ git commit -m &apos;add 爬虫-词条副标题.py&gt; bash: unexpected EOF while looking for matching `&apos;&apos;bash: syntax error: unexpected end of file 如上当输入不完整，双引号，单引号没有闭合就会出现另起一行并出现&lt;符号，这是可以用Ctr+c, 或者Ctr+d退出输入 二. Markdown(记录Typora不能表达的情况)1. 在段落中插入一小段代码，使用英文输入法反引号(~键)引用例如：Python的print() 2.三. Typora1.查看markdown源代码在typora编辑器左下角可以查看 2.退回上一步编辑]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
        <tag>typora</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫 喵星人图片下载工具]]></title>
    <url>%2FSonicHuang%2Fb0f08802%2F</url>
    <content type="text"><![CDATA[爬取喵星人图片下载网站用easygui提供交互界面输入图片尺寸并下载图片到本地。难度不大，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465'''本脚本是利用easygui提供的交互程序在placekitten.com下载喵星人图片'''import urllib.request as urimport easygui as egimport os#基础信息，默认尺寸msg = '输入图片尺寸'title = '喵星人图片下载 v1.0 Developer:SonicHuang'length = '400'width = '600'while True: size = eg.multenterbox(msg, title, ['长', '宽'], [length, width]) if size != None and size[0] != '' and size[1] != '': length = size[0] width = size[1] url = ''.join(['http://placekitten.com/g/', length, '/', width]) #处理网络异常和输入错误 try: #int()的参数必须为整数 int(length) int(width) response = ur.urlopen(url) except: #提示异常信息并返回主循环重新输入 eg.msgbox( '出错了！请检查输入的数据(必须为整数)和检查网络是否连接.', title, 'OK') continue else: #直接读取图片，并保存为一个临时图片 catimage = response.read() with open('temp.jpg', 'wb') as f1: f1.write(catimage) #预览临时图片 asksave = eg.buttonbox('预览\n需要保存请点击图片', title, ('保存图片', '重新输入尺寸', '退出'), 'temp.jpg') #删除临时文件再保存图片，如果点击了图片会返回图片名 if asksave == '保存图片' or asksave == 'temp.jpg': os.remove('temp.jpg') savepath = eg.filesavebox('选择需要保存的位置', title = title, default = 'newimage.jpg', filetypes = ['*.jpg']) if savepath == None:#点击取消或者X返回None ask = eg.buttonbox('要继续下载吗?', title, ('继续', '退出')) if ask == '继续': continue else: break else: with open(savepath, 'wb') as f: f.write(catimage) ask = eg.buttonbox('要继续下载吗?', title, ('继续', '退出')) if ask == '继续': continue else: break #如果不喜欢这个图片或者尺寸，删除之前的临时图片，再重新输入尺寸 elif asksave == '重新输入尺寸': os.remove('temp.jpg') continue #退出时也要删除临时文件 else: os.remove('temp.jpg') break elif size == None:#退出 break else:#输入错误时 feedback = eg.buttonbox('输入错误，请重新输入?', title, ('重新输入', '退出')) if feedback == '重新输入': continue else: break pyinstaller打包后：参考用PyInstaller-3.4打包python程序为exe程序 下载一个封面图片 预览图片 总结： 脚本写好了初次运行没有报错只是完成了一半，剩下一半或者更多的是调试，修改。 要坚持写笔记，对于一个普通人来讲很有用，如果你是天才请忽略。 今天又写了一篇文章记录使用Git, Markdown, Typora使用问题和解决方法。真的感觉Git很复杂，要慢慢理解，遇到问题再解决问题，在实践中学习。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>easygui</tag>
        <tag>pyinstaller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫 利用Easygui和Pyinstaller写一个简单的字典]]></title>
    <url>%2FSonicHuang%2F8f8af010%2F</url>
    <content type="text"><![CDATA[基于上一篇文章《爬虫-有道词典》，我们用Easygui做一个简单的交互界面，用Pyinstaller打包成EXE文件，就可以方便传输和使用了。 1.先单独写一个easgui的脚本然后把字典脚本改成一个模块的形式导入就可以了。修改成模块的 youdao.py文件代码如下： 1234567891011121314151617181920212223242526272829'''本脚本是利用easygui的交互界面和youdao.py模块实现简单的词典功能。基于有道在线词典'''import easygui as egimport youdao as yd#定义相关参数，初始的输入框值为空白msg = '请输入你想查询的单词或句子\n支持英语，中文等'title = 'EvenRui小词典 v1.0 开发者:SonicHuang'words = ''meaning = ''while True: m = eg.multenterbox(msg, title,['请输入：', '意思是：'],[words, meaning]) if m != None and m[0] != '' and m[1] == '': #输入正确的状态是首先m的值不为None,在输入框有值，在'意思是'这一栏是空白. words = m[0]#再赋值给multenterbox的两个参数，使其同时显示单词和他的翻译结果。 meaning = yd.translate(words) m = eg.multenterbox(msg, title,['请输入：', '意思是：'],[words, meaning]) if m == None: #再点击cancel或者X者退出程序 break else: #点击ok 继续查询，参数为空白回到原始状态 words = '' meaning = '' continue else: #输入错误时，错误提示 ask1 = eg.buttonbox('输入错误，请在正确的位置重新输入。', title, ('重新查词', '退出词典')) if ask1 == '重新查词': words = '' meaning = '' continue else: break 2.把两个py文件放在同一个文件夹里，主程序为EvenRui.py, youdao.py为需要导入的模块, 然后CMD进入这个文件夹后，命令行输入pyinstaller -F -w EvenRui.py，具体使用方法参考我之前的文章 用PyInstaller-3.4打包python程序为exe程序。生成EXE文件后可以删除其他多余的文件。 3.双击EXE文件运行一切正常。4.总结： 运用导入模块的方法 运用pyinstaller,easygui]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>easygui</tag>
        <tag>pyinstaller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-有道词典]]></title>
    <url>%2FSonicHuang%2F118256ff%2F</url>
    <content type="text"><![CDATA[本文通过爬取有道词典，用python代码实现简单的翻译任务。 小甲鱼在教材中也讲了有道词典的爬取方法，但是时隔几年后的今天，有道工程师已经部署了反爬机制，对请求的表单数据进行了加密。如果不知道他们的加密方法，我们就会爬取失败，得到的数据是{‘errorCode’: 50} ， 或者请求非法等错误信息。一开始我也遭遇了同样的问题，通过反复的查资料不断的调试代码，虽然我不懂json, 也最终成功达到了我想要的目的。后面我们来看一下是怎么实现的。 我的配置： Python 3.7.0 window 7 简化版 ChromeVersion 69.0.3497.81 (Official Build) (32-bit) 1. Chrome打开有道在线翻译, 鼠标右键审查元素–Network–输入hello回车后就可以看到POST请求数据–点开左边的链接 2.点开链接后在Response我们可以看到有反馈的结果’你好’，返回到Headers就可以看到我们请求的地址Request URL 3.Headers下面图中Request Headers的Cookie, Referer, User-Agent后面的数据我们需要用到。 4.Headers最后面Form Data就是我们向服务器请求的数据，后面我们需要用python脚本把这些数据发到有道服务器。 请注意标注的四个参数，如果我们另外输入一个需要翻译的词语，会发现其中i, salt, sign, ts的数据是变化的。第二图与第一图比较就可以发现。我们需要知道变化的三个参数的规律，才能返回我们想要的结果。 5.找到定义这些参数的json文件，在Elements下面如图可以看到一个fanyi.min.js的json文件，复制他的地址在浏览器中打开，会发现很乱的文本。再复制所有文本到Json在线格式化工具,转化一下就比较清晰了。可以把转化后的文本在记事本中打开，查找salt的文本，就会发现相关的定义。 6.如下图，我们就可以找到ts, salt, sign的值。 我没有学过json以上的方法参考python3爬虫之有道翻译(上)，如有错误请指正。注意定义salt中最后一个字符串“p09@Bn{h02_BIEe]$P^nG”有可能会变化，根据实际情况而定。 7.知道表单参数的值后，我们根据小甲鱼的教材就可以编写Python脚本了。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import urllib.request as urimport urllib.parse as upimport timeimport randomimport hashlibimport json#请求的地址：在第2步中的Request URLurl = 'http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule'#需要翻译的内容：content = input('Input what you want to translate:')#表单数据关键数据ts, salt, sign的计算方法client = 'fanyideskweb'ts = int(time.time() * 1000) num = random.randint(1, 10)#从1到10的随机数salt = str(ts) + str(num)flowerStr = 'p09@Bn&#123;h02_BIEe]$P^nG'sign = hashlib.md5((client + content + salt + flowerStr).encode('utf-8')).hexdigest()data = &#123;&#125;data['i'] = contentdata['from'] = 'AUTO'data['to'] = 'AUTO'data['smartresult'] = 'dict'data['client'] = clientdata['salt'] = salt #salt的值可以是时间戳和随机数数值相加的结果，也可以是对应字符串相加的结果。可以是int,也可以是strdata['sign'] = sign#sign的值是client(固定值),content(变化值),salt(变化值),固定字符串拼接后的md5值data['ts'] = ts #ts的值是一个时间戳，可以是int也可以是strdata['bv'] = '3e6546407cd5c219c8baa670735759b6' #bv是浏览器和版本信息的md5值data['doctype'] = 'json'data['version'] = '2.1'data['keyfrom'] = 'fanyi.web'data['action'] = 'FY_BY_CLICKBUTTION'data['typoResult'] = 'false'#解析表单数据并编码data = up.urlencode(data).encode('utf-8')#下面的header数据需要填入,并实例化req = ur.Request(url = url, data = data, method = 'POST')req.add_header('User-Agent', 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.81 Safari/537.36')req.add_header('Referer', 'http://fanyi.youdao.com/')req.add_header('Cookie', 'OUTFOX_SEARCH_USER_ID=602357771@10.169.0.84')#请求并接受反馈信息, timeout是以秒单位确定连接的超时时间response = ur.urlopen(req, timeout = 5)#读取反馈信息并解码html = response.read().decode('utf-8')#利用json库把json数据转化为python的列表或者字典result = json.loads(html)['translateResult'][0][0]['tgt']print('The result is ==&gt;&gt; %s.' % result) Json和hashlib的用法参考：Python中的json库的简单使用 hashlib简介 总结： 没有学过json要做爬虫很被动，学爬虫本身就是被动的，昨天读了一片文章《爬虫VS反爬虫》，他们之前的内耗摧残着做这些工作的程序猿们，和消耗着网络上绝大多数的流量，浪费！所以我不打算把爬虫作为我今后主攻的方向。 一个人外部最大的敌人不是困难而是诱惑。面对诱惑，仍然坚守正道，才是高人。 知识都是相通的。要学好python，得了解很多其他方面的知识。 通过今天这段代码，了解了json, urllib.parse包，hashlib库]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>json</tag>
        <tag>urllib</tag>
        <tag>hashlib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-访问网站数据并保存在对应文件]]></title>
    <url>%2FSonicHuang%2F183747d7%2F</url>
    <content type="text"><![CDATA[题目：访问网址读取数据并保存在对应的文本文件里 我的代码：12345678910111213141516171819'''此脚本通过读取urls.txt上的网站的信息，并保存在对应的文本文件里。'''import urllib.request as urimport chardet as chdef main(): with open('urls.txt') as file: urls = file.readlines()#分行读取 n = 0 for url in urls: sources = ur.urlopen(url).read() code = ch.detect(sources)['encoding']#检测网站的编码方式 html = sources.decode(code)#对网站信息进行相应的解码,decode默认的是encoding = 'utf-8'的编码 n += 1 filename = ''.join(['file_', str(n), '.txt'])#生成对应的文件 with open(filename, 'w', encoding = code) as nfile:#再用相应的编码写入对应的文件，encoding默认的编码是utf-8 nfile.write(html)if __name__ == '__main__': main() 老师的代码：123456789101112131415161718192021222324252627282930'''读取网站信息并保存在对应的文件里。老师的方法与我的相似，不同之处，读取网站信息时使用了分割换行符的方法'''import urllib.requestimport chardetdef main(): i = 0 with open("urls.txt", "r") as f: # 读取待访问的网址 # 由于urls.txt每一行一个URL # 所以按换行符'\n'分割 urls = f.read().splitlines() for each_url in urls: response = urllib.request.urlopen(each_url) html = response.read() # 识别网页编码 encode = chardet.detect(html)['encoding'] if encode == 'GB2312': encode = 'GBK' i += 1 filename = "url_%d.txt" % i #老师用了格式化，我用了join的方法 with open(filename, "w", encoding=encode) as each_file: each_file.write(html.decode(encode, "ignore")) #decode的第二个参数。对于有些字符的特殊编码方式，我们可以通过这个方式进行忽略,详细请参考后面链接。if __name__ == "__main__": main() 以下内容参考：Python中解码decode()与编码encode()与错误处理UnicodeDecodeError: ‘gbk’ codec can’t decode byte 0xab 使用python的时候经常会遇到文本的编码与解码问题，其中很常见的一种解码错误如题目所示，下面介绍该错误的解决方法，将‘gbk’换成‘utf-8’也适用。（1）、首先在打开文本的时候，设置其编码格式，如：open(‘1.txt’,encoding=’gbk’)；（2）、若（1）不能解决，可能是文本中出现的一些特殊符号超出了gbk的编码范围，可以选择编码范围更广的‘gb18030’，如：open(‘1.txt’,encoding=’gb18030’)；（3）、若（2）仍不能解决，说明文中出现了连‘gb18030’也无法编码的字符，可以使用‘ignore’属性进行忽略，如：open(‘1.txt’,encoding=’gb18030’，errors=‘ignore’)； （4）、还有一种常见解决方法为open(‘1.txt’).read().decode(‘gb18030’,’ignore’ 总结： 复习了文件的读取与写入 decode 与 encode 的应用 urllib 与 chardet的应用]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
        <tag>urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫-编码检测应用]]></title>
    <url>%2FSonicHuang%2Fbd5cd896%2F</url>
    <content type="text"><![CDATA[上一篇我们安装编码检测工具chardet, 小甲鱼老师有道题要求用户输入任意网址，我们通过脚本判断出该网站使用的编码方式。 题目演示： 下面是我的代码：123456789101112131415161718192021222324252627282930'''本脚本是利用文本编码检测工具chardet检测用户输入的网站所使用的编码。'''import urllib.request as urimport chardet as ch#定义一个函数用于接收网站数据def source(): url = input('Please input the URL you want to detect: ') try:#检测用户输入的网址是否正确 content = ur.urlopen(url).read() except: print('The URL is wrong or it\'s not available.') else: return content#检测网站的编码def detect(): try:#如果用户输入的网址有问题这里就会抛出异常，为了使脚本运行正常，这里进行了处理，并提示用户。 sources = source() result = ch.detect(sources) except: print('You should restart the application again.') else: result = result['encoding'] if result == 'GB2312': result = 'GBK' print('The encoding way is %s.' % result)#如果作为单独脚本运行时if __name__ == '__main__': detect() 参考关于GB2312 和 GBK 总结： 要注意 1__name__ == '__main__': 的使用]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
        <tag>urllib</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python中安装编码检测工具chardet]]></title>
    <url>%2FSonicHuang%2Fb9b1e485%2F</url>
    <content type="text"><![CDATA[在Github建好博客后开始学习新的一章，爬虫，首先需要安装一个检测网页代码编码的工具chardet. 参考chardet官网 1. pip 安装在cmd输入命令 1pip install chardet 2. 使用detect()模块检测在IDLE 12345&gt;&gt;&gt; import urllib.request as ur&gt;&gt;&gt; respond = ur.urlopen('https://sonichuang.github.io')&gt;&gt;&gt; import chardet&gt;&gt;&gt; chardet.detect(respond.read())&#123;'encoding': 'utf-8', 'confidence': 0.99, 'language': ''&#125; 意思百分之九十九是utf-8, 还可以检测字符串的语言，如：中文，英文等]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>爬虫</tag>
        <tag>chardet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Github博客加入博客背景 评论功能以及修改侧栏]]></title>
    <url>%2FSonicHuang%2F2be1fd41%2F</url>
    <content type="text"><![CDATA[一.加入博客背景1. 找到主博客\themes\next\source\css_custom下custom.styl文件，原始文件里面是空白的，用记事本打开然后添加如下文本。 123456789101112131415// Custom styles.@media screen and (min-width:1200px) &#123; body &#123; background-image:url(/images/backgroundpicturename.jpg); background-repeat: no-repeat; background-attachment:fixed; background-position:50% 50%; background-size: cover &#125; #footer a &#123; color:#eee; &#125;&#125; 注意上面url后面括号里面内容。 2.把需要的背景图片放在\themes\next\source\images文件夹里然后把图片名称填入步骤1，url后面backgroundpicturename,注意扩展名例子是jpg, 要根据实际情况修改。3. 部署后就可以看到我们的背景了。我把图片进行了修改，在图片编辑器里面修改他的像素为电脑屏幕的实际像素值。 4. 如果感觉背景图片会挡住博客上的字可以调节，图片的透明度。在步骤1的文件中下面加入如下代码，12345//background color and opacity.main-inner &#123; background: #fff; opacity: 0.9;&#125; background后面的值是颜色代码，这里是白色。opacity是指透明度。这里可以根据需要进行调整。 二. 加入评论功能1.网上推荐使用LiveRe, 在其主页申请一个免费帐号，再Install(安装)，city(一般网站)就可以得到一串代码注意data-uid后面引号里面的字符串。 2.再打开主目录下\themes\next的配置文件_config.yml文件找到如下内容，然后在your uid后面填上步骤1的字符串，注意不要引号，并去掉livere uid前面的#。部署好就可以了。 三. 修改侧栏目录的显示在打开某一个文章的时候，侧栏老是在显示（下图一）如果想把隐藏掉。可以这样修改，在主目录下主题的配置文件\themes\next， _config.yml中sidebar display中在post值那里最前面加上#, 再去掉hide前的#, 就可以了（如下图二） 总结：修改博客背景，添加评论功能，修改侧栏目录的显示方式]]></content>
      <categories>
        <category>github pages</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
        <tag>github pages</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何在Typora中插入本地图片和加入Readmore按钮]]></title>
    <url>%2FSonicHuang%2F58b77a70%2F</url>
    <content type="text"><![CDATA[第一部分：插入本地图片我现在遇到的问题是：在用Typora编辑MD时，插入本地图片后，不能在网页上显示, 只能看到空白的方框。 我的博客是用Hexo搭建，使用next主题，用的windows系统，苹果电脑有个专门的软件叫ipic听说很好用，但是windows却没有。图片不能显示的原因是：本地插入的图片在Typora本地是图片的绝对地址，部署到网站后，还是显示的绝对路径，很明显，系统找不到这个地址，也就不能显示图片了。解决的方法在网上也是很多，比如安装插件，修改配置文件等等，我觉得太复杂，其实最简单的办法只要简单的几步: 1. 在博客根目录下&gt;source下新建一个assets文件夹。 2.在Typora&lt;File&lt;Preference选择如下： a. 第一个拷贝图片到指定文件夹b. 第二个选择步骤1的assets文件夹，意思是拖放到Typora中的图片会自动复制到这个assets文件夹。c. 第三，四指适用于拖入的本地图片和网络图片都会复制到assets文件夹d. 第五，使用相对路径3. 在Typora&lt;Edit&lt;Image Tools&lt;Use Image Root Path在这里选择储存图片的根目录，定位到步骤1的source文件夹，不能定位到放图片的assets文件夹，我之前就把这步弄错了，导致网站上不能显示图片。定位好后会在Typora的MD文件头里面出现如图的语句： 4.设置好了后就就可以插入图片了，显示的是图片的相对路径。 5. 部署后就能在网站正确显示了。第二部分：在博客首页加入Readmore按钮有两个办法： 1. 手动添加（推荐），在MD文中的适当位置重启一行纯文本写入如下图这一行，系统就会自动在首页显示之前的内容 2. 自动添加。在主题文件夹的配置文件\themes\next下面_config.yml，中找到如下代码： 实际上官方推荐使用手动加入Readmore按钮。因为自动添加会让文章看起来不人性化, 会出现突然在某个地方咔嚓一下断掉的感觉。另外如果同时设置的手动和自动，会优先在手动的地方显示。 总结：1. 网络上的方法不一定适合自己，所以需要不断试错。2. 不要轻易放弃。3. 本文章解决了Typora编写MD后部署到Github后图片显示不正常的问题以及加入Readmore按钮的方法。本文参考：关于Typora和hexo图片路径的问题]]></content>
      <categories>
        <category>github pages</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>next</tag>
        <tag>typora</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[更新安卓平板固件]]></title>
    <url>%2FSonicHuang%2F379f2134%2F</url>
    <content type="text"><![CDATA[本篇文章单纯为了记录安卓平板固件的更新过程。 之前为了让小孩子的英语作业能够一天多做一点，而寻求通过一些方法修改安卓app的对于时间上的限制，想了很多办法想让平板ROOT后，获取更多的权限。但是最终下载下来的固件，通过Odin重装后机子都无法正常启动。最后看到网上有人讲SM T700的平板升级到这个安卓版本6.0.1后几乎没有办法ROOT，而且后来得知App是部署在服务器上的，没有办法通过修改App的程序改变限制。 但是现在机子几乎变砖，怎么也要把它救过来。 1. 下载平板对应版本的固件。固件下载地址 2. 下载Odin官方下载地址 3. 在AP处添加步骤1的文件，其他参数不用修改。4. 按住平板电源键，音量下键，和Home键几秒中就可以看到下载模式，再按音量上键就开始启动下载。5. 用数据线连接平板和电脑，然后点击Odin3， Start.6. 直到平板安装完成重启，Odin3显示Pass，结束。 结束语：1.第一次使用Typora编辑MD文档，感觉很方便，比想象的更方便。但是现在还有一个问题没有解决，就是在Typora中插入本地的图片然后部署后能够正确的显示。2. 三星平板真难用，按照上面的步骤更新后发现还是启动不了，后来再启动Recovery模式，按住电源键，音量上键和Home。恢复到出厂设置才解决问题。3. 遇到困难的时候，在将要放弃的时候再坚持一下，问题就解决了。]]></content>
      <categories>
        <category>others</category>
      </categories>
      <tags>
        <tag>android</tag>
        <tag>ROM</tag>
        <tag>Record</tag>
      </tags>
  </entry>
</search>
